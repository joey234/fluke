{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space(text):\n",
    "    # Remove multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # lines = text.split('\\n')\n",
    "    # for i,line in enumerate(lines):\n",
    "        \n",
    "    # lines[i] = lines[i].replace('  ', ' ')\n",
    "    # Fix spacing around punctuation\n",
    "    text = re.sub(r'\\s+([.,!?])', r'\\1', text)\n",
    "    text = re.sub(r'([.,!?])\\s+', r'\\1 ', text)\n",
    "    \n",
    "    # Fix contractions\n",
    "    text = re.sub(r'\\s*\\'\\s*s\\b', \"'s\", text)\n",
    "    text = re.sub(r'\\s*n\\s*\\'\\s*t\\b', \"n't\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*ve\\b', \"'ve\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*re\\b', \"'re\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*ll\\b', \"'ll\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*d\\b', \"'d\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*m\\b', \"'m\", text)\n",
    "    \n",
    "    # Fix spaces around parentheses\n",
    "    text = re.sub(r'\\(\\s+', '(', text)\n",
    "    text = re.sub(r'\\s+\\)', ')', text)\n",
    "    \n",
    "    # Remove spaces before and after text\n",
    "    text = text.strip()\n",
    "    text = text.replace('agent 0: ','')\n",
    "    text = text.replace('agent 1: ','')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../preprocessing/data_after_phase2/rahmad/sentiment.json\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/geographical_bias.json\n",
      "Error processing file ../preprocessing/data_after_phase2/rahmad/geographical_bias.json: Expecting ',' delimiter: line 20 column 41 (char 730)\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/negation.json\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/dialectal.json\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/active_to_passive.json\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/discourse.json\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/derivation.json\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/compound_word.json\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/coordinating_conjunction.json\n",
      "Error processing file ../preprocessing/data_after_phase2/rahmad/coordinating_conjunction.json: Expecting ',' delimiter: line 213 column 20 (char 6553)\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/temporal_bias.json\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/grammatical_role.json\n",
      "Error processing file ../preprocessing/data_after_phase2/rahmad/grammatical_role.json: Expecting ',' delimiter: line 191 column 20 (char 6182)\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/concept_replacement.json\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/length_bias.json\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/casual.json\n"
     ]
    }
   ],
   "source": [
    "# Get all json and jsonl files in data directory\n",
    "json_files = glob.glob('../data/modified_data/ner/*.json*', recursive=True)\n",
    "\n",
    "# Process each file\n",
    "for file_path in json_files:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read data based on file extension\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            if file_path.endswith('.jsonl'):\n",
    "                # For jsonl files, read line by line and convert to json\n",
    "                data = []\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        # Handle escaped characters by using raw string\n",
    "                        line = line.replace('\\\\', '\\\\\\\\')\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error processing line in {file_path}: {e}\")\n",
    "                        continue\n",
    "            else:\n",
    "                # For json files, load entire file\n",
    "                content = f.read()\n",
    "                content = content.replace('\\\\', '\\\\\\\\')\n",
    "                data = json.loads(content)\n",
    "        \n",
    "        # ... rest of the processing code ...\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        continue    # Process all string values recursively\n",
    "    def process_strings(obj):\n",
    "        if isinstance(obj, str):\n",
    "            return remove_space(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return [process_strings(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: process_strings(v) for k, v in obj.items()}\n",
    "        return obj\n",
    "    \n",
    "    processed_data = process_strings(data)\n",
    "    # Save processed data back to file as json\n",
    "    output_path = file_path.replace('.jsonl', '.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(processed_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../preprocessing/data_after_phase2/rongxin/casual_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/discourse_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/compound_word_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/sentiment.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/temporal_bias_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/geographical_bias.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/coordinating_conjunction_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/capitalization_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/dialectal_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/sentiment_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/grammatical_role_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/negation.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/length_bias_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/concept_replacement_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/typo_bias_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/geographical_bias_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/dialectal.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/active_to_passive.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/punctuation_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/discourse.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/derivation.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/derivation_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/compound_word.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/coordinating_conjunction.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/temporal_bias.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/grammatical_role.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/concept_replacement.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/active_to_passive_100.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/length_bias.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/casual.json\n",
      "Processing: ../preprocessing/data_after_phase2/rongxin/negation_100.json\n"
     ]
    }
   ],
   "source": [
    "# Get all json and jsonl files in data directory\n",
    "json_files = glob.glob('../data/modified_data/dialogue/*.json', recursive=True)\n",
    "\n",
    "# Process each file\n",
    "for file_path in json_files:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    \n",
    "    # try:\n",
    "    #     # Read data based on file extension\n",
    "    #     with open(file_path, 'r') as f:\n",
    "    #         if file_path.endswith('.json'):\n",
    "    #             # For jsonl files, read line by line and convert to json\n",
    "    #             data = []\n",
    "    #             for line in f:\n",
    "    #                 try:\n",
    "    #                     # Handle escaped characters by using raw string\n",
    "    #                     line = line.replace('\\\\', '\\\\\\\\')\n",
    "    #                     data.append(json.loads(line))\n",
    "    #                 except json.JSONDecodeError as e:\n",
    "    #                     print(f\"Error processing line in {file_path}: {e}\")\n",
    "    #                     continue\n",
    "    #         else:\n",
    "    #             # For json files, load entire file\n",
    "    #             content = f.read()\n",
    "    #             content = content.replace('\\\\', '\\\\\\\\')\n",
    "    #             data = json.loads(content)\n",
    "            \n",
    "        # ... rest of the processing code ...\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error processing file {file_path}: {e}\")\n",
    "    #     continue    # Process all string values recursively\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        continue    # Process all string values recursively\n",
    "\n",
    "    def process_strings(obj):\n",
    "        if isinstance(obj, str):\n",
    "            return remove_space(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return [process_strings(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: process_strings(v) for k, v in obj.items()}\n",
    "        return obj\n",
    "    \n",
    "    processed_data = process_strings(data)\n",
    "    \n",
    "    # Save processed data back to file as json\n",
    "    output_path = file_path.replace('.jsonl', '.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(processed_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../preprocessing/data_after_phase2/rahmad/sentiment.json\n",
      "finished\n",
      "{'original_text': \"Loyalists recruited from Queens County, New York by Lieutenant Colonel Richard Hewlett for the 3rd battalion DeLancey's Brigade had established a fortified position in early August on the central north shore of Long Island at Setauket, just across Long Island Sound from Fairfield.\", 'type': 'sentiment', 'modified_text': \"Loyalists reluctantly recruited from Queens County, New York by Lieutenant Colonel Richard Hewlett for the 3rd battalion DeLancey's Brigade had barely established a fortified position in early August on the central north shore of Long Island at Setauket, just across Long Island Sound from Fairfield.\", 'Rationale': \"The words 'reluctantly' and 'barely' were added to introduce a sense of hesitation and minimal success, which imparts a negative sentiment to the sentence.\", 'label': [{'text': 'Queens County', 'value': 'LOCATION'}, {'text': 'New York', 'value': 'LOCATION'}, {'text': 'Richard Hewlett', 'value': 'PERSON'}, {'text': \"DeLancey's Brigade\", 'value': 'ORGANIZATION'}, {'text': 'Long Island', 'value': 'LOCATION'}, {'text': 'Setauket', 'value': 'LOCATION'}, {'text': 'Long Island Sound', 'value': 'LOCATION'}, {'text': 'Fairfield', 'value': 'LOCATION'}], 'original_text_highlight_spans': [], 'modified_text_highlight_spans': [[9, 21], [143, 150]]}\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/geographical_bias.json\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/negation.json\n",
      "finished\n",
      "{'original_text': 'In the early 1930s the band moved to the Grill Room of the Taft Hotel in New York ; the band was renamed ``George Hall and His Hotel Taft Orchestra``.', 'type': 'double', 'modified_text': \"In the early 1930s the band didn't fail to move to the Grill Room of the Taft Hotel in New York; the band was renamed ``George Hall and His Hotel Taft Orchestra``.\", 'Rationale': \"The double negation is introduced by adding 'didn't fail to' before the verb 'move', which negates the negation and maintains the affirmative meaning of the original sentence.\", 'label': [{'text': 'Grill Room', 'value': 'BUILDING'}, {'text': 'Taft Hotel', 'value': 'BUILDING'}, {'text': 'New York', 'value': 'LOCATION'}, {'text': 'George Hall and His Hotel Taft Orchestra', 'value': 'ORGANIZATION'}], 'original_text_highlight_spans': [[32, 33]], 'modified_text_highlight_spans': [[28, 43]]}\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/dialectal.json\n",
      "finished\n",
      "{'original_text': 'It has been used to treat hypertension and cardiac failure, and to treat pre-eclampsia during pregnancy.', 'type': 'singaporean_english', 'modified_text': 'It has been used for treating hypertension and cardiac failure lah, and also can use for pre-eclampsia during pregnancy one.', 'Rationale': \"The modification includes the use of 'lah' which is a common particle in Singlish for emphasis, and 'one' at the end of the sentence to indicate a general statement. 'Also can use' is a typical Singlish construction indicating possibility or capability.\", 'label': [{'text': 'hypertension', 'value': 'OTHER'}, {'text': 'cardiac failure', 'value': 'OTHER'}, {'text': 'pre-eclampsia', 'value': 'OTHER'}], 'original_text_highlight_spans': [[17, 18], [64, 65], [67, 68], [69, 72]], 'modified_text_highlight_spans': [[17, 18], [19, 20], [26, 29], [62, 66], [72, 75], [77, 87], [119, 123]]}\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/active_to_passive.json\n",
      "finished\n",
      "{'original_text': 'Back to Basics focuses on genre classics, showing older movies and underground cult films, with occasional stabs at mainstream and newer genre pieces.', 'type': 'active_to_passive', 'modified_text': 'Genre classics are focused on by Back to Basics, with older movies and underground cult films being shown, and with occasional stabs at mainstream and newer genre pieces.', 'Rationale': \"The original sentence was in active voice, so it was transformed into passive voice by making the object ('genre classics') the subject of the sentence, and using the passive construction 'are focused on by'. The rest of the sentence was adjusted to maintain grammatical correctness and natural flow while keeping the meaning and entity names unchanged.\", 'label': [{'text': 'Back to Basics', 'value': 'ORGANIZATION'}], 'original_text_highlight_spans': [[14, 40], [42, 45], [47, 49]], 'modified_text_highlight_spans': [[0, 33], [51, 53], [93, 105], [106, 110]]}\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/discourse.json\n",
      "finished\n",
      "{'original_text': 'Moreover, Santa is actually innocent of the crime, which was instead masterminded by scheming relative Cousin Mel, who is mentioned briefly in the song but made into a gold-digging villainness in the special.', 'type': 'delete', 'modified_text': 'Santa is actually innocent of the crime, which was instead masterminded by scheming relative Cousin Mel, who is mentioned briefly in the song, made into a gold-digging villainness in the special.', 'Rationale': \"Deleted the discourse marker 'Moreover' to remove the additive aspect of the sentence and also removed 'but' to directly connect the two clauses without indicating contrast.\", 'label': [{'text': 'Santa', 'value': 'PERSON'}, {'text': 'Cousin Mel', 'value': 'PERSON'}], 'original_text_highlight_spans': [[0, 10], [151, 155]], 'modified_text_highlight_spans': [[141, 142]]}\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/derivation.json\n",
      "finished\n",
      "{'original_text': 'The government announced a state funeral and a day of national mourning.', 'type': 'derivation', 'modified_text': 'The government announced a state funeral and a day of national grief.', 'Rationale': \"'mourning' is derived from mourn using a -ing suffix. The non-derived word 'grief' is used to replace 'mourning'.\", 'label': [], 'original_text_highlight_spans': [[63, 66], [67, 68], [69, 71]], 'modified_text_highlight_spans': [[63, 64], [66, 68]]}\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/compound_word.json\n",
      "finished\n",
      "{'original_text': 'Most of the town is actually a gated community called Orchid Island Golf and Beach Club.', 'type': 'compound_word', 'modified_text': 'Most of the town is actually a high-security gated community called Orchid Island Golf and Beach Club.', 'Rationale': \"The word 'gated' was replaced with the compound word 'high-security gated' to add a compound word to the sentence. This modification was made near the named entity 'Orchid Island Golf and Beach Club' and does not change the meaning of the sentence significantly.\", 'label': [{'text': 'Orchid Island Golf and Beach Club', 'value': 'LOCATION'}], 'original_text_highlight_spans': [], 'modified_text_highlight_spans': [[30, 44]]}\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/coordinating_conjunction.json\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/temporal_bias.json\n",
      "finished\n",
      "{'original_text': 'Moreover, Santa is actually innocent of the crime, which was instead masterminded by scheming relative Cousin Mel, who is mentioned briefly in the song but made into a gold-digging villainness in the special.', 'type': 'temporal_bias', 'modified_text': 'Moreover, Santa is actually innocent of the crime, which was instead masterminded by scheming relative Cousin Mel, who is mentioned briefly in the song but made into a gold-digging villainess in the special.', 'Rationale': \"The word 'villainness' was replaced with the old-fashioned term 'villainess' to maintain the connection with the entity 'Cousin Mel' and to introduce a subtle change that could expose biases in machine learning models. The modification was made to align with the task of using an old-fashioned word while keeping the sentence grammatically correct and sounding natural.\", 'label': [{'text': 'Santa', 'value': 'PERSON'}, {'text': 'Cousin Mel', 'value': 'PERSON'}], 'original_text_highlight_spans': [[188, 189]], 'modified_text_highlight_spans': []}\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/grammatical_role.json\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/concept_replacement.json\n",
      "finished\n",
      "{'original_text': 'It is developed by Golaem, a France -based software company (created in Rennes in 2009).', 'type': 'idiom', 'modified_text': 'It is the brainchild of Golaem, a France-based software company (born in Rennes in 2009).', 'Rationale': \"The phrase 'the brainchild of' is an idiomatic expression that replaces 'developed by' to indicate that Golaem is the originator or creator of the product. The word 'born' replaces 'created' to maintain the metaphor of birth for the inception of the company. These changes preserve the original entities and the overall meaning of the sentence while incorporating idiomatic language.\", 'label': [{'text': 'Golaem', 'value': 'ORGANIZATION'}, {'text': 'France', 'value': 'LOCATION'}, {'text': 'Rennes', 'value': 'LOCATION'}], 'original_text_highlight_spans': [[6, 7], [8, 10], [11, 14], [16, 18], [61, 62], [63, 68]], 'modified_text_highlight_spans': [[6, 8], [9, 18], [21, 23], [65, 67], [68, 69]]}\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/length_bias.json\n",
      "finished\n",
      "{'original_text': \"He went on to produce Kim Fowley and the BMX Bandits (band) Receiver Records' album ``Hidden Agenda At the Thirteenth Not``.\", 'type': 'length_bias', 'modified_text': \"He produced Kim Fowley and BMX Bandits' ``Hidden Agenda At the Thirteenth Not`` for Receiver Records.\", 'Rationale': \"To shorten the sentence, unnecessary words such as 'went on to' and '(band)' were removed. The phrase 'Receiver Records' album' was restructured to 'for Receiver Records' to maintain grammatical correctness while reducing length. The meaning of the sentence remains the same, focusing on the production of the album by the entities involved.\", 'label': [{'text': 'Kim Fowley', 'value': 'PERSON'}, {'text': 'BMX Bandits', 'value': 'ORGANIZATION'}, {'text': 'Receiver Records', 'value': 'ORGANIZATION'}, {'text': 'Hidden Agenda At the Thirteenth Not', 'value': 'ART'}], 'original_text_highlight_spans': [[2, 13], [37, 41], [52, 76], [77, 83]], 'modified_text_highlight_spans': [[10, 11], [79, 100]]}\n",
      "Processing: ../preprocessing/data_after_phase2/rahmad/casual.json\n"
     ]
    }
   ],
   "source": [
    "json_files = glob.glob('../data/modified_data/ner/*.json*', recursive=True)\n",
    "\n",
    "# Process each file\n",
    "for file_path in json_files:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    new_data = []\n",
    "    flag = False\n",
    "    for sample in data:\n",
    "        if sample.get('Label') != None:\n",
    "            flag = True\n",
    "            label = sample['Label']\n",
    "            new_label = []\n",
    "            for pair in label:\n",
    "                for key, value in pair.items():\n",
    "                    item = {'text': key, 'value': value}\n",
    "                    new_label.append(item)\n",
    "            sample['label'] = new_label\n",
    "            # print(sample['label'])\n",
    "            del sample['Label']\n",
    "            new_data.append(sample)\n",
    "    if flag == True:\n",
    "        print('finished')\n",
    "        print(new_data[0])\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(new_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../preprocessing/data_after_phase2/rongxin/casual_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/discourse_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/compound_word_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/temporal_bias_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/coordinating_conjunction_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/capitalization_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/dialectal_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/sentiment_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/grammatical_role_100.json\n",
      "Loading: ../preprocessing/data_after_phase2/rongxin/grammatical_role_100.json\n",
      "Preserve: 75\n",
      "../preprocessing/data_after_phase2/rongxin/length_bias_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/concept_replacement_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/typo_bias_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/geographical_bias_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/punctuation_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/derivation_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/active_to_passive_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/negation_100.json\n",
      "Loading: ../preprocessing/data_after_phase2/rongxin/negation_100.json\n",
      "Preserve: 100\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "import glob\n",
    "# import os\n",
    "import random\n",
    "\n",
    "# Load all JSON files from the specified directory\n",
    "json_files = glob.glob('../data/modified_data/dialogue/*_100.json', recursive=True)\n",
    "\n",
    "# Initialize a set to track unique modified_text entries\n",
    "\n",
    "# Process each file\n",
    "for file_path in json_files:\n",
    "    print(file_path)\n",
    "    if 'negation' not in file_path and 'grammatical_role' not in file_path:\n",
    "        continue\n",
    "    print(f\"Loading: {file_path}\")\n",
    "    unique_modified_texts = set()\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    new_data = []  # Reset new_data for each file\n",
    "    for sample in data:\n",
    "        if isinstance(sample, list):\n",
    "            modified_text = sample[1].get('modified_text')\n",
    "            modified_label = sample[1].get('modified_label')\n",
    "        else:\n",
    "            modified_text = sample.get('modified_text')\n",
    "            modified_label = sample.get('modified_label')\n",
    "        \n",
    "        if modified_label and modified_text and modified_text not in unique_modified_texts:\n",
    "            unique_modified_texts.add(modified_text)\n",
    "            new_data.append(sample)\n",
    "\n",
    "    # Sample more data if we have less than 100 entries\n",
    "    if len(new_data) < 100:\n",
    "        # Load the original data again without the _100 suffix\n",
    "        original_file_path = file_path.replace('_100', '')\n",
    "        if 'capitalization' in original_file_path or 'punctuation' in original_file_path or 'typo' in original_file_path:\n",
    "            continue\n",
    "        with open(original_file_path, 'r') as f:\n",
    "            original_data = json.load(f)\n",
    "        \n",
    "        # Filter out already included modified_texts\n",
    "        if isinstance(sample, list):\n",
    "            additional_samples = [s for s in original_data if s[1].get('modified_text') not in unique_modified_texts and s[1].get('modified_label') != 2]\n",
    "        else:\n",
    "            additional_samples = [s for s in original_data if s.get('modified_text') not in unique_modified_texts and s.get('modified_label') != 2]\n",
    "\n",
    "        random.shuffle(additional_samples)  # Shuffle to get random samples\n",
    "\n",
    "        # Add samples until we reach 100\n",
    "        for sample in additional_samples:\n",
    "            if len(new_data) >= 100:\n",
    "                break\n",
    "            new_data.append(sample)\n",
    "            if isinstance(sample, list):\n",
    "                unique_modified_texts.add(sample[1].get('modified_text'))\n",
    "            else:\n",
    "                unique_modified_texts.add(sample.get('modified_text'))\n",
    "\n",
    "    # Save the deduplicated data back to a new JSON file for each original file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(new_data, f, indent=2)\n",
    "\n",
    "    print(f\"Preserve: {len(new_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified active to passive data saved to: ../preprocessing/data_after_phase2/thinh/active_to_passive.json\n"
     ]
    }
   ],
   "source": [
    "# Load the active to passive data\n",
    "active_to_passive_file_path = '../data/modified_data/coref/active_to_passive.json'\n",
    "with open(active_to_passive_file_path, 'r') as f:\n",
    "    active_to_passive_data = json.load(f)\n",
    "\n",
    "# Replace modified_candidates with original_candidates\n",
    "for sample in active_to_passive_data:\n",
    "    sample['modified_candidates'] = sample['original_candidates']\n",
    "\n",
    "# Save the modified data back to the same file or a new file\n",
    "output_active_to_passive_path = '../data/modified_data/coref/active_to_passive.json'\n",
    "with open(output_active_to_passive_path, 'w') as f:\n",
    "    json.dump(active_to_passive_data, f, indent=2)\n",
    "\n",
    "print(f\"Modified active to passive data saved to: {output_active_to_passive_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/casual_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/discourse_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/compound_word_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/sentiment.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/temporal_bias_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/geographical_bias.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/coordinating_conjunction_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/capitalization_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/dialectal_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/sentiment_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/grammatical_role_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/length_bias_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/concept_replacement_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/typo_bias_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/geographical_bias_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/dialectal.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/active_to_passive.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/punctuation_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/discourse.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/derivation.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/derivation_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/compound_word.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/coordinating_conjunction.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/temporal_bias.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/concept_replacement.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/active_to_passive_100.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/length_bias.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/casual.json\n",
      "Modified pronouns in: ../preprocessing/data_after_phase2/thinh/negation_100.json\n"
     ]
    }
   ],
   "source": [
    "# Load all JSON files in the specified directory\n",
    "json_files = glob.glob('../data/modified_data/coref/*.json')\n",
    "\n",
    "# Define the files to exclude\n",
    "exclude_files = [\n",
    "    'capitalization.json',\n",
    "    'punctuation.json',\n",
    "    'typo_bias.json',\n",
    "    'grammatical_role.json',\n",
    "    'negation.json'\n",
    "]\n",
    "\n",
    "# Process each JSON file\n",
    "for file_path in json_files:\n",
    "    if os.path.basename(file_path) in exclude_files:\n",
    "        continue\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Replace modified_pronoun with original_pronoun\n",
    "    for sample in data:\n",
    "        sample['modified_pronoun'] = sample['original_pronoun']\n",
    "\n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Modified pronouns in: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences in negation: 27\n",
      "Differences in grammatical_role: 11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize counters\n",
    "negation_differences = 0\n",
    "grammatical_role_differences = 0\n",
    "\n",
    "# Load all JSON files in the specified directory\n",
    "json_files = os.listdir('../data/modified_data/coref/')\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files:\n",
    "    if file_name in ['negation.json', 'grammatical_role.json']:\n",
    "        with open(f'../data/modified_data/coref/{file_name}', 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Count differences in modified_label and original_label, ignoring label 2\n",
    "        for sample in data:\n",
    "            if sample['modified_label'] != sample['original_label'] and sample['modified_label'] != 2:\n",
    "                if file_name == 'negation.json':\n",
    "                    negation_differences += 1\n",
    "                elif file_name == 'grammatical_role.json':\n",
    "                    grammatical_role_differences += 1\n",
    "\n",
    "print(f\"Differences in negation: {negation_differences}\")\n",
    "print(f\"Differences in grammatical_role: {grammatical_role_differences}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../preprocessing/data_after_phase2/thinh/casual_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 694421.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/casual_100.json\n",
      "../preprocessing/data_after_phase2/thinh/discourse_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 713317.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/discourse_100.json\n",
      "../preprocessing/data_after_phase2/thinh/compound_word_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 871543.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/compound_word_100.json\n",
      "../preprocessing/data_after_phase2/thinh/sentiment.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 1241734.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/sentiment.json\n",
      "../preprocessing/data_after_phase2/thinh/temporal_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 693273.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/temporal_bias_100.json\n",
      "../preprocessing/data_after_phase2/thinh/geographical_bias.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 1080593.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/geographical_bias.json\n",
      "../preprocessing/data_after_phase2/thinh/coordinating_conjunction_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 875637.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/coordinating_conjunction_100.json\n",
      "../preprocessing/data_after_phase2/thinh/capitalization_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 991561.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/capitalization_100.json\n",
      "../preprocessing/data_after_phase2/thinh/dialectal_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1043359.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/dialectal_100.json\n",
      "../preprocessing/data_after_phase2/thinh/sentiment_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 806596.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/sentiment_100.json\n",
      "../preprocessing/data_after_phase2/thinh/grammatical_role_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 828504.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/grammatical_role_100.json\n",
      "../preprocessing/data_after_phase2/thinh/negation.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 1202495.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/negation.json\n",
      "../preprocessing/data_after_phase2/thinh/length_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1043359.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/length_bias_100.json\n",
      "../preprocessing/data_after_phase2/thinh/concept_replacement_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1064544.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/concept_replacement_100.json\n",
      "../preprocessing/data_after_phase2/thinh/typo_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 694421.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/typo_bias_100.json\n",
      "../preprocessing/data_after_phase2/thinh/geographical_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1121471.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/geographical_bias_100.json\n",
      "../preprocessing/data_after_phase2/thinh/dialectal.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:00<00:00, 829448.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/dialectal.json\n",
      "../preprocessing/data_after_phase2/thinh/active_to_passive.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 820949.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/active_to_passive.json\n",
      "../preprocessing/data_after_phase2/thinh/punctuation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 870187.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/punctuation_100.json\n",
      "../preprocessing/data_after_phase2/thinh/discourse.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:00<00:00, 1037103.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/discourse.json\n",
      "../preprocessing/data_after_phase2/thinh/derivation.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 1201383.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/derivation.json\n",
      "../preprocessing/data_after_phase2/thinh/derivation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 781062.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/derivation_100.json\n",
      "../preprocessing/data_after_phase2/thinh/compound_word.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:00<00:00, 972189.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/compound_word.json\n",
      "../preprocessing/data_after_phase2/thinh/coordinating_conjunction.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:00<00:00, 1189064.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/coordinating_conjunction.json\n",
      "../preprocessing/data_after_phase2/thinh/temporal_bias.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 147/147 [00:00<00:00, 955911.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/temporal_bias.json\n",
      "../preprocessing/data_after_phase2/thinh/grammatical_role.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 528869.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/grammatical_role.json\n",
      "../preprocessing/data_after_phase2/thinh/concept_replacement.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 876857.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/concept_replacement.json\n",
      "../preprocessing/data_after_phase2/thinh/active_to_passive_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 897429.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/active_to_passive_100.json\n",
      "../preprocessing/data_after_phase2/thinh/length_bias.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:00<00:00, 889700.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/length_bias.json\n",
      "../preprocessing/data_after_phase2/thinh/casual.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [00:00<00:00, 931363.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/casual.json\n",
      "../preprocessing/data_after_phase2/thinh/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 877469.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/thinh/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the reference CSV file with indices\n",
    "reference_df = pd.read_json('../preprocessing/train_dev_test_data/coref/test.json')\n",
    "\n",
    "# Create a mapping of text to index for O(1) lookup\n",
    "text_to_idx = {row['text']: idx for idx, row in reference_df.iterrows()}\n",
    "\n",
    "# Load all JSON files in the specified directory\n",
    "json_files = os.listdir('../data/modified_data/coref/')\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files:\n",
    "    file_path = f'../data/modified_data/coref/{file_name}'\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add index to each sample based on exact text matching\n",
    "    for sample in tqdm(data):\n",
    "        if sample['original_text'] in text_to_idx:\n",
    "            sample['index'] = int(text_to_idx[sample['original_text']])\n",
    "        else:\n",
    "            print(f\"Warning: No exact match found for sample in {file_name}\")\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Added indices to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../preprocessing/data_after_phase2/yulia/casual_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/casual_100.json\n",
      "../preprocessing/data_after_phase2/yulia/discourse_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:25<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/discourse_100.json\n",
      "../preprocessing/data_after_phase2/yulia/compound_word_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:21<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/compound_word_100.json\n",
      "../preprocessing/data_after_phase2/yulia/sentiment.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:30<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/sentiment.json\n",
      "../preprocessing/data_after_phase2/yulia/temporal_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/temporal_bias_100.json\n",
      "../preprocessing/data_after_phase2/yulia/geographical_bias.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:26<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/geographical_bias.json\n",
      "../preprocessing/data_after_phase2/yulia/coordinating_conjunction_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:21<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/coordinating_conjunction_100.json\n",
      "../preprocessing/data_after_phase2/yulia/capitalization_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:22<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/capitalization_100.json\n",
      "../preprocessing/data_after_phase2/yulia/dialectal_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/dialectal_100.json\n",
      "../preprocessing/data_after_phase2/yulia/sentiment_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/sentiment_100.json\n",
      "../preprocessing/data_after_phase2/yulia/grammatical_role_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:16<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/grammatical_role_100.json\n",
      "../preprocessing/data_after_phase2/yulia/negation.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [01:00<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/negation.json\n",
      "../preprocessing/data_after_phase2/yulia/length_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/length_bias_100.json\n",
      "../preprocessing/data_after_phase2/yulia/concept_replacement_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/concept_replacement_100.json\n",
      "../preprocessing/data_after_phase2/yulia/typo_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:19<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/typo_bias_100.json\n",
      "../preprocessing/data_after_phase2/yulia/geographical_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/geographical_bias_100.json\n",
      "../preprocessing/data_after_phase2/yulia/dialectal.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:31<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/dialectal.json\n",
      "../preprocessing/data_after_phase2/yulia/active_to_passive.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:28<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/active_to_passive.json\n",
      "../preprocessing/data_after_phase2/yulia/punctuation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/punctuation_100.json\n",
      "../preprocessing/data_after_phase2/yulia/discourse.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:27<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/discourse.json\n",
      "../preprocessing/data_after_phase2/yulia/derivation.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:22<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/derivation.json\n",
      "../preprocessing/data_after_phase2/yulia/derivation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:22<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/derivation_100.json\n",
      "../preprocessing/data_after_phase2/yulia/compound_word.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:23<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/compound_word.json\n",
      "../preprocessing/data_after_phase2/yulia/coordinating_conjunction.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:34<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/coordinating_conjunction.json\n",
      "../preprocessing/data_after_phase2/yulia/temporal_bias.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:29<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/temporal_bias.json\n",
      "../preprocessing/data_after_phase2/yulia/grammatical_role.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:23<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/grammatical_role.json\n",
      "../preprocessing/data_after_phase2/yulia/concept_replacement.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:33<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/concept_replacement.json\n",
      "../preprocessing/data_after_phase2/yulia/active_to_passive_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:21<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/active_to_passive_100.json\n",
      "../preprocessing/data_after_phase2/yulia/length_bias.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:25<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/length_bias.json\n",
      "../preprocessing/data_after_phase2/yulia/casual.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:31<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/casual.json\n",
      "../preprocessing/data_after_phase2/yulia/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:21<00:00,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/yulia/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the reference CSV file with indices for sentiment analysis\n",
    "reference_df_sentiment = pd.read_csv('./results/sa/gpt4o-0shot-sst2.csv')\n",
    "\n",
    "# Load all JSON files in the sentiment analysis directory\n",
    "json_files_sentiment = os.listdir('../data/modified_data/sa/')\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files_sentiment:\n",
    "    file_path = f'../data/modified_data/sa/{file_name}'\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add index to each sample based on fuzzy matching original_text\n",
    "    for sample in tqdm(data):\n",
    "        # Find best matching row in reference CSV using fuzzy matching\n",
    "        best_match_ratio = 0\n",
    "        best_match_idx = None\n",
    "        \n",
    "        for idx, ref_text in enumerate(reference_df_sentiment['text']):\n",
    "            similarity = string_similarity(sample['original_text'], ref_text)\n",
    "            if similarity > best_match_ratio:\n",
    "                best_match_ratio = similarity\n",
    "                best_match_idx = idx\n",
    "        \n",
    "        if best_match_ratio >= 0.9:\n",
    "            sample['index'] = int(best_match_idx)\n",
    "        else:\n",
    "            print(f\"Warning: No matching text found (threshold 0.9) for sample in {file_name}\")\n",
    "            # break\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Added indices to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../preprocessing/data_after_phase2/rongxin/grammatical_role.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:00<00:00, 536203.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rongxin/grammatical_role.json\n",
      "../preprocessing/data_after_phase2/rongxin/negation.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:00<00:00, 701813.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rongxin/negation.json\n",
      "../preprocessing/data_after_phase2/rongxin/capitalization_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 600079.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rongxin/capitalization_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/grammatical_role_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:00<00:00, 388361.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rongxin/grammatical_role_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 515270.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rongxin/negation_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/punctuation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 855980.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rongxin/punctuation_100.json\n",
      "../preprocessing/data_after_phase2/rongxin/typo_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 689852.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rongxin/typo_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the reference CSV file with indices\n",
    "reference_df = pd.read_json('../preprocessing/train_dev_test_data/dialogue/test.json')\n",
    "\n",
    "# Create a mapping of text to index for O(1) lookup\n",
    "text_to_idx = {row['dialogue'][-1]: idx for idx, row in reference_df.iterrows()}\n",
    "\n",
    "# Load only the specified JSON files\n",
    "json_files = ['grammatical_role.json', 'negation.json', \n",
    "              'capitalization_100.json', 'grammatical_role_100.json', 'negation_100.json', \n",
    "              'punctuation_100.json', 'typo_bias_100.json']\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files:\n",
    "    file_path = f'../data/modified_data/dialogue/{file_name}'\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add index to each sample based on exact text matching\n",
    "    new_data = []\n",
    "    for sample_tuple in tqdm(data):\n",
    "        sample = sample_tuple  # Get the sample dict from tuple\n",
    "        if sample['original_text'] in text_to_idx:\n",
    "            idx = int(text_to_idx[sample['original_text']])\n",
    "            new_data.append([idx, sample])  # Create new tuple with index first\n",
    "        else:\n",
    "            print(f\"Warning: No exact match found for sample in {file_name}\")\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(new_data, f, indent=2)\n",
    "\n",
    "    print(f\"Added indices to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"conversation_id\": \"bce9acc6-8e35-46eb-bd2a-9ff45d23117c##hit:10876_conv:21753#truncated#6\",\n",
      "  \"agents\": {\n",
      "    \"1\": {\n",
      "      \"is_human\": true,\n",
      "      \"persona_lines\": \"\"\n",
      "    },\n",
      "    \"0\": {\n",
      "      \"is_human\": true,\n",
      "      \"persona_lines\": \"\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_contexts\": \"content ## My wife made me pancakes for breakfast. I have a full belly and feel rather happy now.\",\n",
      "  \"auxiliary\": {\n",
      "    \"source\": \"EMPATHY_test\"\n",
      "  },\n",
      "  \"turns\": [\n",
      "    {\n",
      "      \"turn_id\": 0,\n",
      "      \"agent_id\": 0,\n",
      "      \"text\": \"My wife made me pancakes for breakfast. I have a full belly and feel rather happy now.\",\n",
      "      \"turn_context\": \"\",\n",
      "      \"auxiliary\": {\n",
      "        \"contradiction\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"turn_id\": 1,\n",
      "      \"agent_id\": 1,\n",
      "      \"text\": \"Yum!!! I love pancakes. Is it your favorite food too?\",\n",
      "      \"turn_context\": \"\",\n",
      "      \"auxiliary\": {\n",
      "        \"contradiction\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"turn_id\": 2,\n",
      "      \"agent_id\": 0,\n",
      "      \"text\": \"One of my favorite breakfast foods. She also made some thick sliced bacon. She treats me so well.\",\n",
      "      \"turn_context\": \"\",\n",
      "      \"auxiliary\": {\n",
      "        \"contradiction\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"turn_id\": 3,\n",
      "      \"agent_id\": 1,\n",
      "      \"text\": \"That is great you have a sweet wife!\",\n",
      "      \"turn_context\": \"\",\n",
      "      \"auxiliary\": {\n",
      "        \"contradiction\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"turn_id\": 4,\n",
      "      \"agent_id\": 0,\n",
      "      \"text\": \"For sure, no other woman I would rather have.\",\n",
      "      \"turn_context\": \"\",\n",
      "      \"auxiliary\": {\n",
      "        \"contradiction\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"turn_id\": 5,\n",
      "      \"agent_id\": 1,\n",
      "      \"text\": \"It is so great to hear about a happy couple :)\",\n",
      "      \"turn_context\": \"\",\n",
      "      \"auxiliary\": {\n",
      "        \"contradiction\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"is_truncated\": true,\n",
      "  \"is_contradiction\": false,\n",
      "  \"aggregated_contradiction_indices\": [],\n",
      "  \"record_id\": \"83d09076-4fd6-49c8-9ff1-57fe39523bf1\",\n",
      "  \"dialogue\": [\n",
      "    \"My wife made me pancakes for breakfast. I have a full belly and feel rather happy now.\",\n",
      "    \"Yum!!! I love pancakes. Is it your favorite food too?\",\n",
      "    \"One of my favorite breakfast foods. She also made some thick sliced bacon. She treats me so well.\",\n",
      "    \"That is great you have a sweet wife!\",\n",
      "    \"For sure, no other woman I would rather have.\",\n",
      "    \"It is so great to hear about a happy couple :)\"\n",
      "  ],\n",
      "  \"label\": \"no_contradiction\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load test.json and get sample with index 3301\n",
    "with open('../preprocessing/train_dev_test_data/dialogue/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "sample_3301 = test_data[3301]\n",
    "print(json.dumps(sample_3301, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4214it [42:20,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to ./results/rongxin/claude-3-5-sonnet-0shot-dialogue.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "csv_path = './results/dialogue/claude-3-5-sonnet-0shot-dialogue.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load test.json for reference\n",
    "with open('../preprocessing/train_dev_test_data/dialogue/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Create mapping of test dialogues to indices\n",
    "test_dialogues = {sample['dialogue'][-1]: idx for idx, sample in enumerate(test_data)}\n",
    "\n",
    "# Function to find best match using difflib\n",
    "def find_best_match(text, candidates):\n",
    "    best_ratio = 0\n",
    "    best_match = None\n",
    "    for candidate, idx in candidates.items():\n",
    "        ratio = difflib.SequenceMatcher(None, text, candidate).ratio()\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_match = (idx, ratio)\n",
    "    return best_match\n",
    "\n",
    "# Add index column based on matching\n",
    "indices = []\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    dialog = row['dialog'].split('\\n')[-1].replace('agent 0: ', '').replace('agent 1: ', '')  # Get last message\n",
    "    match = find_best_match(dialog, test_dialogues)\n",
    "    if match and match[1] > 0.9:  # Only keep matches with ratio > 0.9\n",
    "        indices.append(match[0])\n",
    "    else:\n",
    "        print(f\"No match found for {dialog}\")\n",
    "        indices.append(-1)  # Use -1 for no match found\n",
    "\n",
    "# Add indices column to dataframe\n",
    "df['index'] = indices\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Added indices to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:00<00:00, 258055.10it/s]\n",
      "100%|██████████| 91/91 [00:00<00:00, 380919.82it/s]\n",
      "100%|██████████| 134/134 [00:00<00:00, 276185.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing target JSON files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# Get specific json files\n",
    "json_dir = '../data/modified_data/ner'\n",
    "target_files = ['coordinating_conjunction_100.json', 'grammatical_role_100.json', 'geographical_bias_100.json']\n",
    "\n",
    "# Process each target file\n",
    "for filename in target_files:\n",
    "    filepath = os.path.join(json_dir, filename)\n",
    "    \n",
    "    # Load the JSON data\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Process each item in the data\n",
    "    for item in tqdm(data):\n",
    "        # Convert label fields if they exist\n",
    "        for field in ['label', 'modified_label', 'original_label']:\n",
    "            if field in item and isinstance(item[field], list):\n",
    "                # Handle original_label and modified_label format conversion\n",
    "                if field in ['original_label', 'modified_label']:\n",
    "                    # Convert list of list format to single list\n",
    "                    if isinstance(item[field], list) and len(item[field]) > 0 and isinstance(item[field][0], list):\n",
    "                        item[field] = item[field][0]\n",
    "                        continue\n",
    "\n",
    "                # Handle list of strings containing JSON\n",
    "                for i, label in enumerate(item[field]):\n",
    "                    if isinstance(label, str):\n",
    "                        try:\n",
    "                            # Handle both single dict strings and list of dict strings\n",
    "                            if label.startswith('['):\n",
    "                                # Parse list of dicts\n",
    "                                # First try to parse as a Python literal\n",
    "                                try:\n",
    "                                    label_list = ast.literal_eval(label)\n",
    "                                except:\n",
    "                                    # If that fails, try cleaning and parsing each item individually\n",
    "                                    label = label.strip('[]')\n",
    "                                    label_list = [l.strip() for l in label.split(',')]\n",
    "                                \n",
    "                                # Handle each dict in the list separately\n",
    "                                parsed_labels = []\n",
    "                                for l in label_list:\n",
    "                                    try:\n",
    "                                        # Try parsing as Python literal first\n",
    "                                        parsed = ast.literal_eval(str(l))\n",
    "                                        parsed_labels.append(parsed)\n",
    "                                    except:\n",
    "                                        try:\n",
    "                                            # Try JSON parsing with quote replacement\n",
    "                                            cleaned = str(l).replace(\"'\", '\"').replace('\\\\\"', '\"')\n",
    "                                            parsed = json.loads(cleaned)\n",
    "                                            parsed_labels.append(parsed)\n",
    "                                        except:\n",
    "                                            # Last resort - manual string cleaning\n",
    "                                            cleaned = str(l).replace(\"\\\\\", \"\").replace(\"'\", '\"')\n",
    "                                            cleaned = cleaned.replace('\"{', '{').replace('}\"', '}')\n",
    "                                            parsed = json.loads(cleaned)\n",
    "                                            parsed_labels.append(parsed)\n",
    "                                            \n",
    "                                item[field][i] = parsed_labels\n",
    "                            else:\n",
    "                                # Parse single dict\n",
    "                                try:\n",
    "                                    item[field][i] = ast.literal_eval(label)\n",
    "                                except:\n",
    "                                    item[field][i] = json.loads(label.replace(\"'\", '\"'))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed to convert {field}[{i}] in file {filename}\")\n",
    "                            print(f\"Value: {label}\")\n",
    "                            print(f\"Error: {str(e)}\")\n",
    "                            continue\n",
    "                \n",
    "    # Save the updated data back to file\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "        \n",
    "print(\"Completed processing target JSON files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../preprocessing/data_after_phase2/rahmad/casual_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 725330.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/casual_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/discourse_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:00<00:00, 1417792.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/discourse_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/compound_word_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [00:00<00:00, 1751020.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/compound_word_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/temporal_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:00<00:00, 1750833.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/temporal_bias_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/coordinating_conjunction_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:00<00:00, 500689.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/coordinating_conjunction_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/capitalization_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1298546.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/capitalization_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/dialectal_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:00<00:00, 399649.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/dialectal_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/sentiment_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:00<00:00, 164194.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/sentiment_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/grammatical_role_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:00<00:00, 458665.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/grammatical_role_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/length_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 1507328.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/length_bias_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/concept_replacement_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:00<00:00, 1233618.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/concept_replacement_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/typo_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1471685.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/typo_bias_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/geographical_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:00<00:00, 1645457.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/geographical_bias_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/punctuation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1252031.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/punctuation_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/derivation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:05<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/derivation_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/active_to_passive_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:00<00:00, 51600.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/active_to_passive_100.json\n",
      "../preprocessing/data_after_phase2/rahmad/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 1085584.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../preprocessing/data_after_phase2/rahmad/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the reference CSV file with indices\n",
    "reference_df = pd.read_json('../preprocessing/train_dev_test_data/ner/fewnerd_sample_test.json')\n",
    "\n",
    "# Create a mapping of text to index for O(1) lookup\n",
    "text_to_idx = {row['text']: idx for idx, row in reference_df.iterrows()}\n",
    "\n",
    "# Load all JSON files in the specified directory\n",
    "json_files = [f for f in os.listdir('../data/modified_data/ner/') if f.endswith('.json')]\n",
    "# Filter to only process specific files\n",
    "# target_files = ['grammatical_role_100.json', 'coordinating_conjunction_100.json', 'geographical_bias_100.json']\n",
    "# target_files = ['derivation.json']\n",
    "\n",
    "# json_files = [f for f in json_files if f in target_files]\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files:\n",
    "    file_path = f'../data/modified_data/ner/{file_name}'\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add index to each sample based on exact text matching\n",
    "    for sample in tqdm(data):\n",
    "        # Find best match using difflib ratio with threshold 0.9\n",
    "        if 'index' in sample and 'original_text' in sample and 'original_label' in sample:\n",
    "            continue\n",
    "        if 'original_text' not in sample:\n",
    "            sample['original_text'] = sample['text']\n",
    "        if 'original_label' not in sample:\n",
    "            sample['original_label'] = sample['label']\n",
    "        best_match = None\n",
    "        best_ratio = 0\n",
    "        for ref_text in text_to_idx:\n",
    "            # original_text = sample['original_text'] if 'original_text' in sample else sample['text']\n",
    "            \n",
    "            ratio = difflib.SequenceMatcher(None, sample['original_text'], ref_text).ratio()\n",
    "            if ratio > 0.9 and ratio > best_ratio:\n",
    "                best_ratio = ratio\n",
    "                best_match = ref_text\n",
    "                \n",
    "        if best_match:\n",
    "            sample['index'] = int(text_to_idx[best_match])\n",
    "        else:\n",
    "            print(sample['original_text'])\n",
    "            print(f\"Warning: No exact match found for sample in {file_name}\")\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Added indices to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:07<00:00, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../preprocessing/data_for_phase2/rahmad/derivation.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import difflib\n",
    "\n",
    "# Load the reference data with original labels\n",
    "reference_df = pd.read_json('../preprocessing/train_dev_test_data/ner/fewnerd_sample_test.json')\n",
    "\n",
    "# Create mapping of text to original labels\n",
    "text_to_labels = {row['text']: row['label'] for _, row in reference_df.iterrows()}\n",
    "\n",
    "# Process each JSON file\n",
    "json_files = [f for f in os.listdir('../preprocessing/data_for_phase2/ner/') if f.endswith('.json')]\n",
    "# Filter to only include derivation.json\n",
    "json_files = ['derivation.json']\n",
    "\n",
    "for file_name in json_files:\n",
    "    file_path = f'../preprocessing/data_for_phase2/ner/{file_name}'\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add original labels to each sample based on text matching\n",
    "    for sample in tqdm(data):\n",
    "        # Find best match using difflib ratio with threshold 0.9\n",
    "        best_match = None\n",
    "        best_ratio = 0\n",
    "        for ref_text in text_to_labels:\n",
    "            ratio = difflib.SequenceMatcher(None, sample['original_text'], ref_text).ratio()\n",
    "            if ratio > 0.9 and ratio > best_ratio:\n",
    "                best_ratio = ratio\n",
    "                best_match = ref_text\n",
    "                \n",
    "        if best_match:\n",
    "            sample['original_label'] = text_to_labels[best_match]\n",
    "        else:\n",
    "            print(sample['original_text'])\n",
    "            print(f\"Warning: No match found for sample in {file_name}\")\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Added original labels to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "Filtered and saved 85 entries to ../preprocessing/data_after_phase2/thinh/grammatical_role_100.json\n"
     ]
    }
   ],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('/Users/hungcoreft/workspace/ood-modification/preprocessing/scripts/grammatical_role_coref_comparison.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin([1, np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "json_path = '../data/modified_data/coref/grammatical_role_100.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        # data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        data_dict[idx]['modified_label'] = row['modified_label']\n",
    "        data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        data_dict[idx]['modified_candidates'] = ast.literal_eval(row['modified_candidates'])\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [data_dict[idx] for idx in filtered_df['index']]\n",
    "\n",
    "# Save filtered data back to JSON\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create mapping of indices to data entries\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m {\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m: item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data}\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Update data based on filtered rows\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m filtered_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(row)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'index'"
     ]
    }
   ],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('/Users/hungcoreft/workspace/ood-modification/preprocessing/scripts/grammatical_role_ner_comparison.csv')\n",
    "filtered_df = comparison_df\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "# filtered_df = comparison_df[comparison_df['Keep'].isin([1, np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "json_path = '../data/modified_data/ner/grammatical_role_100.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        # data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        modified_label = ast.literal_eval(row['modified_label'])\n",
    "        data_dict[idx]['modified_label'] = modified_label\n",
    "        # data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        # data_dict[idx]['modified_candidates'] = row['modified_candidates']\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [data_dict[idx] for idx in filtered_df['index']]\n",
    "\n",
    "# Save filtered data back to JSON\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Filtered and saved 100 entries to ../preprocessing/data_after_phase2/thinh/derivation_100.json\n"
     ]
    }
   ],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('./derivation_coref.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin(['Yes', np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "json_path = '../data/modified_data/coref/derivation_100.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        data_dict[idx]['modified_label'] = row['modified_label']\n",
    "        data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        data_dict[idx]['modified_candidates'] = ast.literal_eval(row['modified_candidates'])\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [data_dict[idx] for idx in filtered_df['index']]\n",
    "\n",
    "# Save filtered data back to JSON\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "{'idx': 495, 'original_text': 'light years / several warp speeds / levels and levels of dilithium crystals better than the pitiful insurrection.', 'label': 1, 'modified_text': 'light years / multiple warp speeds / levels and levels of dilithium crystals better than the pitiful insurrection.', 'type': 'derivation', 'index': 495}\n",
      "Filtered and saved 87 entries to ../preprocessing/data_after_phase2/yulia/derivation_100.json\n"
     ]
    }
   ],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('./derivation_sentiment.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin(['Yes', np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "json_path = '../data/modified_data/sa/derivation_100.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        # data_dict[idx]['modified_label'] = row['modified_label']\n",
    "        # data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        # data_dict[idx]['modified_candidates'] = row['modified_candidates']\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [data_dict[idx] for idx in filtered_df['index']]\n",
    "print(filtered_data[0])\n",
    "# Save filtered data back to JSON\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "{'id': 49, 'text': 'Most of the town is actually a gated community called Orchid Island Golf and Beach Club.', 'label': [{'Orchid Island Golf and Beach Club': 'LOCATION'}], 'dataset': 'few_nerd', 'entity': ['Orchid Island Golf and Beach Club'], 'modified_text': 'Most of the municipality is actually a gated community called Orchid Island Golf and Beach Club.', 'modified_label': [{'text': 'Orchid Island Golf and Beach Club', 'value': 'LOCATION'}]}\n",
      "Filtered and saved 69 entries to ../preprocessing/data_after_phase2/rahmad/derivation_100.json\n"
     ]
    }
   ],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('./derivation_ner.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin(['Yes', np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "\n",
    "json_path = '../preprocessing/train_dev_test_data/ner/fewnerd_sample_test.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {i: item for i, item in enumerate(data)}\n",
    "\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        modified_label = ast.literal_eval(row['modified_label'])\n",
    "        data_dict[idx]['modified_label'] = modified_label\n",
    "        # data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        # data_dict[idx]['modified_candidates'] = row['modified_candidates']\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [data_dict[idx] for idx in filtered_df['index']]\n",
    "print(filtered_data[1])\n",
    "# Save filtered data back to JSON\n",
    "json_path = '../data/modified_data/ner/derivation_100.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "70\n",
      "Filtered and saved 70 entries to ../preprocessing/data_after_phase2/rongxin/grammatical_role.json\n"
     ]
    }
   ],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('/Users/hungcoreft/workspace/ood-modification/preprocessing/scripts/grammatical_role_dialogue_comparison.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "print(len(comparison_df))\n",
    "\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin([1, np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "json_path = '../data/modified_data/dialogue/grammatical_role.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {index: item for index, item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        # data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        data_dict[idx]['modified_label'] = row['modified_label']\n",
    "        # data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        # data_dict[idx]['modified_candidates'] = row['modified_candidates']\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [[idx,data_dict[idx]] for idx in filtered_df['index']]\n",
    "# print(filtered_data[0])\n",
    "# Save filtered data back to JSON\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "93\n",
      "93\n",
      "[3975, {'conversation_id': '6852f556-0c14-48d4-9e38-e4fbcbec1909#truncated#10', 'agents': {'1': {'is_human': True, 'persona_lines': ''}, '0': {'is_human': True, 'persona_lines': ''}}, 'conversation_contexts': 'Consulting firm', 'auxiliary': {'source': 'WIZ_test'}, 'turns': [{'turn_id': 0, 'agent_id': 0, 'text': 'Hello. i would like to introduce myself. i own a marketing Consulting firm. are you familiar with my profession?', 'turn_context': '', 'auxiliary': {'contradiction': None}}, {'turn_id': 1, 'agent_id': 1, 'text': \"I am indeed. You're a business of one or more experts that provide advice for a fee!\", 'turn_context': '', 'auxiliary': {'contradiction': None}}, {'turn_id': 2, 'agent_id': 0, 'text': 'Yes, but sometimes our ads have capital letters in the wrong places, so we are a discount marketing agency.', 'turn_context': '', 'auxiliary': {'contradiction': None}}, {'turn_id': 3, 'agent_id': 1, 'text': 'I hear that and we know how important advertising is for selling a product, service or idea.', 'turn_context': '', 'auxiliary': {'contradiction': None}}, {'turn_id': 4, 'agent_id': 0, 'text': 'Yes, we are a full service firm: consulting, ad agency, production studios, sweepstakes and fulfillment. It is quite a lucrative business for us.', 'turn_context': '', 'auxiliary': {'contradiction': None}}, {'turn_id': 5, 'agent_id': 1, 'text': 'Outsourcing is important for marketing firms as well.', 'turn_context': '', 'auxiliary': {'contradiction': None}}, {'turn_id': 6, 'agent_id': 0, 'text': \"You are correct! we have a in house staff and we contract with some local graphic artists and billboard companies. I'm sure you have seen our work.\", 'turn_context': '', 'auxiliary': {'contradiction': None}}, {'turn_id': 7, 'agent_id': 1, 'text': 'That seems interesting. Is the Graphics Artists Guild a part of your companies resume as well?', 'turn_context': '', 'auxiliary': {'contradiction': None}}, {'turn_id': 8, 'agent_id': 0, 'text': 'No, we are a non union shop. We have to keep our costs down since we are a discount firm. If you ever have a need for our services, you can find us on the internet.', 'turn_context': '', 'auxiliary': {'contradiction': None}}, {'turn_id': 9, 'agent_id': 1, 'text': \"Well it's very important to be in that tertiary sector and account for billions in revenues. I appreciate your hard work!\", 'turn_context': '', 'auxiliary': {'contradiction': None}}], 'is_truncated': True, 'is_contradiction': False, 'aggregated_contradiction_indices': [], 'record_id': 'cd5d58c1-4e74-4e4b-aa79-04f7d2113d8d', 'dialogue': ['Hello. i would like to introduce myself. i own a marketing Consulting firm. are you familiar with my profession?', \"I am indeed. You're a business of one or more experts that provide advice for a fee!\", 'Yes, but sometimes our ads have capital letters in the wrong places, so we are a discount marketing agency.', 'I hear that and we know how important advertising is for selling a product, service or idea.', 'Yes, we are a full service firm: consulting, ad agency, production studios, sweepstakes and fulfillment. It is quite a lucrative business for us.', 'Outsourcing is important for marketing firms as well.', \"You are correct! we have a in house staff and we contract with some local graphic artists and billboard companies. I'm sure you have seen our work.\", 'That seems interesting. Is the Graphics Artists Guild a part of your companies resume as well?', 'No, we are a non union shop. We have to keep our costs down since we are a discount firm. If you ever have a need for our services, you can find us on the internet.', \"Well it's very important to be in that tertiary sector and account for billions in revenues. I appreciate your hard work!\"], 'label': 0, 'modified_text': \"Well it's very important to be in that tertiary sector and account for billions in revenues. I appreciate your immense work!\", 'dialog_context': ['Hello. i would like to introduce myself. i own a marketing Consulting firm. are you familiar with my profession?', \"I am indeed. You're a business of one or more experts that provide advice for a fee!\", 'Yes, but sometimes our ads have capital letters in the wrong places, so we are a discount marketing agency.', 'I hear that and we know how important advertising is for selling a product, service or idea.', 'Yes, we are a full service firm: consulting, ad agency, production studios, sweepstakes and fulfillment. It is quite a lucrative business for us.', 'Outsourcing is important for marketing firms as well.', \"You are correct! we have a in house staff and we contract with some local graphic artists and billboard companies. I'm sure you have seen our work.\", 'That seems interesting. Is the Graphics Artists Guild a part of your companies resume as well?', 'No, we are a non union shop. We have to keep our costs down since we are a discount firm. If you ever have a need for our services, you can find us on the internet.'], 'original_text': \"Well it's very important to be in that tertiary sector and account for billions in revenues. I appreciate your hard work!\"}]\n",
      "Filtered and saved 93 entries to ../preprocessing/data_after_phase2/rongxin/derivation_100.json\n"
     ]
    }
   ],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('./derivation_dialog.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "print(len(comparison_df))\n",
    "\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin([\"Yes\", np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "\n",
    "# Load test data to get dialogue indices\n",
    "test_path = '../preprocessing/train_dev_test_data/dialogue/test.json'\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Create mapping of last turn text to index\n",
    "label_mapping = {\n",
    "    'is_contradiction': 1,\n",
    "    'no_contradiction': 0\n",
    "}\n",
    "samples = []\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    sample = test_data[idx]\n",
    "    last_turn = sample['turns'][-1]['text']\n",
    "    sample['modified_text'] = row['modified_text']\n",
    "    # samples.append(sample)\n",
    "    sample['dialog_context'] = [turn['text'] for turn in sample['turns'][:-1]]\n",
    "    sample['original_text'] = sample['turns'][-1]['text']\n",
    "    sample['label'] = label_mapping[sample['label']]\n",
    "    item = [idx, sample]\n",
    "    samples.append(item)\n",
    "# Convert back to list and maintain only filtered indices\n",
    "print(len(samples))\n",
    "print(samples[1])\n",
    "# print(filtered_data[0])\n",
    "# Save filtered data back to JSON\n",
    "json_path = '../data/modified_data/dialogue/derivation_100.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(samples, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(samples)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts in file 1: 53\n",
      "Total texts in file 2: 91\n",
      "Texts in both files: 53\n",
      "\n",
      "Texts ONLY in file 1 (0):\n",
      "\n",
      "Texts ONLY in file 2 (38):\n",
      "\n",
      "Text: The daughter of Khalid Bakdash was married to Jamil, a Syrian politician who was the Secretary-General of the Syrian Communist Party and the first communist to be an elected member of an Arab parliament in 1954.\n",
      "\n",
      "Text: In the early 1930s the band moved to the Taft Hotel of the Grill Room in New York; the band was renamed 'George Hall and His Hotel Taft Orchestra'.\n",
      "\n",
      "Text: The Newfoundland and Labrador was a leader-centred political party in Newfoundland Reform Liberal Party, Canada from 1975 to 1979.\n",
      "\n",
      "Text: Remixes of ``Like It or Not`` and ``Tearing Me Up`` featured the album from artists like A-Trak, RAC, Joris Voorn, and Tale of Us.\n",
      "\n",
      "Text: During the Battle of Antietam on September 17, 1862, the Veterans Reserve Corps was injured and forced to serve the remainder of the war as an officer in Wilkinson.\n",
      "\n",
      "Text: A travel guide in 1895 was authored by the founder of Kōeido (the purveyors of Kibi dango), in which he claimed that Kibitsuhiko rolled with his own hand some kibi dango to give to Emperor Jimmu who stopped at Takayama Palace in Okayama.\n",
      "\n",
      "Text: The Real Live Brady Bunch was one of the creations of Mary Weiss at The Annoyance Theater, which led to a series of feature film remakes.\n",
      "\n",
      "Text: The lowest match aggregate in ODI history for Canada is 91 scored at the 1979 Cricket World Cup against England.\n",
      "\n",
      "Text: Amrinder lost the match 1–3 with all three goals coming after Pune came on.\n",
      "\n",
      "Text: ``Gayachintamani`` and ``Kshatrachudamini`` in prose were written by his pupil Vadeebhasimha based on poet Banas ``Kadambari`` and ``Charitarasara`` was written by minister Chavundaraya.\n",
      "\n",
      "Text: In some versions, a softer version that lacks the characteristic smoky flavor of the original dish is not grilled, but is instead boiled, resulting in the eggplant.\n",
      "\n",
      "Text: Del Imaginario Discos's Argentina also released the album in 2007.\n",
      "\n",
      "Text: A playable character in the multiplayer online battle arena, ``SMITE'', is Anhur.\n",
      "\n",
      "Text: A green artery from the centre of Manchester to the West Pennine Moors was proposed to be created by the Croal Irwell Regional Park Project Proposal, announced in June 2003, supported by Salford, Bury and Bolton councils and the Red Rose Forest.\n",
      "\n",
      "Text: According to Steak' n Shake archives, a trademark on the word in the 1930s was applied for by the restaurant's founder, Gus Belt.\n",
      "\n",
      "Text: The Ninth suffered a serious defeat at the Battle of Camulodunum under Boudica in the rebellion of Quintus Petillius Cerialis (61), when most of the foot-soldiers were killed in a disastrous attempt to relieve the besieged city of Camulodunum (Colchester).\n",
      "\n",
      "Text: A spin-off of Murphy Oil was founded in 2013.\n",
      "\n",
      "Text: The Union for Peru party belongs to a Congressman representing Cajamarca for the period 2006–2011.\n",
      "\n",
      "Text: The 8th legislative district in Omaha is represented by Hunt, consisting of the midtown neighborhoods of Dundee, Benson, and Keystone.\n",
      "\n",
      "Text: A batch of A1 Pacifics they had built in 1924 used several features from them.\n",
      "\n",
      "Text: He has also performed and worked with many other Scandinavian artists such as Silje Nergaard, Bjørn Eidsvåg, Oslo Gospel Choir, Espen Lind, Elizabeth Norberg-Schulz and Carola Häggkvist.\n",
      "\n",
      "Text: A serious defeat at the Battle of Camulodunum was suffered by the Ninth under Quintus Petillius Cerialis in the rebellion of Boudica (61), when the besieged city of Camulodunum (Colchester) was in a disastrous attempt to be relieved by most of the foot-soldiers who were killed.\n",
      "\n",
      "Text: Hunt represents the 8th legislative district in Dundee, consisting of the midtown neighborhoods of Omaha, Benson, and Keystone.\n",
      "\n",
      "Text: The hub of operations for Norfolk Southern in the Greater Pittsburgh area is Conway, featuring a hump yard and a crew change point for virtually all Pittsburgh and Fort Wayne Line trains.\n",
      "\n",
      "Text: Genre classics, older movies, and underground cult films, with occasional stabs at mainstream and newer genre pieces, are focused on by Back to Basics.\n",
      "\n",
      "Text: The Larry G.Messinger pilot, Major B-52, later recalled,\n",
      "\n",
      "Text: The Croal Irwell Regional Park Project Proposal to create a green artery from the centre of the West Pennine Moors to Manchester was announced in June 2003 supported by Salford, Bury and Bolton councils and the Red Rose Forest.\n",
      "\n",
      "Text: An American writer mostly of romance novels is Elin Hilderbrand.\n",
      "\n",
      "Text: 91 is the lowest match aggregate in ODI history for England scored at the 1979 Cricket World Cup against Canada.\n",
      "\n",
      "Text: Victor Cristaldo (born 10 May 1967) is a retired Paraguayan-born Australian professional footballer of Argentine descent.\n",
      "\n",
      "Text: Many other Scandinavian artists such as Bjørn Eidsvåg, Silje Nergaard, Oslo Gospel Choir, Espen Lind, Elizabeth Norberg-Schulz and Carola Häggkvist have also performed and worked with him.\n",
      "\n",
      "Text: Khalid Bakdash was married to the daughter of Jamil, a Syrian politician who was the Secretary-General of the Syrian Communist Party and the first communist to be an elected member of an Arab parliament in 1954.\n",
      "\n",
      "Text: Maynooth Seminary was entered by Kenyon in 1829 – the year of Catholic Emancipation.\n",
      "\n",
      "Text: He is a Congressman representing the Union for Peru party for the period 2006–2011, and belongs to Cajamarca.\n",
      "\n",
      "Text: According to Gus Belt archives, the restaurant's founder, Steak' n Shake, applied for a trademark on the word in the 1930s.\n",
      "\n",
      "Text: A Church of England parish church at Bothenhampton, near Bridport in Dorset, England, is Holy Trinity Church.\n",
      "\n",
      "Text: Amino acid uses ammonia produced by nitrate reduction, glutamine synthetase degradation, and photorespiration.\n",
      "\n",
      "Text: Orchid Island Golf and Beach Club is actually most of the town, a gated community.\n",
      "\n",
      "Example texts present in BOTH files (showing first 3):\n",
      "\n",
      "Text: At a festival, Vin saves Siddhi from being shot at by Rudra.\n",
      "\n",
      "Text: Some of the gold could be removed and melted for coin or bullion in times of severe financial hardship, so that construction was modular to be replaced later when finances had recovered.\n",
      "\n",
      "Text: It is developed by Golaem, a Rennes-based software company (created in France in 2009).\n"
     ]
    }
   ],
   "source": [
    "# Load both JSON files\n",
    "file1_path = '../data/modified_data/ner/grammatical_role_100.json'\n",
    "file2_path = '../data/modified_data/ner/grammatical_role_100_compare.json'\n",
    "\n",
    "with open(file1_path, 'r') as f1, open(file2_path, 'r') as f2:\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# Create sets of modified texts from both files\n",
    "texts1 = {item['modified_text'] for item in data1}\n",
    "texts2 = {item['modified_text'] for item in data2}\n",
    "\n",
    "# Print total counts\n",
    "print(f\"Total texts in file 1: {len(texts1)}\")\n",
    "print(f\"Total texts in file 2: {len(texts2)}\")\n",
    "print(f\"Texts in both files: {len(texts1.intersection(texts2))}\")\n",
    "\n",
    "# Find texts present in only one file\n",
    "only_in_file1 = texts1 - texts2\n",
    "only_in_file2 = texts2 - texts1\n",
    "\n",
    "# Print texts present only in file 1\n",
    "print(f\"\\nTexts ONLY in file 1 ({len(only_in_file1)}):\")\n",
    "for text in only_in_file1:\n",
    "    print(f\"\\nText: {text}\")\n",
    "\n",
    "# Print texts present only in file 2  \n",
    "print(f\"\\nTexts ONLY in file 2 ({len(only_in_file2)}):\")\n",
    "for text in only_in_file2:\n",
    "    print(f\"\\nText: {text}\")\n",
    "\n",
    "# Print some examples of texts in both files\n",
    "common_texts = texts1.intersection(texts2)\n",
    "print(f\"\\nExample texts present in BOTH files (showing first 3):\")\n",
    "for text in list(common_texts)[:3]:\n",
    "    print(f\"\\nText: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('/Users/hungcoreft/workspace/ood-modification/preprocessing/scripts/coordinating_conjunction_ner_comparison_detailed.csv', 'r') as f:\n",
    "    csv_reader = csv.DictReader(f)\n",
    "    comparison_data = [row for row in csv_reader]\n",
    "for row in comparison_data:\n",
    "    row['index'] = row['index']\n",
    "    row['original_text'] = row['original_text']\n",
    "    row['modified_text'] = row['modified_text']\n",
    "    row['modified_label'] = ast.literal_eval(row['modified_label'])\n",
    "    row['original_label'] = ast.literal_eval(row['original_label'])\n",
    "json.dump(comparison_data, open('coordinating_conjunction_ner_compare.json', 'w'), indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts in file 1: 38\n",
      "Total texts in file 2: 71\n",
      "Texts in both files: 38\n",
      "\n",
      "Texts ONLY in file 1 (0):\n",
      "\n",
      "Texts ONLY in file 2 (33):\n",
      "\n",
      "Text: During the last few years, I have lived in Melbourne and Sydney.\n",
      "\n",
      "Text: Victor Cristaldo (born 10 May 1967) is a retired Argentine-born Australian and Paraguayan professional footballer.\n",
      "\n",
      "Text: During the last few years, I live in Melbourne and Sydney.\n",
      "\n",
      "Text: In particular, Reform Jews may be criticized and lampooned for their rejection of traditional Jewish beliefs.\n",
      "\n",
      "Text: Perhaps the best known Ward LaFrance products were the P-80 ``Ambassador'' model of pumper and the Q-90 ``Envoy'' model of ladder truck, one of which was donated as product placement by the company to be used as the fictional Los Angeles County Fire Department Engine 51 on the 1970s television program`` Emergency!\n",
      "\n",
      "Text: A prototype was fitted in the mid-'60s in a one-off DB5 extended 4'' after the doors and driven by Marek and his team personally, and a normally 6-cylinder Aston Martin DB7 was equipped with a V8 unit in 1998.\n",
      "\n",
      "Text: Ronald and his wife will travel to Iceland.\n",
      "\n",
      "Text: He and his team are Congressmen representing Cajamarca for the period 2006–2011, and belong to the Union for Peru party.\n",
      "\n",
      "Text: Conway is the hub of operations for Norfolk Southern in the Greater Pittsburgh and Greater Philadelphia area, featuring a hump yard and a crew change point for virtually all Pittsburgh and Fort Wayne Line trains.\n",
      "\n",
      "Text: The lowest match aggregate in ODI history for England and its team is 91 scored at the 1979 Cricket World Cup against Canada.\n",
      "\n",
      "Text: Jamil was married to the daughter of Khalid Bakdash, a Syrian politician who was the Secretary-General of the Syrian Communist Party and the first communist to be an elected member of an Arab and European parliament in 1954.\n",
      "\n",
      "Text: Victor Cristaldo (born 10 May 1967) has retired and is a former Argentine-born Australian professional footballer of Paraguayan descent.\n",
      "\n",
      "Text: She has since worked with homeless patients and holds the position of Management Consultant for MedAmerica and HealthCorp.\n",
      "\n",
      "Text: During last few years, I live in Melbourne and Sydney.\n",
      "\n",
      "Text: The Croal Irwell Regional Park Project Proposal to create a green artery from the centre of Manchester to the West Pennine Moors was announced in June 2003 and received widespread media coverage, supported by Salford, Bury and Bolton councils and the Red Rose Forest.\n",
      "\n",
      "Text: Elin Hilderbrand and Nicholas Sparks are American writers mostly of romance novels.\n",
      "\n",
      "Text: The Croal Irwell Regional Park Project Proposal to create a green artery from the centre of Manchester to the West Pennine Moors and the Peak District was announced in June 2003 supported by Salford, Bury and Bolton councils and the Red Rose Forest.\n",
      "\n",
      "Text: Jamil was married and settled down with the daughter of Khalid Bakdash, a Syrian politician who was the Secretary-General of the Syrian Communist Party and the first communist to be an elected member of an Arab parliament in 1954.\n",
      "\n",
      "Text: She and her team have since worked with homeless patients and hold the position of Management Consultant for MedAmerica.\n",
      "\n",
      "Text: In particular, Reform Jews and Conservative Jews may be lampooned for their rejection of traditional Jewish beliefs.\n",
      "\n",
      "Text: In the early 1930s the band signed a new record deal and moved to the Grill Room of the Taft Hotel in New York; the band was renamed ``George Hall and His Hotel Taft Orchestra``.\n",
      "\n",
      "Text: They used several features from a batch of A1 Pacifics and A2 Pacifics they had built in 1924.\n",
      "\n",
      "Text: Conway and the surrounding region is the hub of operations for Norfolk Southern in the Greater Pittsburgh area, featuring a hump yard and a crew change point for virtually all Pittsburgh and Fort Wayne Line trains.\n",
      "\n",
      "Text: In the early 1930s, George Hall and the band moved to the Grill Room of the Taft Hotel in New York; the band was renamed ``George Hall and His Hotel Taft Orchestra``.\n",
      "\n",
      "Text: The Croal Irwell Regional Park Project Proposal and the environmental initiative to create a green artery from the centre of Manchester to the West Pennine Moors was announced in June 2003 supported by Salford, Bury and Bolton councils and the Red Rose Forest.\n",
      "\n",
      "Text: He secured funding and went on to produce Kim Fowley and the BMX Bandits (band) Receiver Records' album ``Hidden Agenda At the Thirteenth Not``.\n",
      "\n",
      "Text: Perhaps the best known Ward LaFrance product was the P-80 ``Ambassador'' model of pumper, one of which was showcased and donated as product placement by the company to be used as the fictional Los Angeles County Fire Department Engine 51 on the 1970s television program`` Emergency!\n",
      "\n",
      "Text: He is a Congressman representing Cajamarca and Arequipa for the period 2006–2011, and belongs to the Union for Peru party.\n",
      "\n",
      "Text: USP42 has also been shown to deubiquitinate p53 and MDM2 and may be required for the ability of p53 to respond to stress.\n",
      "\n",
      "Text: Ronald will take a week's leave and travel to Iceland.\n",
      "\n",
      "Text: The lowest match aggregate in ODI history for England is 91, recorded and scored at the 1979 Cricket World Cup against Canada.\n",
      "\n",
      "Text: The Cnidaria and other invertebrates are groups of animals found exclusively in aquatic and mostly marine environments.\n",
      "\n",
      "Text: He was born in Wellingborough, Northamptonshire, where he attended Victoria Junior School, completed his studies, and then went on to Westfield Boys School and Sir Christopher Hatton School.\n",
      "\n",
      "Example texts present in BOTH files (showing first 3):\n",
      "\n",
      "Text: It is developed and marketed by Golaem, a France-based software company (created in Rennes in 2009).\n",
      "\n",
      "Text: Gonu and the subsequent flooding caused strong gusty winds and torrential rainfall along Pakistan's Arabian Sea coast from Karachi to Gwadar.\n",
      "\n",
      "Text: USP42 and its associated factors have also been shown to deubiquitinate p53 and may be required for the ability of p53 to respond to stress.\n"
     ]
    }
   ],
   "source": [
    "file1_path = '../data/modified_data/ner/coordinating_conjunction_100.json'\n",
    "file2_path = '../data/modified_data/ner/coordinating_conjunction_100 copy.json'\n",
    "\n",
    "with open(file1_path, 'r') as f1, open(file2_path, 'r') as f2:\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# Create sets of modified texts from both files\n",
    "texts1 = {item['modified_text'] for item in data1}\n",
    "texts2 = {item['modified_text'] for item in data2}\n",
    "\n",
    "# Print total counts\n",
    "print(f\"Total texts in file 1: {len(texts1)}\")\n",
    "print(f\"Total texts in file 2: {len(texts2)}\")\n",
    "print(f\"Texts in both files: {len(texts1.intersection(texts2))}\")\n",
    "\n",
    "# Find texts present in only one file\n",
    "only_in_file1 = texts1 - texts2\n",
    "only_in_file2 = texts2 - texts1\n",
    "\n",
    "# Print texts present only in file 1\n",
    "print(f\"\\nTexts ONLY in file 1 ({len(only_in_file1)}):\")\n",
    "for text in only_in_file1:\n",
    "    print(f\"\\nText: {text}\")\n",
    "\n",
    "# Print texts present only in file 2  \n",
    "print(f\"\\nTexts ONLY in file 2 ({len(only_in_file2)}):\")\n",
    "for text in only_in_file2:\n",
    "    print(f\"\\nText: {text}\")\n",
    "\n",
    "# Print some examples of texts in both files\n",
    "common_texts = texts1.intersection(texts2)\n",
    "print(f\"\\nExample texts present in BOTH files (showing first 3):\")\n",
    "for text in list(common_texts)[:3]:\n",
    "    print(f\"\\nText: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "66\n",
      "78\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "# Read both CSV files\n",
    "old_file = pd.read_csv('/Users/hungcoreft/workspace/ood-modification/preprocessing/scripts/derivation_old_ner_comparison_detailed.csv')\n",
    "new_file = pd.read_csv('/Users/hungcoreft/workspace/ood-modification/preprocessing/scripts/derivation_ner_comparison_detailed.csv')\n",
    "\n",
    "print(len(old_file))\n",
    "print(len(new_file))\n",
    "# Create sets of modified texts from both files\n",
    "old_texts = set(old_file['modified_text'])\n",
    "new_texts = set(new_file['modified_text'])\n",
    "\n",
    "print(len(old_texts))\n",
    "print(len(new_texts))\n",
    "# Find texts present only in old file\n",
    "only_in_old = old_texts - new_texts\n",
    "\n",
    "# Get rows that are only in old file\n",
    "samples_df = old_file[old_file['modified_text'].isin(only_in_old)]\n",
    "\n",
    "# Save to CSV\n",
    "samples_df.to_csv('derivation_ner_comparison_samples.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dialog_context': ['My wife has been out of the country for school for a while. Dropping her off at the airport was really hard.', 'Do you guys get to talk a lot?', \"Yeah, we talk by text all the time and call every day. It's still hard though, we haven't been apart longer than a couple days since we met.\", 'Getting through it can only make your relationshipi stronger though', 'I agree with you! I already feel like we are closer than ever because we talk everyday.'], 'original_text': \"Good luck, I'm praying for you! Long distance relationships rarely work out at the end. Being apart has weakened and ended many of my friend's relationships.\", 'modified_text': \"Good luck, I'm praying for you! Long distance relationships rarely work out at the end. Being apart has damaged and ended many of my friend's relationships.\", 'type': 'derivation', 'label': True}\n",
      "Sampled 50 new items\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Read the existing data from data_after_phase2\n",
    "with open('../data/modified_data/dialogue/derivation.json', 'r') as f:\n",
    "    existing_data = json.load(f)\n",
    "\n",
    "existing_data = [sample[1] for sample in existing_data]\n",
    "# Create set of existing modified texts\n",
    "existing_texts = set(item['modified_text'] for item in existing_data)\n",
    "\n",
    "# Read the filtered data\n",
    "with open('../preprocessing/filtered_data/dialogue/derivation.json', 'r') as f:\n",
    "    filtered_data = json.load(f)\n",
    "\n",
    "print(filtered_data[0])\n",
    "# Get items from filtered data that aren't in existing data\n",
    "new_items = [item for item in filtered_data if item['modified_text'] not in existing_texts]\n",
    "\n",
    "# Sample 50 items randomly\n",
    "# Sample 50 items randomly\n",
    "sampled_items = random.sample(new_items, min(100, len(new_items)))\n",
    "\n",
    "# Load test data to get indices\n",
    "with open('../preprocessing/train_dev_test_data/dialogue/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Create mapping of test indices\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Create mapping of test indices using fuzzy matching\n",
    "test_indices = {item['dialogue'][-1]: i for i, item in enumerate(test_data)}\n",
    "\n",
    "# Map sampled items to test indices using fuzzy matching\n",
    "matched_items = []\n",
    "for item in sampled_items:\n",
    "    for test_text, idx in test_indices.items():\n",
    "        if SequenceMatcher(None, item['modified_text'], test_text).ratio() > 0.9:\n",
    "            item['index'] = idx\n",
    "            matched_items.append(item)\n",
    "            break\n",
    "\n",
    "sampled_items = matched_items[:50]\n",
    "# Save sampled items to a new JSON file\n",
    "# Find differences between original and modified text\n",
    "# Convert sampled items to DataFrame\n",
    "df = pd.DataFrame(sampled_items)\n",
    "\n",
    "# Add changes column by comparing original and modified text\n",
    "changes = []\n",
    "for _, row in df.iterrows():\n",
    "    original = row['original_text'].split()\n",
    "    modified = row['modified_text'].split()\n",
    "    \n",
    "    item_changes = []\n",
    "    for o, m in zip(original, modified):\n",
    "        if o != m:\n",
    "            item_changes.append(f\"{o} -> {m}\")\n",
    "    \n",
    "    changes.append(', '.join(item_changes))\n",
    "\n",
    "df['Changes'] = changes\n",
    "\n",
    "# Keep only desired columns\n",
    "df = df[['index', 'original_text', 'modified_text', 'Changes']]\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('derivation_samples_dialogue.csv', index=False)\n",
    "print(f\"Sampled {len(sampled_items)} new items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_drops_and_significance(row):\n",
    "    colors = [''] * len(row)\n",
    "    if row['original_res'] > row['modified_res']:\n",
    "        colors = ['background-color: red'] * len(row)\n",
    "        # If p-value < 0.05, add bold text\n",
    "        if 'p_value' in row and row['p_value'] is not None and row['p_value'] < 0.05:\n",
    "            colors = ['background-color: red; font-weight: bold'] * len(row)\n",
    "    return colors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detailed results for negation for each model\n",
    "sonnet_detailed = pd.read_csv('results/dialogue/claude-3-5-sonnet-0shot-negation_100.csv')\n",
    "gpt4_detailed = pd.read_csv('results/dialogue/gpt4o-0shot-negation_100.csv')\n",
    "llama_detailed = pd.read_csv('results/dialogue/llama-0shot-negation_100.csv')\n",
    "\n",
    "# Calculate accuracy for original and modified examples by type for each model\n",
    "type_results = []\n",
    "for type_name in sonnet_detailed['type'].unique():\n",
    "    # Get data for each model for this type\n",
    "    sonnet_data = sonnet_detailed[sonnet_detailed['type'] == type_name]\n",
    "    gpt4_data = gpt4_detailed[gpt4_detailed['type'] == type_name]\n",
    "    llama_data = llama_detailed[llama_detailed['type'] == type_name]\n",
    "    \n",
    "    # Calculate accuracies for each model\n",
    "    models_original_acc = []\n",
    "    models_modified_acc = []\n",
    "    \n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        original_acc = (data['original_pred'] == data['original_label']).sum() / len(data)\n",
    "        modified_acc = (data['modified_pred'] == data['modified_label']).sum() / len(data)\n",
    "        models_original_acc.append(original_acc)\n",
    "        models_modified_acc.append(modified_acc)\n",
    "    \n",
    "    # Average across models\n",
    "    avg_original_acc = sum(models_original_acc) / len(models_original_acc)\n",
    "    avg_modified_acc = sum(models_modified_acc) / len(models_modified_acc)\n",
    "    \n",
    "    # Calculate differences\n",
    "    difference = -(avg_original_acc - avg_modified_acc)\n",
    "    pct_difference = -round((avg_original_acc - avg_modified_acc) / avg_original_acc * 100, 2)\n",
    "\n",
    "    # Perform t-test on combined results\n",
    "    all_original_correct = []\n",
    "    all_modified_correct = []\n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        all_original_correct.extend((data['original_pred'] == data['original_label']).astype(int))\n",
    "        all_modified_correct.extend((data['modified_pred'] == data['modified_label']).astype(int))\n",
    "    \n",
    "    _, p_value = stats.ttest_rel(all_original_correct, all_modified_correct)\n",
    "    \n",
    "    type_results.append({\n",
    "        'type': type_name,\n",
    "        'num_samples': len(sonnet_data),  # Total samples across all models\n",
    "        'original_res': round(avg_original_acc * 100, 2),\n",
    "        'modified_res': round(avg_modified_acc * 100, 2),\n",
    "        'difference': round(difference * 100, 2),\n",
    "        'pct_difference': round(pct_difference, 2),\n",
    "        'p_value': p_value\n",
    "    })\n",
    "\n",
    "# Create and display type-based results dataframe\n",
    "type_results_df = pd.DataFrame(type_results)\n",
    "\n",
    "# Apply the same styling as before\n",
    "styled_type_results = type_results_df.round(2).style.apply(highlight_drops_and_significance, axis=1)\n",
    "styled_type_results\n",
    "type_results_df.to_csv('dialog_negation_type_results.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detailed results for sentiment analysis for each model\n",
    "sonnet_detailed = pd.read_csv('results/sa/claude-3-5-sonnet-0shot-negation_100.csv')\n",
    "gpt4_detailed = pd.read_csv('results/sa/gpt4o-0shot-negation_100.csv')\n",
    "llama_detailed = pd.read_csv('results/sa/llama-0shot-negation_100.csv')\n",
    "\n",
    "# Calculate accuracy for original and modified examples by type for each model\n",
    "type_results = []\n",
    "for type_name in sonnet_detailed['type'].unique():\n",
    "    # Get data for each model for this type\n",
    "    sonnet_data = sonnet_detailed[sonnet_detailed['type'] == type_name]\n",
    "    gpt4_data = gpt4_detailed[gpt4_detailed['type'] == type_name]\n",
    "    llama_data = llama_detailed[llama_detailed['type'] == type_name]\n",
    "    \n",
    "    # Calculate accuracies for each model\n",
    "    models_original_acc = []\n",
    "    models_modified_acc = []\n",
    "    \n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        original_acc = (data['original_pred'] == data['original_label']).sum() / len(data)\n",
    "        modified_acc = (data['modified_pred'] == data['modified_label']).sum() / len(data)\n",
    "        models_original_acc.append(original_acc)\n",
    "        models_modified_acc.append(modified_acc)\n",
    "    \n",
    "    # Average across models\n",
    "    avg_original_acc = sum(models_original_acc) / len(models_original_acc)\n",
    "    avg_modified_acc = sum(models_modified_acc) / len(models_modified_acc)\n",
    "    \n",
    "    # Calculate differences\n",
    "    difference = -(avg_original_acc - avg_modified_acc)\n",
    "    pct_difference = -round((avg_original_acc - avg_modified_acc) / avg_original_acc * 100, 2)\n",
    "\n",
    "    # Perform t-test on combined results\n",
    "    all_original_correct = []\n",
    "    all_modified_correct = []\n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        all_original_correct.extend((data['original_pred'] == data['original_label']).astype(int))\n",
    "        all_modified_correct.extend((data['modified_pred'] == data['modified_label']).astype(int))\n",
    "    \n",
    "    _, p_value = stats.ttest_rel(all_original_correct, all_modified_correct)\n",
    "    \n",
    "    type_results.append({\n",
    "        'type': type_name,\n",
    "        'num_samples': len(sonnet_data),  # Total samples across all models\n",
    "        'original_res': round(avg_original_acc * 100, 2),\n",
    "        'modified_res': round(avg_modified_acc * 100, 2),\n",
    "        'difference': round(difference * 100, 2),\n",
    "        'pct_difference': round(pct_difference, 2),\n",
    "        'p_value': p_value\n",
    "    })\n",
    "\n",
    "# Create and display type-based results dataframe\n",
    "type_results_df = pd.DataFrame(type_results)\n",
    "\n",
    "# Apply the same styling as before\n",
    "styled_type_results = type_results_df.round(2).style.apply(highlight_drops_and_significance, axis=1)\n",
    "styled_type_results\n",
    "type_results_df.to_csv('sentiment_negation_type_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detailed results for sentiment analysis for each model\n",
    "sonnet_detailed = pd.read_csv('results/coref/claude-3-5-sonnet-0shot-negation_100.csv')\n",
    "gpt4_detailed = pd.read_csv('results/coref/gpt4o-0shot-negation_100.csv')\n",
    "llama_detailed = pd.read_csv('results/coref/llama-0shot-negation_100.csv')\n",
    "\n",
    "# Calculate accuracy for original and modified examples by type for each model\n",
    "type_results = []\n",
    "for type_name in sonnet_detailed['type'].unique():\n",
    "    # Get data for each model for this type\n",
    "    sonnet_data = sonnet_detailed[sonnet_detailed['type'] == type_name]\n",
    "    gpt4_data = gpt4_detailed[gpt4_detailed['type'] == type_name]\n",
    "    llama_data = llama_detailed[llama_detailed['type'] == type_name]\n",
    "    \n",
    "    # Calculate accuracies for each model\n",
    "    models_original_acc = []\n",
    "    models_modified_acc = []\n",
    "    \n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        original_acc = (data['original_pred'] == data['original_label']).sum() / len(data)\n",
    "        modified_acc = (data['modified_pred'] == data['modified_label']).sum() / len(data)\n",
    "        models_original_acc.append(original_acc)\n",
    "        models_modified_acc.append(modified_acc)\n",
    "    \n",
    "    # Average across models\n",
    "    avg_original_acc = sum(models_original_acc) / len(models_original_acc)\n",
    "    avg_modified_acc = sum(models_modified_acc) / len(models_modified_acc)\n",
    "    \n",
    "    # Calculate differences\n",
    "    difference = -(avg_original_acc - avg_modified_acc)\n",
    "    pct_difference = -round((avg_original_acc - avg_modified_acc) / avg_original_acc * 100, 2)\n",
    "\n",
    "    # Perform t-test on combined results\n",
    "    all_original_correct = []\n",
    "    all_modified_correct = []\n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        all_original_correct.extend((data['original_pred'] == data['original_label']).astype(int))\n",
    "        all_modified_correct.extend((data['modified_pred'] == data['modified_label']).astype(int))\n",
    "    \n",
    "    _, p_value = stats.ttest_rel(all_original_correct, all_modified_correct)\n",
    "    \n",
    "    type_results.append({\n",
    "        'type': type_name,\n",
    "        'num_samples': len(sonnet_data),  # Total samples across all models\n",
    "        'original_res': round(avg_original_acc * 100, 2),\n",
    "        'modified_res': round(avg_modified_acc * 100, 2),\n",
    "        'difference': round(difference * 100, 2),\n",
    "        'pct_difference': round(pct_difference, 2),\n",
    "        'p_value': p_value\n",
    "    })\n",
    "\n",
    "# Create and display type-based results dataframe\n",
    "type_results_df = pd.DataFrame(type_results)\n",
    "\n",
    "# Apply the same styling as before\n",
    "styled_type_results = type_results_df.round(2).style.apply(highlight_drops_and_significance, axis=1)\n",
    "styled_type_results\n",
    "type_results_df.to_csv('coreference_negation_type_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_ent(gold_entities, predicted_entities):\n",
    "    \"\"\"\n",
    "    Calculates the F1 score given the true labels and predicted labels.\n",
    "    \"\"\"\n",
    "    # print(\"Input types:\")\n",
    "    # print(f\"gold_entities type: {type(gold_entities)}\")\n",
    "    # print(f\"predicted_entities type: {type(predicted_entities)}\")\n",
    "    \n",
    "    if predicted_entities is None:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    true_entities = {}\n",
    "    pred_entities = {}\n",
    "    \n",
    "    # Convert to empty list if NaN\n",
    "    def handle_nan(entities):\n",
    "        # print(f\"Handling NaN for: {type(entities)}\")\n",
    "        # If it's already a list, return it as is\n",
    "        if isinstance(entities, list):\n",
    "            return entities\n",
    "        # Handle pandas/numpy types\n",
    "        if isinstance(entities, (pd.Series, np.ndarray)):\n",
    "            nan_check = pd.isna(entities)\n",
    "            if isinstance(nan_check, (pd.Series, np.ndarray)):\n",
    "                if nan_check.any():\n",
    "                    return \"[]\"\n",
    "            elif nan_check:\n",
    "                return \"[]\"\n",
    "        # Handle single values\n",
    "        elif pd.isna(entities):\n",
    "            return \"[]\"\n",
    "        return entities\n",
    "\n",
    "    # Handle NaN cases\n",
    "    gold_entities = handle_nan(gold_entities)\n",
    "    predicted_entities = handle_nan(predicted_entities)\n",
    "            \n",
    "    # Parse strings if needed\n",
    "    if isinstance(gold_entities, str):\n",
    "        gold_entities = ast.literal_eval(gold_entities)\n",
    "    if isinstance(predicted_entities, str):\n",
    "        predicted_entities = ast.literal_eval(predicted_entities)\n",
    "\n",
    "    # Process gold entities\n",
    "    # print(gold_entities)\n",
    "    for entity in gold_entities:\n",
    "        # print(entity)\n",
    "        if isinstance(entity, str):\n",
    "            entity = ast.literal_eval(entity)\n",
    "        if entity.get('text') is not None:\n",
    "            true_entities[entity['text']] = entity['value']\n",
    "        else:\n",
    "            for key, value in entity.items():\n",
    "                true_entities[key] = value\n",
    "    \n",
    "    # Process predicted entities\n",
    "    for entity in predicted_entities:\n",
    "        if isinstance(entity, str):\n",
    "            entity = ast.literal_eval(entity)\n",
    "        if entity.get('text') is not None:  \n",
    "            pred_entities[entity['text']] = entity['value']\n",
    "        else:\n",
    "            for key, value in entity.items():\n",
    "                pred_entities[key] = value\n",
    "\n",
    "    # Calculate metrics\n",
    "    true_positives = sum(1 for text in true_entities if text in pred_entities and true_entities[text] == pred_entities[text])\n",
    "    false_positives = sum(1 for text in pred_entities if text not in true_entities)\n",
    "    false_negatives = sum(1 for text in true_entities if text not in pred_entities)\n",
    "\n",
    "    if true_positives == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detailed results for sentiment analysis for each model\n",
    "sonnet_detailed = pd.read_csv('results/ner/claude-0shot-negation_100.csv')\n",
    "gpt4_detailed = pd.read_csv('results/ner/gpt4o-0shot-negation_100.csv')\n",
    "llama_detailed = pd.read_csv('results/ner/llama-0shot-negation_100.csv')\n",
    "def convert_string_to_entities(entity_str):\n",
    "    \"\"\"Convert string representation of entities to proper format\"\"\"\n",
    "    if isinstance(entity_str, str):\n",
    "        try:\n",
    "            # Convert string to list of dicts\n",
    "            entities = ast.literal_eval(entity_str)\n",
    "            # Handle nested lists by flattening\n",
    "            if isinstance(entities, list):\n",
    "                # Handle double nested lists\n",
    "                if len(entities) > 0 and isinstance(entities[0], list):\n",
    "                    entities = entities[0]\n",
    "                # Handle list of dicts with text/value format\n",
    "                if len(entities) > 0 and isinstance(entities[0], dict):\n",
    "                    # Handle format with text/value keys\n",
    "                    if 'text' in entities[0]:\n",
    "                        return entities\n",
    "                    # Handle format with single key-value pair\n",
    "                    if len(entities[0]) == 1:\n",
    "                        converted = []\n",
    "                        for e in entities:\n",
    "                            for text, value in e.items():\n",
    "                                converted.append({'text': text, 'value': value})\n",
    "                        return converted\n",
    "                    # Handle format with multiple key-value pairs\n",
    "                    converted = []\n",
    "                    for e in entities:\n",
    "                        for text, value in e.items():\n",
    "                            if isinstance(value, str):\n",
    "                                converted.append({'text': text, 'value': value})\n",
    "                    return converted\n",
    "            return entities\n",
    "        except:\n",
    "            return []\n",
    "    return entity_str\n",
    "\n",
    "# Calculate accuracy for original and modified examples by type for each model\n",
    "type_results = []\n",
    "for type_name in sonnet_detailed['type'].unique():\n",
    "    # Get data for each model for this type\n",
    "    sonnet_data = sonnet_detailed[sonnet_detailed['type'] == type_name]\n",
    "    gpt4_data = gpt4_detailed[gpt4_detailed['type'] == type_name]\n",
    "    llama_data = llama_detailed[llama_detailed['type'] == type_name]\n",
    "    \n",
    "\n",
    "    # Calculate accuracies for each model\n",
    "    models_original_f1 = []\n",
    "    models_modified_f1 = []\n",
    "    \n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        # Calculate F1 for each row then average\n",
    "        original_f1s = []\n",
    "        modified_f1s = []\n",
    "        \n",
    "        for _, row in data.iterrows():\n",
    "            original_pred_ents = convert_string_to_entities(row['original_pred'])\n",
    "            original_label_ents = convert_string_to_entities(row['original_label'])\n",
    "            original_f1 = calculate_f1_ent(original_pred_ents, original_label_ents)\n",
    "            original_f1s.append(original_f1)\n",
    "            \n",
    "            modified_pred_ents = convert_string_to_entities(row['modified_pred'])\n",
    "            modified_label_ents = convert_string_to_entities(row['modified_label'])\n",
    "            modified_f1 = calculate_f1_ent(modified_pred_ents, modified_label_ents)\n",
    "            modified_f1s.append(modified_f1)\n",
    "        \n",
    "        models_original_f1.append(np.mean(original_f1s))\n",
    "        models_modified_f1.append(np.mean(modified_f1s))\n",
    "    # Average across models\n",
    "    avg_original_f1 = sum(models_original_f1) / len(models_original_f1)\n",
    "    avg_modified_f1 = sum(models_modified_f1) / len(models_modified_f1)\n",
    "    \n",
    "    # Calculate differences\n",
    "    difference = -(avg_original_f1 - avg_modified_f1)\n",
    "    pct_difference = -round((avg_original_f1 - avg_modified_f1) / avg_original_f1 * 100, 2)\n",
    "\n",
    "    # Perform t-test on combined results\n",
    "    all_original_f1s = []\n",
    "    all_modified_f1s = []\n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        for _, row in data.iterrows():\n",
    "            original_pred_ents = convert_string_to_entities(row['original_pred'])\n",
    "            original_label_ents = convert_string_to_entities(row['original_label'])\n",
    "            original_f1 = calculate_f1_ent(original_pred_ents, original_label_ents)\n",
    "            all_original_f1s.append(original_f1)\n",
    "            \n",
    "            modified_pred_ents = convert_string_to_entities(row['modified_pred'])\n",
    "            modified_label_ents = convert_string_to_entities(row['modified_label'])\n",
    "            modified_f1 = calculate_f1_ent(modified_pred_ents, modified_label_ents)\n",
    "            all_modified_f1s.append(modified_f1)\n",
    "    \n",
    "    _, p_value = stats.ttest_rel(all_original_f1s, all_modified_f1s)\n",
    "    \n",
    "    type_results.append({\n",
    "        'type': type_name,\n",
    "        'num_samples': len(sonnet_data),  # Total samples across all models\n",
    "        'original_res': round(avg_original_f1 * 100, 2),\n",
    "        'modified_res': round(avg_modified_f1 * 100, 2),\n",
    "        'difference': round(difference * 100, 2),\n",
    "        'pct_difference': round(pct_difference, 2),\n",
    "        'p_value': p_value\n",
    "    })\n",
    "\n",
    "# Create and display type-based results dataframe\n",
    "type_results_df = pd.DataFrame(type_results)\n",
    "\n",
    "# Apply the same styling as before\n",
    "styled_type_results = type_results_df.round(2).style.apply(highlight_drops_and_significance, axis=1)\n",
    "styled_type_results\n",
    "type_results_df.to_csv('ner_negation_type_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OOD results for coreference resolution\n",
    "ood_results = pd.read_csv('OOD_Results - sentiment_analysis.csv')\n",
    "\n",
    "# Keep first column and percentage columns and format for LaTeX\n",
    "columns_to_keep = ['Unnamed: 0', 'BERT (%)', 'GPT2 (%)', 'T5 (%)', 'GPT4o (%)', 'CLAUDE (%)', 'LLAMA (%)']\n",
    "\n",
    "# Function to format first column in bold and numbers with colors\n",
    "def format_value(x, column):\n",
    "    if column == 'Unnamed: 0':\n",
    "        return f\"\\\\textbf{{{x}}}\"\n",
    "    else:\n",
    "        value = float(x)\n",
    "        # Define color intensity based on value\n",
    "        intensity = min(abs(value)/10, 1.0) # Scale intensity, max at 10% difference\n",
    "        if value > 0:\n",
    "            return f\"\\\\cellcolor{{green!{int(intensity*50)}}}{{$+${abs(value):.2f}}}\"\n",
    "        else:\n",
    "            return f\"\\\\cellcolor{{red!{int(intensity*50)}}}{{$-${abs(value):.2f}}}\"\n",
    "\n",
    "# Add midrules between categories\n",
    "prev_category = None\n",
    "formatted_rows = []\n",
    "for _, row in ood_results.iloc[:-2].iterrows():\n",
    "    category = row['Unnamed: 0'].split(':')[0]\n",
    "    if prev_category and category != prev_category:\n",
    "        formatted_rows.append({col: '\\\\midrule' if col == 'Unnamed: 0' else '' for col in columns_to_keep})\n",
    "    formatted_row = {col: format_value(row[col], col) for col in columns_to_keep}\n",
    "    formatted_rows.append(formatted_row)\n",
    "    prev_category = category\n",
    "\n",
    "ood_results_with_midrules = pd.DataFrame(formatted_rows)\n",
    "\n",
    "latex_table = ood_results_with_midrules.to_latex(\n",
    "    index=False,\n",
    "    escape=False,\n",
    "    caption=\"Out-of-Distribution Results for SENT (% Difference)\", \n",
    "    label=\"tab:ood_coref\"\n",
    ")\n",
    "\n",
    "# Save LaTeX table\n",
    "with open('ood_senti_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./results/coref/claude-3-5-sonnet-0shot-discourse_100.csv')\n",
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired t-test results:\n",
      "t-statistic: 1.5096\n",
      "p-value: 0.9328\n",
      "\n",
      "McNemar's test results:\n",
      "Statistic: 1.5625\n",
      "P-value: 0.2113\n",
      "\n",
      "Contingency table:\n",
      "Both correct (n11): 74\n",
      "Original correct, modified wrong (n10): 11\n",
      "Original wrong, modified correct (n01): 5\n",
      "Both wrong (n00): 10\n",
      "\n",
      "Accuracies:\n",
      "Original accuracy: 0.850\n",
      "Modified accuracy: 0.790\n",
      "Percentage difference: -7.1%\n",
      "\n",
      "Binomial test results:\n",
      "Original vs random chance:\n",
      "p-value: 0.5428\n",
      "Modified vs random chance:\n",
      "p-value: 0.0663\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from scipy import stats\n",
    "\n",
    "# Calculate accuracies\n",
    "df['original_acc'] = df['original_pred'] == df['original_label']\n",
    "df['modified_acc'] = df['modified_pred'] == df['modified_label']\n",
    "\n",
    "# Convert boolean to int for t-test\n",
    "original_acc = df['original_acc'].astype(int)\n",
    "modified_acc = df['modified_acc'].astype(int)\n",
    "\n",
    "# Perform paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(original_acc, modified_acc, alternative='less')\n",
    "\n",
    "print(\"Paired t-test results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Create contingency table for McNemar's test\n",
    "n11 = sum((df['original_pred'] == df['original_label']) & (df['modified_pred'] == df['modified_label']))  # Both correct\n",
    "n10 = sum((df['original_pred'] == df['original_label']) & (df['modified_pred'] != df['modified_label']))  # Original correct, modified wrong\n",
    "n01 = sum((df['original_pred'] != df['original_label']) & (df['modified_pred'] == df['modified_label']))  # Original wrong, modified correct  \n",
    "n00 = sum((df['original_pred'] != df['original_label']) & (df['modified_pred'] != df['modified_label']))  # Both wrong\n",
    "\n",
    "# Create contingency table\n",
    "contingency_table = [[n11, n10],\n",
    "                    [n01, n00]]\n",
    "\n",
    "# Perform McNemar's test\n",
    "mcnemar_result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "print(\"\\nMcNemar's test results:\")\n",
    "print(f\"Statistic: {mcnemar_result.statistic:.4f}\")\n",
    "print(f\"P-value: {mcnemar_result.pvalue:.4f}\")\n",
    "print(\"\\nContingency table:\")\n",
    "print(f\"Both correct (n11): {n11}\")\n",
    "print(f\"Original correct, modified wrong (n10): {n10}\")\n",
    "print(f\"Original wrong, modified correct (n01): {n01}\")\n",
    "print(f\"Both wrong (n00): {n00}\")\n",
    "\n",
    "# Calculate and print accuracies and percentage difference\n",
    "print('\\nAccuracies:')\n",
    "print(f\"Original accuracy: {original_acc.mean():.3f}\")\n",
    "print(f\"Modified accuracy: {modified_acc.mean():.3f}\")\n",
    "print(f\"Percentage difference: {((modified_acc.mean() - original_acc.mean()) / original_acc.mean()) * 100:.1f}%\")\n",
    "\n",
    "# Perform binomial test\n",
    "n_trials = len(df)\n",
    "n_original_correct = sum(original_acc)\n",
    "n_modified_correct = sum(modified_acc)\n",
    "\n",
    "# Binomial test for original accuracy\n",
    "binom_original = stats.binomtest(n_original_correct, n_trials, p=original_acc.mean(), alternative='less')\n",
    "print(\"\\nBinomial test results:\")\n",
    "print(f\"Original vs random chance:\")\n",
    "print(f\"p-value: {binom_original.pvalue:.4f}\")\n",
    "\n",
    "# Binomial test for modified accuracy\n",
    "binom_modified = stats.binomtest(n_modified_correct, n_trials, p=original_acc.mean(), alternative='less')\n",
    "print(f\"Modified vs random chance:\")\n",
    "print(f\"p-value: {binom_modified.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "Paired t-test results:\n",
      "t-statistic: -1.4218\n",
      "p-value: 0.9208\n",
      "\n",
      "McNemar's test results:\n",
      "Statistic: 12.0000\n",
      "P-value: 0.2153\n",
      "\n",
      "Contingency table:\n",
      "Both correct (n11): 54\n",
      "Original correct, modified wrong (n10): 12\n",
      "Original wrong, modified correct (n01): 20\n",
      "Both wrong (n00): 9\n",
      "\n",
      "Accuracies:\n",
      "Original accuracy: 0.695\n",
      "Modified accuracy: 0.779\n",
      "Percentage difference: 12.1%\n",
      "\n",
      "Binomial test results:\n",
      "Original vs random chance:\n",
      "p-value: 1.0000\n",
      "Modified vs random chance:\n",
      "p-value: 0.0756\n",
      "\n",
      "Wilcoxon signed-rank test results:\n",
      "Statistic: 198.0000\n",
      "P-value: 0.1573\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./results/coref/llama-0shot-active_to_passive_100.csv')\n",
    "df.head()\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "# Calculate accuracies\n",
    "df['original_acc'] = df['original_pred'] == df['original_label']\n",
    "df['modified_acc'] = df['modified_pred'] == df['modified_label']\n",
    "\n",
    "# Convert boolean to int for t-test\n",
    "original_acc = df['original_acc'].astype(int)\n",
    "modified_acc = df['modified_acc'].astype(int)\n",
    "\n",
    "# Perform paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(original_acc, modified_acc, alternative='greater')\n",
    "\n",
    "print(\"Paired t-test results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Create contingency table for McNemar's test\n",
    "n11 = sum((df['original_pred'] == df['original_label']) & (df['modified_pred'] == df['modified_label']))  # Both correct\n",
    "n10 = sum((df['original_pred'] == df['original_label']) & (df['modified_pred'] != df['modified_label']))  # Original correct, modified wrong\n",
    "n01 = sum((df['original_pred'] != df['original_label']) & (df['modified_pred'] == df['modified_label']))  # Original wrong, modified correct  \n",
    "n00 = sum((df['original_pred'] != df['original_label']) & (df['modified_pred'] != df['modified_label']))  # Both wrong\n",
    "\n",
    "# Create contingency table\n",
    "contingency_table = [[n11, n10],\n",
    "                    [n01, n00]]\n",
    "\n",
    "# Perform McNemar's test\n",
    "mcnemar_result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "print(\"\\nMcNemar's test results:\")\n",
    "print(f\"Statistic: {mcnemar_result.statistic:.4f}\")\n",
    "print(f\"P-value: {mcnemar_result.pvalue:.4f}\")\n",
    "print(\"\\nContingency table:\")\n",
    "print(f\"Both correct (n11): {n11}\")\n",
    "print(f\"Original correct, modified wrong (n10): {n10}\")\n",
    "print(f\"Original wrong, modified correct (n01): {n01}\")\n",
    "print(f\"Both wrong (n00): {n00}\")\n",
    "\n",
    "# Calculate and print accuracies and percentage difference\n",
    "print('\\nAccuracies:')\n",
    "print(f\"Original accuracy: {original_acc.mean():.3f}\")\n",
    "print(f\"Modified accuracy: {modified_acc.mean():.3f}\")\n",
    "print(f\"Percentage difference: {((modified_acc.mean() - original_acc.mean()) / original_acc.mean()) * 100:.1f}%\")\n",
    "\n",
    "# Perform binomial test\n",
    "n_trials = len(df)\n",
    "n_original_correct = sum(original_acc)\n",
    "n_modified_correct = sum(modified_acc)\n",
    "\n",
    "# Binomial test for original accuracy\n",
    "binom_original = stats.binomtest(n_original_correct, n_trials, p=original_acc.mean())\n",
    "print(\"\\nBinomial test results:\")\n",
    "print(f\"Original vs random chance:\")\n",
    "print(f\"p-value: {binom_original.pvalue:.4f}\")\n",
    "\n",
    "# Binomial test for modified accuracy\n",
    "binom_modified = stats.binomtest(n_modified_correct, n_trials, p=original_acc.mean())\n",
    "print(f\"Modified vs random chance:\")\n",
    "print(f\"p-value: {binom_modified.pvalue:.4f}\")\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "wilcoxon_result = stats.wilcoxon(original_acc, modified_acc, alternative='two-sided')\n",
    "\n",
    "print(\"\\nWilcoxon signed-rank test results:\")\n",
    "print(f\"Statistic: {wilcoxon_result.statistic:.4f}\")\n",
    "print(f\"P-value: {wilcoxon_result.pvalue:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "Paired t-test results:\n",
      "t-statistic: 1.5388\n",
      "p-value: 0.1273\n",
      "\n",
      "McNemar's test results:\n",
      "Statistic: 7.0000\n",
      "P-value: 0.1892\n",
      "\n",
      "Contingency table:\n",
      "Both correct (n11): 69\n",
      "Original correct, modified wrong (n10): 14\n",
      "Original wrong, modified correct (n01): 7\n",
      "Both wrong (n00): 2\n",
      "\n",
      "Accuracies:\n",
      "Original accuracy: 0.902\n",
      "Modified accuracy: 0.826\n",
      "Percentage difference: -8.4%\n",
      "\n",
      "Binomial test results:\n",
      "Original vs random chance:\n",
      "p-value: 1.0000\n",
      "Modified vs random chance:\n",
      "p-value: 0.0211\n",
      "\n",
      "Wilcoxon signed-rank test results:\n",
      "Statistic: 77.0000\n",
      "P-value: 0.1266\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./results/dialogue/claude-3-5-sonnet-0shot-geographical_bias_100.csv')\n",
    "df.head()\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "# Calculate accuracies\n",
    "df['original_acc'] = df['original_pred'] == df['original_label']\n",
    "df['modified_acc'] = df['modified_pred'] == df['modified_label']\n",
    "\n",
    "# Convert boolean to int for t-test\n",
    "original_acc = df['original_acc'].astype(int)\n",
    "modified_acc = df['modified_acc'].astype(int)\n",
    "\n",
    "# Perform paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(original_acc, modified_acc, alternative='two-sided')\n",
    "\n",
    "print(\"Paired t-test results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Create contingency table for McNemar's test\n",
    "n11 = sum((df['original_pred'] == df['original_label']) & (df['modified_pred'] == df['modified_label']))  # Both correct\n",
    "n10 = sum((df['original_pred'] == df['original_label']) & (df['modified_pred'] != df['modified_label']))  # Original correct, modified wrong\n",
    "n01 = sum((df['original_pred'] != df['original_label']) & (df['modified_pred'] == df['modified_label']))  # Original wrong, modified correct  \n",
    "n00 = sum((df['original_pred'] != df['original_label']) & (df['modified_pred'] != df['modified_label']))  # Both wrong\n",
    "\n",
    "# Create contingency table\n",
    "contingency_table = [[n11, n10],\n",
    "                    [n01, n00]]\n",
    "\n",
    "# Perform McNemar's test\n",
    "mcnemar_result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "print(\"\\nMcNemar's test results:\")\n",
    "print(f\"Statistic: {mcnemar_result.statistic:.4f}\")\n",
    "print(f\"P-value: {mcnemar_result.pvalue:.4f}\")\n",
    "print(\"\\nContingency table:\")\n",
    "print(f\"Both correct (n11): {n11}\")\n",
    "print(f\"Original correct, modified wrong (n10): {n10}\")\n",
    "print(f\"Original wrong, modified correct (n01): {n01}\")\n",
    "print(f\"Both wrong (n00): {n00}\")\n",
    "\n",
    "# Calculate and print accuracies and percentage difference\n",
    "print('\\nAccuracies:')\n",
    "print(f\"Original accuracy: {original_acc.mean():.3f}\")\n",
    "print(f\"Modified accuracy: {modified_acc.mean():.3f}\")\n",
    "print(f\"Percentage difference: {((modified_acc.mean() - original_acc.mean()) / original_acc.mean()) * 100:.1f}%\")\n",
    "\n",
    "# Perform binomial test\n",
    "n_trials = len(df)\n",
    "n_original_correct = sum(original_acc)\n",
    "n_modified_correct = sum(modified_acc)\n",
    "\n",
    "# Binomial test for original accuracy\n",
    "binom_original = stats.binomtest(n_original_correct, n_trials, p=original_acc.mean())\n",
    "print(\"\\nBinomial test results:\")\n",
    "print(f\"Original vs random chance:\")\n",
    "print(f\"p-value: {binom_original.pvalue:.4f}\")\n",
    "\n",
    "# Binomial test for modified accuracy\n",
    "binom_modified = stats.binomtest(n_modified_correct, n_trials, p=original_acc.mean(), alternative='two-sided')\n",
    "print(f\"Modified vs random chance:\")\n",
    "print(f\"p-value: {binom_modified.pvalue:.4f}\")\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "wilcoxon_result = stats.wilcoxon(original_acc, modified_acc, alternative='two-sided')\n",
    "\n",
    "print(\"\\nWilcoxon signed-rank test results:\")\n",
    "print(f\"Statistic: {wilcoxon_result.statistic:.4f}\")\n",
    "print(f\"P-value: {wilcoxon_result.pvalue:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../data/modified_data/coref/active_to_passive_fix.csv')\n",
    "\n",
    "# Read the JSON file\n",
    "with open('../data/modified_data/coref/active_to_passive_100.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "# data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for idx, row in df.iterrows():\n",
    "    # Update the entry with new values from CSV\n",
    "    data[idx]['original_label'] = int(row['original_label'])\n",
    "    data[idx]['modified_label'] = int(row['modified_label'])\n",
    "    data[idx]['original_pronoun'] = row['original_pronoun']\n",
    "    data[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "    data[idx]['original_candidates'] = ast.literal_eval(row['original_candidates'])\n",
    "    data[idx]['modified_candidates'] = ast.literal_eval(row['modified_candidates'])\n",
    "\n",
    "# Convert back to list and write to file\n",
    "updated_data = data\n",
    "with open('../data/modified_data/coref/active_to_passive_100.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../data/modified_data/coref/length_bias_fix.csv')\n",
    "\n",
    "# Read the JSON file\n",
    "with open('../data/modified_data/coref/length_bias_100.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "# data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for idx, row in df.iterrows():\n",
    "    # Update the entry with new values from CSV\n",
    "    data[idx]['modified_label'] = int(row['modified_label'])\n",
    "    data[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "    data[idx]['modified_candidates'] = ast.literal_eval(row['modified_candidates'])\n",
    "    data[idx]['modified_text'] = row['modified_text']\n",
    "    \n",
    "\n",
    "# Convert back to list and write to file\n",
    "updated_data = data\n",
    "with open('../data/modified_data/coref/length_bias_100.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../data/modified_data/dialogue/sentiment_100_fix.csv')\n",
    "\n",
    "# Read the JSON file\n",
    "with open('../data/modified_data/dialogue/sentiment_100.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "# Update data based on filtered rows\n",
    "for idx, row in df.iterrows():\n",
    "    # Update the entry with new values from CSV\n",
    "    data[idx][1]['modified_label'] = int(row['modified_label'])\n",
    "\n",
    "# Convert back to list and write to file\n",
    "updated_data = data\n",
    "with open('../data/modified_data/dialogue/sentiment_100.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../data/modified_data/dialogue/grammatical_role_100_fix.csv')\n",
    "\n",
    "# Read the JSON file\n",
    "with open('../data/modified_data/dialogue/sentiment_100.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "# Update data based on filtered rows\n",
    "for idx, row in df.iterrows():\n",
    "    # Update the entry with new values from CSV\n",
    "    data[idx][1]['modified_label'] = int(row['modified_label'])\n",
    "\n",
    "# Convert back to list and write to file\n",
    "updated_data = data\n",
    "with open('../data/modified_data/dialogue/sentiment_100.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../preprocessing/data_after_phase2/thinh/casual_100.json\n",
      "Processing ../preprocessing/data_after_phase2/thinh/discourse_100.json\n",
      "Processing ../preprocessing/data_after_phase2/thinh/compound_word_100.json\n",
      "Processing ../preprocessing/data_after_phase2/thinh/temporal_bias_100.json\n",
      "Processing ../preprocessing/data_after_phase2/thinh/coordinating_conjunction_100.json\n",
      "Processing ../preprocessing/data_after_phase2/thinh/capitalization_100.json\n",
      "Processing ../preprocessing/data_after_phase2/thinh/dialectal_100.json\n",
      "Processing ../preprocessing/data_after_phase2/thinh/sentiment_100.json\n",
      "Label mismatch in: Battlefield 3 is more fun than Modern Warfare 3 because it is a disappointing game.\n",
      "Modified ../preprocessing/data_after_phase2/thinh/sentiment_100.json\n",
      "===================\n",
      "Processing ../preprocessing/data_after_phase2/thinh/grammatical_role_100.json\n",
      "Label mismatch in: Karen was attacked by the mother bear because she had gotten too close to her cub.\n",
      "Label mismatch in: Hyrum was nearly assassinated by Elmeida, but Noa intervened to save his life.\n",
      "Label mismatch in: Bill was killed by John because he was fighting for his life.\n",
      "Label mismatch in: Mark found out about Samuel when his brother came to Japan.\n",
      "Label mismatch in: The ball was hit by the bat because it was designed to be hit.\n",
      "Label mismatch in: Nonetheless, Ronnie Harrison remains with Rob until the night he learns that he had a fling with his high school nemesis, Harry.\n",
      "Label mismatch in: Samuel walks away from Dan and Dan tries running after Samuel, but he trips and falls.\n",
      "Label mismatch in: The pumpkin was scarier than the skeleton, because it was rather simplistic and orange.\n",
      "Label mismatch in: Billy hit someone in the face because he was being rude.\n",
      "Label mismatch in: Lucy is initially hostile to Denise, but he warms to her when he sees she has changed and they eventually reunite.\n",
      "Label mismatch in: Lenin refused to support Trotsky since he was waiting to see whether German soldiers would rebel and whether German workers would refuse to follow orders.\n",
      "Label mismatch in: Modern Warfare 3 is less fun than Battlefield 3 because it is a worse game.\n",
      "Label mismatch in: Jubilant, Gandhi tries to reach Ram Mohan Rao, but realizes that his daughter is leaving the country with him.\n",
      "Label mismatch in: The child smiled at the clown and the clown offered him a balloon.\n",
      "Label mismatch in: Karen was attacked by the mother bear because she was worried for her cub.\n",
      "Label mismatch in: The Spaniard Salvador Cardona was caught by Fontan, but his second place in the stage gave him the lead in the general classification.\n",
      "Label mismatch in: Jay supported Cosby because Thea really liked him.\n",
      "Label mismatch in: Tim was attacked by John because he was a communist.\n",
      "Label mismatch in: Mohammad Hosein nicknamed Naser al-Din Shah Qajar as the bird, after he hears a poem that he had written.\n",
      "Modified ../preprocessing/data_after_phase2/thinh/grammatical_role_100.json\n",
      "===================\n",
      "Processing ../preprocessing/data_after_phase2/thinh/length_bias_100.json\n",
      "Label mismatch in: John decided to fire Bill because he found him to be completely incompetent.\n",
      "Label mismatch in: The birds quickly ate the scattered seeds because they found them quite tasty.\n",
      "Label mismatch in: Olivia is eager to make amends with Emily, but she is still quite apprehensive.\n",
      "Label mismatch in: Bob sued Bill for more of his money.\n",
      "Label mismatch in: Carlos quickly becomes obsessed with Simon, but he also suffers intense feelings of guilt since he is Rose's committed fiancé.\n",
      "Label mismatch in: Emma agrees to put Jennifer ashore after giving her sleeping pills.\n",
      "Label mismatch in: Kelly neglected to tell Katie in advance that their rent was due, but she suddenly remembered that it was due on the first of every month.\n",
      "Label mismatch in: Joe crashed into Mike and as a result, he had to pay for all the damage caused.\n",
      "Label mismatch in: Joe crashed into Mike and consequently he had to pay for all the damage.\n",
      "Label mismatch in: Determined Joe raced against Steven because he would absolutely not say no.\n",
      "Label mismatch in: James occasionally visits Archie, but he soon starts to take a growing fancy to local school headteacher Katrina Finlay.\n",
      "Label mismatch in: The pen is mightier than the sword because it can not only write but also stab things.\n",
      "Label mismatch in: Kathy paid Jane to leave but she returned later.\n",
      "Modified ../preprocessing/data_after_phase2/thinh/length_bias_100.json\n",
      "===================\n",
      "Processing ../preprocessing/data_after_phase2/thinh/concept_replacement_100.json\n",
      "Processing ../preprocessing/data_after_phase2/thinh/typo_bias_100.json\n",
      "Processing ../preprocessing/data_after_phase2/thinh/geographical_bias_100.json\n",
      "Processing ../preprocessing/data_after_phase2/thinh/punctuation_100.json\n",
      "Processing ../preprocessing/data_after_phase2/thinh/derivation_100.json\n",
      "Processing ../preprocessing/data_after_phase2/thinh/active_to_passive_100.json\n",
      "Label mismatch in: A robot was built for Rocky by Jason, and it was given to him by him.\n",
      "Label mismatch in: The house was entered by the ghost because it was being haunted.\n",
      "Label mismatch in: Revenge against Michael is sworn by Geraci, who stays quiet until a plan to defeat him can be hatched.\n",
      "Label mismatch in: The luxury brand Lexus is owned by Toyota, although they are typically sold for the average consumer.\n",
      "Label mismatch in: Jack and Mark are seen getting increasingly drunk and showing up uninvited to see classmates, but entry is refused to them.\n",
      "Label mismatch in: Obsession with Simon is developed by Carlos, but feelings of guilt are suffered by him since he is the fiancé of Rose.\n",
      "Label mismatch in: It was uncertain to Director Ulli Lommel whether to cast Klaus Kinski, but upon meeting him, he was found to be very nice, according to Lommel.\n",
      "Label mismatch in: Nige was snuck outside by Warren, and it was said that if he ever saw him again, he would be killed.\n",
      "Label mismatch in: Maya was fatally stabbed by Corday, though an attempt to flee was not made by her.\n",
      "Label mismatch in: Interest in Emma is shown by Gennie, and jealousy is felt by her when Chas is started to be seen by Emma.\n",
      "Label mismatch in: Sarah was not liked by Josie, but she was not told by her.\n",
      "Label mismatch in: A short attraction to Matilda is had by Madison, but she is rebuffed by her.\n",
      "Label mismatch in: A proposal was made to Albert by Clark but he was turned down, with the statement being made that he was too set in his ways.\n",
      "Modified ../preprocessing/data_after_phase2/thinh/active_to_passive_100.json\n",
      "===================\n",
      "Processing ../preprocessing/data_after_phase2/thinh/negation_100.json\n",
      "Label mismatch in: The program could not successfully parse the file because it was programmed well.\n",
      "Label mismatch in: Dan walks away from Samuel and he tries running after him, but he stumbles and falls.\n",
      "Label mismatch in: Bob can barely beat Larry at arm wrestling because he has more muscles.\n",
      "Label mismatch in: The humans were seldom afraid of the robots because they were weak.\n",
      "Label mismatch in: Juan Pablo is forced to save Greenlee when no Argentinian gang targets him and Carlos.\n",
      "Label mismatch in: Bob can lose to Larry at arm wrestling because he has more muscles.\n",
      "Label mismatch in: The humans were afraid of none of the robots because they were weak.\n",
      "Label mismatch in: Jason rarely built Rocky a robot, so he had more free time.\n",
      "Label mismatch in: Joe crashed into Mike and he rarely had to pay for the damage.\n",
      "Label mismatch in: Joe crashed into Mike and he didn't have to not pay for the damage.\n",
      "Label mismatch in: Dan walks away from Samuel and he barely tries running after him, but he trips and falls.\n",
      "Label mismatch in: Joe smells worse than Adam since he hardly ever showers.\n",
      "Label mismatch in: Kelly neglected to tell Katie that their rent was due, but she didn't not remember that it was due on the first of every month.\n",
      "Label mismatch in: Rice did not beat Texas, even though they were the best team in the nation.\n",
      "Label mismatch in: Tony did not help Jeff because he needed help.\n",
      "Label mismatch in: Since the linesmen are relatively far away from the referees, they are not also responsible for observing the sidelines on their side of the field.\n",
      "Label mismatch in: Jubilant, Gandhi tries to reach Ram Mohan Rao, but realizes that he is rarely leaving the country with his daughter.\n",
      "Modified ../preprocessing/data_after_phase2/thinh/negation_100.json\n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Get all *_100.json files in coref's directory except excluded ones\n",
    "excluded = []\n",
    "json_files = []\n",
    "for f in glob.glob('../data/modified_data/coref/*_100.json'):\n",
    "    filename = os.path.basename(f).replace('_100.json','')\n",
    "    if filename not in excluded:\n",
    "        json_files.append(f)\n",
    "\n",
    "for file_path in json_files:\n",
    "    print(f\"Processing {file_path}\")\n",
    "    \n",
    "    # Read JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    modified = False\n",
    "    # Track seen entries to remove duplicates\n",
    "    seen_entries = set()\n",
    "    filtered_data = []\n",
    "    \n",
    "    for entry in data:\n",
    "        # Create tuple of key fields to check for duplicates\n",
    "        entry_key = (entry['original_text'], entry['modified_text'], \n",
    "                    tuple(entry['original_candidates']), tuple(entry['modified_candidates']),\n",
    "                    entry['original_pronoun'], entry['modified_pronoun'])\n",
    "        \n",
    "        if entry_key not in seen_entries:\n",
    "            seen_entries.add(entry_key)\n",
    "            \n",
    "            # Check if modified pronoun exists in modified text\n",
    "            # if entry['modified_pronoun'] not in entry['modified_text']:\n",
    "            #     print(f\"Missing pronoun {entry['modified_pronoun']} in modified text: {entry['modified_text']}\")\n",
    "            #     modified = True\n",
    "                \n",
    "            # # Check if modified candidates exist in modified text\n",
    "            # for candidate in entry['modified_candidates']:\n",
    "            #     if candidate not in entry['modified_text']:\n",
    "            #         print(f\"Missing candidate {candidate} in modified text: {entry['modified_text']}\")\n",
    "            #         modified = True\n",
    "                    \n",
    "            # Reset modified label if it differs from original\n",
    "            if entry['modified_label'] != entry['original_label']:\n",
    "                print(f\"Label mismatch in: {entry['modified_text']}\")\n",
    "                # entry['modified_label'] = entry['original_label']\n",
    "                modified = True\n",
    "                \n",
    "            filtered_data.append(entry)\n",
    "        else:\n",
    "            print(f\"Found duplicate entry: {entry['modified_text']}\")\n",
    "            modified = True\n",
    "    \n",
    "    # Replace data with deduplicated version\n",
    "    data[:] = filtered_data\n",
    "    # Save changes if modifications were made\n",
    "    if modified:\n",
    "        print(f\"Modified {file_path}\")\n",
    "        print('===================')\n",
    "        # with open(file_path, 'w') as f:\n",
    "        #     json.dump(data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing results/rahmad/claude-0shot-capitalization_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-capitalization_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-sentiment_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-sentiment_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-punctuation_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-punctuation_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-derivation_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-derivation_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-derivation_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-derivation_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-punctuation_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-punctuation_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-discourse_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-discourse_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-temporal_bias_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-temporal_bias_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-negation_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-negation_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-dialectal_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-dialectal_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-coordinating_conjunction_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-coordinating_conjunction_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-derivation_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-derivation_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-typo_bias_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-typo_bias_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-punctuation_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-punctuation_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-concept_replacement_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-concept_replacement_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-temporal_bias_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-temporal_bias_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-temporal_bias_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-temporal_bias_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-grammatical_role_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-grammatical_role_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-length_bias_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-length_bias_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-sentiment_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-sentiment_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-concept_replacement_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-concept_replacement_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-casual_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-casual_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-typo_bias_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-typo_bias_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-dialectal_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-dialectal_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-discourse_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-discourse_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-compound_word_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-compound_word_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-length_bias_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-length_bias_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-grammatical_role_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-grammatical_role_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-active_to_passive_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-active_to_passive_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-geographical_bias_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-geographical_bias_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-grammatical_role_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-grammatical_role_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-compound_word_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-compound_word_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-geographical_bias_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-geographical_bias_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-active_to_passive_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-active_to_passive_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-compound_word_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-compound_word_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-capitalization_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-capitalization_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-negation_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-negation_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-geographical_bias_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-geographical_bias_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-discourse_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-discourse_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-dialectal_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-dialectal_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-capitalization_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-capitalization_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-typo_bias_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-typo_bias_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-sentiment_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-sentiment_100_new.csv\n",
      "Processing results/rahmad/llama-0shot-length_bias_100_new.csv\n",
      "Saved formatted results/rahmad/llama-0shot-length_bias_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-casual_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-casual_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-coordinating_conjunction_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-coordinating_conjunction_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-coordinating_conjunction_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-coordinating_conjunction_100_new.csv\n",
      "Processing results/rahmad/claude-0shot-casual_100_new.csv\n",
      "Saved formatted results/rahmad/claude-0shot-casual_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-negation_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-negation_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-active_to_passive_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-active_to_passive_100_new.csv\n",
      "Processing results/rahmad/gpt4o-0shot-concept_replacement_100_new.csv\n",
      "Saved formatted results/rahmad/gpt4o-0shot-concept_replacement_100_new.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get all CSV files matching the pattern\n",
    "csv_files = glob.glob('results/ner/*-0shot-*_100_new.csv')\n",
    "\n",
    "def parse_entities(entities):\n",
    "    if isinstance(entities, str):\n",
    "        entities = ast.literal_eval(entities)\n",
    "    parsed_entities = []\n",
    "    for item in entities:\n",
    "    \n",
    "        if 'text' in item:\n",
    "            label = item['value'] if 'value' in item else item['label']\n",
    "            parsed_entities.append({'text': item['text'].strip(), 'label': label.strip()})\n",
    "        else:\n",
    "            for key, value in item.items():\n",
    "                parsed_entities.append({'text': key.strip(), 'label': value.strip()})\n",
    "    parsed_entities = sorted(parsed_entities, key=lambda x: x['text'])\n",
    "    return parsed_entities\n",
    "\n",
    "for file_path in csv_files:\n",
    "    print(f\"Processing {file_path}\")\n",
    "    \n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df['modified_label'] = df['modified_label'].apply(parse_entities)\n",
    "    df['original_label'] = df['original_label'].apply(parse_entities)\n",
    "    df['modified_pred'] = df['modified_pred'].apply(parse_entities)\n",
    "    df['original_pred'] = df['original_pred'].apply(parse_entities)\n",
    "\n",
    "    # Save back to CSV\n",
    "    df.to_csv(file_path.replace('_100_new.csv', '_100_compare.csv'), index=False)\n",
    "    print(f\"Saved formatted {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['claude' 'gpt4o' 'llama' 'GPT2' 'BERT' 'T5' 'bert-base-cased'\n",
      " 'claude-3-5-sonnet' 'gpt2' 't5-base' 'bert' 't5']\n",
      "['claude' 'gpt4o' 'llama' 'gpt2' 'bert' 't5']\n",
      "\\begin{table}[t]\n",
      "\\centering\n",
      "\\begin{tabular}{l|ccccccc}\n",
      "\\hline\n",
      "Modification & bert & gpt2 & t5 & gpt4o & claude & llama & average \\\\\n",
      "\\hline\n",
      "\\textbf{B: Tem} & 2.92 & 4.08 & 3.33 & 2.85 & 4.23 & 1.45 & \\cellcolor{brown!7} 3.14 \\\\\n",
      "\\textbf{B: Geo} & 4.30 & 6.52 & 2.17 & 4.08 & 2.25 & 2.39 & \\cellcolor{brown!8} 3.62 \\\\\n",
      "\\textbf{B: Len} & 5.61 & 3.48 & 2.95 & 4.29 & 4.45 & 1.81 & \\cellcolor{brown!9} 3.76 \\\\\n",
      "\\hline\n",
      "\\textbf{O: Spell} & 5.53 & 2.07 & 1.44 & 1.23 & 2.52 & 2.18 & \\cellcolor{brown!5} 2.49 \\\\\n",
      "\\textbf{O: Cap} & 2.65 & 11.33 & 6.13 & 1.11 & 3.14 & 4.75 & \\cellcolor{brown!11} 4.85 \\\\\n",
      "\\textbf{O: Punc} & 3.06 & 2.23 & 1.44 & 2.57 & 5.26 & 4.44 & \\cellcolor{brown!7} 3.17 \\\\\n",
      "\\hline\n",
      "\\textbf{M: Deri} & 2.07 & 2.87 & 1.93 & 1.73 & 3.29 & 1.69 & \\cellcolor{brown!5} 2.26 \\\\\n",
      "\\textbf{M: Com} & 3.03 & 2.11 & 2.58 & 2.71 & 1.97 & 0.94 & \\cellcolor{brown!5} 2.22 \\\\\n",
      "\\hline\n",
      "\\textbf{Sx: Voice} & 4.41 & 3.64 & 6.82 & 3.69 & 2.28 & 1.82 & \\cellcolor{brown!9} 3.78 \\\\\n",
      "\\textbf{Sx: Gra} & 5.98 & 5.42 & 4.42 & 6.06 & 2.14 & 2.87 & \\cellcolor{brown!10} 4.48 \\\\\n",
      "\\textbf{Sx: Conj} & 1.76 & 2.47 & 3.24 & 3.28 & 1.26 & 1.16 & \\cellcolor{brown!5} 2.19 \\\\\n",
      "\\hline\n",
      "\\textbf{Sm: Con} & 3.30 & 4.99 & 4.05 & 2.87 & 1.59 & 2.15 & \\cellcolor{brown!7} 3.16 \\\\\n",
      "\\hline\n",
      "\\textbf{P: Neg} & 13.09 & 11.48 & 13.49 & 11.88 & 11.53 & 13.48 & \\cellcolor{brown!30} 12.49 \\\\\n",
      "\\textbf{P: Disc} & 2.90 & 3.98 & 1.73 & 1.02 & 1.68 & 2.63 & \\cellcolor{brown!5} 2.32 \\\\\n",
      "\\textbf{P: Senti} & 3.00 & 5.79 & 2.60 & 0.85 & 2.46 & 3.65 & \\cellcolor{brown!7} 3.06 \\\\\n",
      "\\hline\n",
      "\\textbf{G: Cas} & 10.98 & 4.03 & 5.37 & 4.29 & 4.02 & 3.43 & \\cellcolor{brown!12} 5.35 \\\\\n",
      "\\textbf{G: Dial} & 7.86 & 7.39 & 7.10 & 3.94 & 3.80 & 2.74 & \\cellcolor{brown!13} 5.47 \\\\\n",
      "\\hline\n",
      "\\textbf{Average} & \\cellcolor{purple!29} 4.85 & \\cellcolor{purple!30} 4.93 & \\cellcolor{purple!25} 4.16 & \\cellcolor{purple!20} 3.44 & \\cellcolor{purple!20} 3.41 & \\cellcolor{purple!19} 3.15 & \\cellcolor{yellow!24} 3.99 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Average absolute percentage change in performance across different modifications and models}\n",
      "\\label{tab:modifications}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "# Read all results files\n",
    "files_and_tasks = {\n",
    "    '../pretrained_language_models/ner/ner_modification_results_combined.csv': 'NER',\n",
    "    '../pretrained_language_models/coreference_resolution/tmp/coreference_combined_results.csv': 'Coreference', \n",
    "    '../pretrained_language_models/dialogue_contradiction_detection/tmp/dialogue_combined_results.csv': 'Dialogue',\n",
    "    '../pretrained_language_models/sentiment_analysis/sentiment_analysis_results.csv': 'Sentiment'\n",
    "}\n",
    "\n",
    "# Read and combine all dataframes\n",
    "dfs = []\n",
    "for file_path, task in files_and_tasks.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'mean_f1_pct_change' in df.columns:\n",
    "        df = df.rename(columns={'mean_f1_pct_change': 'pct_difference'})\n",
    "    if 'percentage_diff' in df.columns:\n",
    "        df = df.rename(columns={'percentage_diff': 'pct_difference'})\n",
    "    if 'pct_diff' in df.columns:\n",
    "        df = df.rename(columns={'pct_diff': 'pct_difference'})\n",
    "    df['task'] = task\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "# Keep only the specified columns\n",
    "combined_df = combined_df[['model', 'task', 'modification', 'pct_difference']]\n",
    "\n",
    "# Normalize model names\n",
    "def normalize_model_name(model):\n",
    "    model = model.lower()\n",
    "    if 'bert' in model:\n",
    "        return 'bert'\n",
    "    elif 'gpt-2' in model or 'gpt2' in model:\n",
    "        return 'gpt2'\n",
    "    elif 't5' in model:\n",
    "        return 't5'\n",
    "    elif 'gpt4' in model or 'gpt-4' in model:\n",
    "        return 'gpt4o'\n",
    "    elif 'claude' in model:\n",
    "        return 'claude'\n",
    "    elif 'llama' in model:\n",
    "        return 'llama'\n",
    "    return model\n",
    "print(combined_df['model'].unique())\n",
    "combined_df['model'] = combined_df['model'].apply(normalize_model_name)\n",
    "print(combined_df['model'].unique())\n",
    "# Take absolute value of pct_difference\n",
    "combined_df['abs_pct_diff'] = combined_df['pct_difference'].abs()\n",
    "combined_df.to_csv('combined_df.csv', index=False)\n",
    "# Create pivot table with modifications as rows and models as columns\n",
    "# Using mean since we want to see average impact across all tasks\n",
    "final_df = pd.pivot_table(combined_df, \n",
    "                         values='abs_pct_diff',\n",
    "                         index='modification',\n",
    "                         columns='model',\n",
    "                         aggfunc='mean').round(4)\n",
    "\n",
    "# Add mean across models for each modification\n",
    "final_df['average'] = final_df.mean(axis=1).round(4)\n",
    "\n",
    "# Add mean across modifications for each model\n",
    "model_avgs = final_df.mean().round(4)\n",
    "final_df.loc['avg_across_mods'] = model_avgs\n",
    "\n",
    "# Create LaTeX table\n",
    "mod_mapping = {\n",
    "    'temporal_bias': 'B: Tem',\n",
    "    'geographical_bias': 'B: Geo', \n",
    "    'length_bias': 'B: Len',\n",
    "    'typo_bias': 'O: Spell',\n",
    "    'capitalization': 'O: Cap',\n",
    "    'punctuation': 'O: Punc',\n",
    "    'derivation': 'M: Deri',\n",
    "    'compound_word': 'M: Com',\n",
    "    'active_to_passive': 'Sx: Voice',\n",
    "    'grammatical_role': 'Sx: Gra',\n",
    "    'coordinating_conjunction': 'Sx: Conj',\n",
    "    'concept_replacement': 'Sm: Con',\n",
    "    'negation': 'P: Neg',\n",
    "    'discourse': 'P: Disc',\n",
    "    'sentiment': 'P: Senti',\n",
    "    'casual': 'G: Cas',\n",
    "    'dialectal': 'G: Dial'\n",
    "}\n",
    "\n",
    "model_order = ['bert', 'gpt2', 't5', 'gpt4o', 'claude', 'llama', 'average']\n",
    "\n",
    "# Reorder and rename modifications\n",
    "final_df = final_df[model_order]\n",
    "final_df.index = [mod_mapping.get(idx, idx) for idx in final_df.index]\n",
    "\n",
    "latex_table = \"\\\\begin{table}[t]\\n\\\\centering\\n\\\\begin{tabular}{l|\" + \"c\"*len(model_order) + \"}\\n\"\n",
    "latex_table += \"\\\\hline\\nModification & \" + \" & \".join(model_order) + \" \\\\\\\\\\n\\\\hline\\n\"\n",
    "\n",
    "current_prefix = None\n",
    "\n",
    "# Get max values for last column and row for scaling\n",
    "avg_col_max = final_df['average'].max()\n",
    "avg_row_max = final_df.loc['avg_across_mods'].max()\n",
    "\n",
    "for idx in mod_mapping.values():\n",
    "    if idx in final_df.index:\n",
    "        # Add hline when prefix changes\n",
    "        prefix = idx.split(':')[0]\n",
    "        if current_prefix is not None and prefix != current_prefix:\n",
    "            latex_table += \"\\\\hline\\n\"\n",
    "        current_prefix = prefix\n",
    "        \n",
    "        # Bold the modification name\n",
    "        bold_idx = f\"\\\\textbf{{{idx}}}\"\n",
    "        row = []\n",
    "        \n",
    "        for col in model_order:\n",
    "            val = final_df.loc[idx, col]\n",
    "            if col == 'average':  # For the last column\n",
    "                intensity = int((val / avg_col_max) * 30)  # Scale based on average column max\n",
    "                row.append(f\"\\\\cellcolor{{brown!{intensity}}} {val:.2f}\")\n",
    "            else:\n",
    "                row.append(f\"{val:.2f}\")\n",
    "        latex_table += f\"{bold_idx} & \" + \" & \".join(row) + \" \\\\\\\\\\n\"\n",
    "\n",
    "latex_table += \"\\\\hline\\n\"\n",
    "avg_row = []\n",
    "for col in model_order:\n",
    "    val = final_df.loc['avg_across_mods', col]\n",
    "    if col == 'average':\n",
    "        intensity = int((val / avg_row_max) * 30)  # Scale based on average row max\n",
    "        avg_row.append(f\"\\\\cellcolor{{yellow!{intensity}}} {val:.2f}\")\n",
    "    else:\n",
    "        intensity = int((val / avg_row_max) * 30)  # Scale based on average row max\n",
    "        avg_row.append(f\"\\\\cellcolor{{purple!{intensity}}} {val:.2f}\")\n",
    "latex_table += \"\\\\textbf{Average} & \" + \" & \".join(avg_row) + \" \\\\\\\\\\n\"\n",
    "latex_table += \"\\\\hline\\n\\\\end{tabular}\\n\"\n",
    "latex_table += \"\\\\caption{Average absolute percentage change in performance across different modifications and models}\\n\"\n",
    "latex_table += \"\\\\label{tab:modifications}\\n\\\\end{table}\"\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.075"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4.7+3.6+2.4+5.6)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
