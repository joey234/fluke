{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "# Read all modification files\n",
    "modifications = [f.split('_modif.txt')[0] for f in os.listdir('../../eval/PLM_results/sentiment_analysis/BERT') if f.endswith('_modif.txt')]\n",
    "\n",
    "bert_predictions = {}\n",
    "gpt2_predictions = {}\n",
    "t5_predictions = {}\n",
    "model_predictions = {}\n",
    "\n",
    "# Load BERT predictions\n",
    "for mod in modifications:\n",
    "    bert_predictions[f'{mod}_modif'] = pd.read_csv(f'../../eval/PLM_results/sentiment_analysis/BERT/{mod}_modif.txt', header=None)[0].tolist()\n",
    "    bert_predictions[f'{mod}_ori'] = pd.read_csv(f'../../eval/PLM_results/sentiment_analysis/BERT/{mod}_ori.txt', header=None)[0].tolist()\n",
    "    if len(bert_predictions[f'{mod}_modif']) != len(bert_predictions[f'{mod}_ori']):\n",
    "        print(mod)\n",
    "# Load GPT2 predictions  \n",
    "for mod in modifications:\n",
    "    gpt2_predictions[f'{mod}_modif'] = pd.read_csv(f'../../eval/PLM_results/sentiment_analysis/GPT2/{mod}_modif.txt', header=None)[0].tolist()\n",
    "    gpt2_predictions[f'{mod}_ori'] = pd.read_csv(f'../../eval/PLM_results/sentiment_analysis/GPT2/{mod}_ori.txt', header=None)[0].tolist()\n",
    "    if len(gpt2_predictions[f'{mod}_modif']) != len(gpt2_predictions[f'{mod}_ori']):\n",
    "        print(mod)\n",
    "# Load T5 predictions\n",
    "for mod in modifications:\n",
    "    t5_predictions[f'{mod}_modif'] = pd.read_csv(f'../../eval/PLM_results/sentiment_analysis/T5/{mod}_modif.txt', header=None)[0].tolist()\n",
    "    t5_predictions[f'{mod}_ori'] = pd.read_csv(f'../../eval/PLM_results/sentiment_analysis/T5/{mod}_ori.txt', header=None)[0].tolist()\n",
    "\n",
    "# Read the original labels and model predictions\n",
    "labels = {}\n",
    "model_names = ['gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "for mod in modifications:\n",
    "    labels[mod] = pd.read_csv(f'../../eval/results/sa/gpt4o-0shot-{mod}_100.csv')\n",
    "    for model in model_names:\n",
    "        key = f'{model}_{mod}'\n",
    "        model_predictions[key] = pd.read_csv(f'../../eval/results/sa/{model}-0shot-{mod}_100.csv')\n",
    "        if len(model_predictions[key]) != len(labels[mod]): \n",
    "            print(mod, model)\n",
    "\n",
    "# Create comparison files for BERT, GPT2, T5\n",
    "for mod in modifications:\n",
    "    # BERT comparison\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs('tmp/bert', exist_ok=True)\n",
    "    os.makedirs('tmp/gpt2', exist_ok=True) \n",
    "    os.makedirs('tmp/t5', exist_ok=True)\n",
    "    # print(mod)\n",
    "    bert_comp = pd.DataFrame({\n",
    "        'original_text': labels[mod]['original_text'],\n",
    "        'modified_text': labels[mod]['text'],\n",
    "        'original_label': labels[mod]['original_label'],\n",
    "        'original_pred': bert_predictions[f'{mod}_ori'],\n",
    "        'modified_label': labels[mod]['modified_label'],\n",
    "        'modified_pred': bert_predictions[f'{mod}_modif'],\n",
    "    })\n",
    "    bert_comp.to_csv(f'tmp/bert/{mod}_comparison.csv', index=False)\n",
    "    \n",
    "    # GPT2 comparison  \n",
    "    gpt2_comp = pd.DataFrame({\n",
    "        'original_text': labels[mod]['original_text'],\n",
    "        'modified_text': labels[mod]['text'],\n",
    "        'original_label': labels[mod]['original_label'],\n",
    "        'original_pred': gpt2_predictions[f'{mod}_ori'],\n",
    "        'modified_label': labels[mod]['modified_label'],\n",
    "        'modified_pred': gpt2_predictions[f'{mod}_modif'],\n",
    "    })\n",
    "    gpt2_comp.to_csv(f'tmp/gpt2/{mod}_comparison.csv', index=False)\n",
    "    \n",
    "    # T5 comparison\n",
    "    t5_comp = pd.DataFrame({\n",
    "        'original_text': labels[mod]['original_text'],\n",
    "        'modified_text': labels[mod]['text'],\n",
    "        'original_label': labels[mod]['original_label'],\n",
    "        'original_pred': t5_predictions[f'{mod}_ori'],\n",
    "        'modified_label': labels[mod]['modified_label'],\n",
    "        'modified_pred': t5_predictions[f'{mod}_modif'],\n",
    "    })\n",
    "    t5_comp.to_csv(f'tmp/t5/{mod}_comparison.csv', index=False)\n",
    "# Calculate results for all models\n",
    "results = []\n",
    "negation_results = []\n",
    "\n",
    "def get_significance_level(pvalue):\n",
    "    if pvalue < 0.01:\n",
    "        return \"**\"   # Very significant\n",
    "    elif pvalue < 0.05:\n",
    "        return \"*\"    # Significant\n",
    "    elif pvalue < 0.1:\n",
    "        return \".\"    # Weakly significant\n",
    "    else:\n",
    "        return \"ns\"   # Not significant\n",
    "\n",
    "# Process BERT\n",
    "for mod in modifications:\n",
    "    bert_orig = np.array([1 if p == l else 0 for p, l in zip(bert_predictions[f'{mod}_ori'], labels[mod]['original_label'])])\n",
    "    bert_mod = np.array([1 if p == l else 0 for p, l in zip(bert_predictions[f'{mod}_modif'], labels[mod]['modified_label'])])\n",
    "    bert_orig_acc = np.round(bert_orig.mean(), decimals=3)\n",
    "    bert_mod_acc = np.round(bert_mod.mean(), decimals=3)\n",
    "    bert_pct_diff = np.round(((bert_mod_acc - bert_orig_acc) / bert_orig_acc) * 100, decimals=1)\n",
    "    \n",
    "    if np.array_equal(bert_orig, bert_mod):\n",
    "        wilcoxon_pvalue = 1.0\n",
    "        mannwhitney_pvalue = 1.0\n",
    "    else:\n",
    "        _, wilcoxon_pvalue = stats.wilcoxon(bert_orig, bert_mod)\n",
    "        _, mannwhitney_pvalue = stats.mannwhitneyu(bert_orig, bert_mod, alternative='two-sided')\n",
    "    pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "    significance = get_significance_level(pvalue)\n",
    "\n",
    "    results.append({\n",
    "        'model': 'bert',\n",
    "        'modification': mod,\n",
    "        'original_acc': bert_orig_acc,\n",
    "        'modified_acc': bert_mod_acc,\n",
    "        'pct_diff': bert_pct_diff,\n",
    "        'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "        'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "        'pvalue': pvalue,\n",
    "        'significance': significance,\n",
    "        'significant': wilcoxon_pvalue < 0.05 or mannwhitney_pvalue < 0.05\n",
    "    })\n",
    "\n",
    "    if mod == 'negation':\n",
    "        # Get negation types from gpt4o data\n",
    "        neg_types = model_predictions['gpt4o_negation']['type'].tolist()\n",
    "        for neg_type in set(neg_types):\n",
    "            type_indices = [i for i, t in enumerate(neg_types) if t == neg_type]\n",
    "            bert_orig_type = bert_orig[type_indices]\n",
    "            bert_mod_type = bert_mod[type_indices]\n",
    "            \n",
    "            bert_orig_acc_type = np.round(bert_orig_type.mean(), decimals=3)\n",
    "            bert_mod_acc_type = np.round(bert_mod_type.mean(), decimals=3)\n",
    "            bert_pct_diff_type = np.round(((bert_mod_acc_type - bert_orig_acc_type) / bert_orig_acc_type) * 100, decimals=1)\n",
    "            \n",
    "            if np.array_equal(bert_orig_type, bert_mod_type):\n",
    "                wilcoxon_pvalue = 1.0\n",
    "                mannwhitney_pvalue = 1.0\n",
    "            else:\n",
    "                _, wilcoxon_pvalue = stats.wilcoxon(bert_orig_type, bert_mod_type)\n",
    "                _, mannwhitney_pvalue = stats.mannwhitneyu(bert_orig_type, bert_mod_type, alternative='two-sided')\n",
    "            pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "            significance = get_significance_level(pvalue)\n",
    "            \n",
    "            negation_results.append({\n",
    "                'model': 'bert',\n",
    "                'modification': f'{neg_type}',\n",
    "                'original_acc': bert_orig_acc_type,\n",
    "                'modified_acc': bert_mod_acc_type,\n",
    "                'pct_diff': bert_pct_diff_type,\n",
    "                'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "                'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "                'pvalue': pvalue,\n",
    "                'significance': significance,\n",
    "                'significant': pvalue < 0.05\n",
    "            })\n",
    "    \n",
    "\n",
    "# Process GPT2\n",
    "for mod in modifications:\n",
    "    gpt2_orig = np.array([1 if p == l else 0 for p, l in zip(gpt2_predictions[f'{mod}_ori'], labels[mod]['original_label'])])\n",
    "    gpt2_mod = np.array([1 if p == l else 0 for p, l in zip(gpt2_predictions[f'{mod}_modif'], labels[mod]['modified_label'])])\n",
    "    gpt2_orig_acc = np.round(gpt2_orig.mean(), decimals=3)\n",
    "    gpt2_mod_acc = np.round(gpt2_mod.mean(), decimals=3)\n",
    "    gpt2_pct_diff = np.round(((gpt2_mod_acc - gpt2_orig_acc) / gpt2_orig_acc) * 100, decimals=1)\n",
    "    \n",
    "    if np.array_equal(gpt2_orig, gpt2_mod):\n",
    "        wilcoxon_pvalue = 1.0\n",
    "        mannwhitney_pvalue = 1.0\n",
    "    else:\n",
    "        _, wilcoxon_pvalue = stats.wilcoxon(gpt2_orig, gpt2_mod)\n",
    "        _, mannwhitney_pvalue = stats.mannwhitneyu(gpt2_orig, gpt2_mod, alternative='two-sided')\n",
    "    pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "    significance = get_significance_level(pvalue)\n",
    "    results.append({\n",
    "        'model': 'gpt2',\n",
    "        'modification': mod,\n",
    "        'original_acc': gpt2_orig_acc,\n",
    "        'modified_acc': gpt2_mod_acc,\n",
    "        'pct_diff': gpt2_pct_diff,\n",
    "        'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "        'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "        'pvalue': pvalue,\n",
    "        'significance': significance,\n",
    "        'significant': wilcoxon_pvalue < 0.05 or mannwhitney_pvalue < 0.05\n",
    "    })\n",
    "\n",
    "    if mod == 'negation':\n",
    "        # Get negation types from gpt4o data\n",
    "        neg_types = model_predictions['gpt4o_negation']['type'].tolist()\n",
    "        for neg_type in set(neg_types):\n",
    "            type_indices = [i for i, t in enumerate(neg_types) if t == neg_type]\n",
    "            gpt2_orig_type = gpt2_orig[type_indices]\n",
    "            gpt2_mod_type = gpt2_mod[type_indices]\n",
    "            \n",
    "            gpt2_orig_acc_type = np.round(gpt2_orig_type.mean(), decimals=3)\n",
    "            gpt2_mod_acc_type = np.round(gpt2_mod_type.mean(), decimals=3)\n",
    "            gpt2_pct_diff_type = np.round(((gpt2_mod_acc_type - gpt2_orig_acc_type) / gpt2_orig_acc_type) * 100, decimals=1)\n",
    "            \n",
    "            if np.array_equal(gpt2_orig_type, gpt2_mod_type):\n",
    "                wilcoxon_pvalue = 1.0\n",
    "                mannwhitney_pvalue = 1.0\n",
    "            else:\n",
    "                _, wilcoxon_pvalue = stats.wilcoxon(gpt2_orig_type, gpt2_mod_type)\n",
    "                _, mannwhitney_pvalue = stats.mannwhitneyu(gpt2_orig_type, gpt2_mod_type, alternative='two-sided')\n",
    "            pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "            significance = get_significance_level(pvalue)\n",
    "            \n",
    "            negation_results.append({\n",
    "                'model': 'gpt2',\n",
    "                'modification': f'{neg_type}',\n",
    "                'original_acc': gpt2_orig_acc_type,\n",
    "                'modified_acc': gpt2_mod_acc_type,\n",
    "                'pct_diff': gpt2_pct_diff_type,\n",
    "                'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "                'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "                'pvalue': pvalue,\n",
    "                'significance': significance,\n",
    "                'significant': pvalue < 0.05\n",
    "            })\n",
    "    \n",
    "\n",
    "# Process T5\n",
    "for mod in modifications:\n",
    "    t5_orig = np.array([1 if p == l else 0 for p, l in zip(t5_predictions[f'{mod}_ori'], labels[mod]['original_label'])])\n",
    "    t5_mod = np.array([1 if p == l else 0 for p, l in zip(t5_predictions[f'{mod}_modif'], labels[mod]['modified_label'])])\n",
    "    t5_orig_acc = np.round(t5_orig.mean(), decimals=3)\n",
    "    t5_mod_acc = np.round(t5_mod.mean(), decimals=3)\n",
    "    t5_pct_diff = np.round(((t5_mod_acc - t5_orig_acc) / t5_orig_acc) * 100, decimals=1)\n",
    "    \n",
    "    if np.array_equal(t5_orig, t5_mod):\n",
    "        wilcoxon_pvalue = 1.0\n",
    "        mannwhitney_pvalue = 1.0\n",
    "    else:\n",
    "        _, wilcoxon_pvalue = stats.wilcoxon(t5_orig, t5_mod)\n",
    "        _, mannwhitney_pvalue = stats.mannwhitneyu(t5_orig, t5_mod, alternative='two-sided')\n",
    "    pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "    significance = get_significance_level(pvalue)\n",
    "    results.append({\n",
    "        'model': 't5',\n",
    "        'modification': mod,\n",
    "        'original_acc': t5_orig_acc,\n",
    "        'modified_acc': t5_mod_acc,\n",
    "        'pct_diff': t5_pct_diff,\n",
    "        'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "        'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "        'pvalue': pvalue,\n",
    "        'significance': significance,\n",
    "        'significant': wilcoxon_pvalue < 0.05 or mannwhitney_pvalue < 0.05\n",
    "    })\n",
    "\n",
    "    if mod == 'negation':\n",
    "        # Get negation types from gpt4o data\n",
    "        neg_types = model_predictions['gpt4o_negation']['type'].tolist()\n",
    "        for neg_type in set(neg_types):\n",
    "            type_indices = [i for i, t in enumerate(neg_types) if t == neg_type]\n",
    "            t5_orig_type = t5_orig[type_indices]\n",
    "            t5_mod_type = t5_mod[type_indices]\n",
    "            \n",
    "            t5_orig_acc_type = np.round(t5_orig_type.mean(), decimals=3)\n",
    "            t5_mod_acc_type = np.round(t5_mod_type.mean(), decimals=3)\n",
    "            t5_pct_diff_type = np.round(((t5_mod_acc_type - t5_orig_acc_type) / t5_orig_acc_type) * 100, decimals=1)\n",
    "            \n",
    "            if np.array_equal(t5_orig_type, t5_mod_type):\n",
    "                wilcoxon_pvalue = 1.0\n",
    "                mannwhitney_pvalue = 1.0\n",
    "            else:\n",
    "                _, wilcoxon_pvalue = stats.wilcoxon(t5_orig_type, t5_mod_type)\n",
    "                _, mannwhitney_pvalue = stats.mannwhitneyu(t5_orig_type, t5_mod_type, alternative='two-sided')\n",
    "            pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "            significance = get_significance_level(pvalue)\n",
    "            \n",
    "            negation_results.append({\n",
    "                'model': 't5',\n",
    "                'modification': f'{neg_type}',\n",
    "                'original_acc': t5_orig_acc_type,\n",
    "                'modified_acc': t5_mod_acc_type,\n",
    "                'pct_diff': t5_pct_diff_type,\n",
    "                'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "                'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "                'pvalue': pvalue,\n",
    "                'significance': significance,\n",
    "                'significant': pvalue < 0.05\n",
    "            })\n",
    "    \n",
    "\n",
    "# Process other models\n",
    "for model in model_names:\n",
    "    for mod in modifications:\n",
    "        model_data = model_predictions[f'{model}_{mod}']\n",
    "        model_orig = np.array([1 if p == l else 0 for p, l in zip(model_data['original_pred'], labels[mod]['original_label'])])\n",
    "        model_mod = np.array([1 if p == l else 0 for p, l in zip(model_data['modified_pred'], labels[mod]['modified_label'])])\n",
    "        model_orig_acc = np.round(model_orig.mean(), decimals=3)\n",
    "        model_mod_acc = np.round(model_mod.mean(), decimals=3)\n",
    "        model_pct_diff = np.round(((model_mod_acc - model_orig_acc) / model_orig_acc) * 100, decimals=1)\n",
    "        \n",
    "        if np.array_equal(model_orig, model_mod):\n",
    "            wilcoxon_pvalue = 1.0\n",
    "            mannwhitney_pvalue = 1.0\n",
    "        else:\n",
    "            _, wilcoxon_pvalue = stats.wilcoxon(model_orig, model_mod)\n",
    "            _, mannwhitney_pvalue = stats.mannwhitneyu(model_orig, model_mod, alternative='two-sided')\n",
    "        pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "        significance = get_significance_level(pvalue)\n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'modification': mod,\n",
    "            'original_acc': model_orig_acc,\n",
    "            'modified_acc': model_mod_acc,\n",
    "            'pct_diff': model_pct_diff,\n",
    "            'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "            'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "            'pvalue': pvalue,\n",
    "            'significance': significance,\n",
    "            'significant': pvalue < 0.05\n",
    "        })\n",
    "\n",
    "        if mod == 'negation':\n",
    "            # Get negation types\n",
    "            neg_types = model_data['type'].tolist()\n",
    "            for neg_type in set(neg_types):\n",
    "                type_indices = [i for i, t in enumerate(neg_types) if t == neg_type]\n",
    "                model_orig_type = model_orig[type_indices]\n",
    "                model_mod_type = model_mod[type_indices]\n",
    "                \n",
    "                model_orig_acc_type = np.round(model_orig_type.mean(), decimals=3)\n",
    "                model_mod_acc_type = np.round(model_mod_type.mean(), decimals=3)\n",
    "                model_pct_diff_type = np.round(((model_mod_acc_type - model_orig_acc_type) / model_orig_acc_type) * 100, decimals=1)\n",
    "                \n",
    "                if np.array_equal(model_orig_type, model_mod_type):\n",
    "                    wilcoxon_pvalue = 1.0\n",
    "                    mannwhitney_pvalue = 1.0\n",
    "                else:\n",
    "                    _, wilcoxon_pvalue = stats.wilcoxon(model_orig_type, model_mod_type)\n",
    "                    _, mannwhitney_pvalue = stats.mannwhitneyu(model_orig_type, model_mod_type, alternative='two-sided')\n",
    "                pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "                significance = get_significance_level(pvalue)\n",
    "                \n",
    "                negation_results.append({\n",
    "                    'model': model,\n",
    "                    'modification': f'{neg_type}',\n",
    "                    'original_acc': model_orig_acc_type,\n",
    "                    'modified_acc': model_mod_acc_type,\n",
    "                    'pct_diff': model_pct_diff_type,\n",
    "                    'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "                    'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "                    'pvalue': pvalue,\n",
    "                    'significance': significance,\n",
    "                    'significant': pvalue < 0.05\n",
    "                })\n",
    "        \n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "negation_results_df = pd.DataFrame(negation_results)\n",
    "\n",
    "results_df.to_csv('sentiment_analysis_results.csv', index=False)\n",
    "negation_results_df.to_csv('sentiment_analysis_negation_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table saved to ner_results_table.tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "modification_order = [\n",
    "    \"B: Tem\", \"B: Geo\", \"B: Len\", \n",
    "    \"O: Spell\", \"O: Cap\", \"O: Punc\",\n",
    "    \"M: Deri\", \"M: Com\",\n",
    "    \"Sx: Voice\", \"Sx: Gra\", \"Sx: Conj\", \n",
    "    \"Sm: Con\",\n",
    "    \"P: Neg\", \"P: Disc\", \"P: Senti\",\n",
    "    \"G: Cas\", \"G: Dial\"\n",
    "]\n",
    "\n",
    "# Read the combined results\n",
    "df = pd.read_csv('sentiment_analysis_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "mod_mapping = {\n",
    "    'temporal_bias': 'B: Tem',\n",
    "    'geographical_bias': 'B: Geo', \n",
    "    'length_bias': 'B: Len',\n",
    "    'typo_bias': 'O: Spell',\n",
    "    'capitalization': 'O: Cap',\n",
    "    'punctuation': 'O: Punc',\n",
    "    'derivation': 'M: Deri',\n",
    "    'compound_word': 'M: Com',\n",
    "    'active_to_passive': 'Sx: Voice',\n",
    "    'grammatical_role': 'Sx: Gra',\n",
    "    'coordinating_conjunction': 'Sx: Conj',\n",
    "    'concept_replacement': 'Sm: Con',\n",
    "    'negation': 'P: Neg',\n",
    "    'discourse': 'P: Disc',\n",
    "    'sentiment': 'P: Senti',\n",
    "    'casual': 'G: Cas',\n",
    "    'dialectal': 'G: Dial'\n",
    "}\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Define model order\n",
    "model_order = ['bert', 'gpt2', 't5', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='pct_diff')\n",
    "significance_df = df.pivot(index='modification', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows and columns\n",
    "pivot_df = pivot_df.reindex(modification_order, axis=0)\n",
    "pivot_df = pivot_df.reindex(model_order, axis=1)\n",
    "significance_df = significance_df.reindex(modification_order, axis=0)\n",
    "significance_df = significance_df.reindex(model_order, axis=1)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, significance):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if significance == '**':\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif significance == '*':\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif significance == '.':\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if significance == '**':\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif significance == '*':\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif significance == '.':\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(pivot_df.columns) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in pivot_df.columns]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(val, significance_df.loc[idx, col]) for col, val in row.items()]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open('sa_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to ner_results_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model   modification  original_acc  modified_acc  pct_diff  wilcoxon_pvalue  \\\n",
      "1  bert  temporal_bias          0.92          0.89      -3.3         0.179712   \n",
      "\n",
      "   mannwhitney_pvalue    pvalue significance  significant  \n",
      "1            0.471985  0.179712           ns        False  \n",
      "Results saved to sentiment_analysis_results_df.csv and negation_results_df.csv\n"
     ]
    }
   ],
   "source": [
    "# Load results from CSV\n",
    "results_df = pd.read_csv('sentiment_analysis_results.csv')\n",
    "negation_df = pd.read_csv('sentiment_analysis_negation_results.csv')\n",
    "\n",
    "# Create lists of unique modifications and models\n",
    "modification_order = list(mod_mapping.keys())\n",
    "negation_order = ['verbal', 'lexical', 'double', 'approximate', 'absolute']\n",
    "model_order = ['bert', 'gpt2', 't5', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Create empty DataFrame with multi-level columns\n",
    "columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "results_df_pivot = pd.DataFrame(index=modification_order, columns=columns)\n",
    "negation_df_pivot = pd.DataFrame(index=negation_order, columns=columns)\n",
    "\n",
    "# Fill DataFrame for sentiment analysis results\n",
    "for mod in modification_order:\n",
    "    for model in model_order:\n",
    "        row = results_df[(results_df['modification'] == mod) & (results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            results_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'diff')] = row['pct_diff'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "\n",
    "# Fill DataFrame for negation results\n",
    "for mod in negation_order:\n",
    "    for model in model_order:\n",
    "        row = negation_df[(negation_df['modification'] == mod) & (negation_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            negation_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            negation_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0]\n",
    "            negation_df_pivot.loc[mod, (model, 'diff')] = row['pct_diff'].values[0]\n",
    "\n",
    "# Save to CSV\n",
    "results_df_pivot.to_csv('sentiment_analysis_results_df.csv')\n",
    "negation_df_pivot.to_csv('negation_results_df.csv')\n",
    "\n",
    "print(\"Results saved to sentiment_analysis_results_df.csv and negation_results_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model         bert  claude-3-5-sonnet  gpt2  gpt4o  llama    t5\n",
      "modification                                                   \n",
      "Verbal       -17.7              -11.8 -23.6  -22.2   -5.5 -33.3\n",
      "Lexical       -6.7                0.0   0.0    7.2    0.0 -20.0\n",
      "Double       -25.0              -36.8 -11.1  -22.2  -10.0 -30.0\n",
      "Approximate   -8.7              -18.2 -13.6  -10.0  -18.2 -25.0\n",
      "Absolute     -33.3                0.0 -20.0   -8.4  -14.3 -20.0\n",
      "LaTeX table saved to sentiment_analysis_negation_type_results_table.tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "df = pd.read_csv('sentiment_analysis_negation_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "negation_order= ['Verbal', 'Lexical', 'Double', 'Approximate', 'Absolute']\n",
    "mod_mapping = {\n",
    "    'verbal': 'Verbal',\n",
    "    'lexical': 'Lexical',\n",
    "    'double': 'Double',\n",
    "    'approximate': 'Approximate',\n",
    "    'absolute': 'Absolute',\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "model_order = ['bert', 'gpt2', 't5', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='pct_diff')\n",
    "p_values = df.pivot(index='modification', columns='model', values='pvalue')\n",
    "significance = df.pivot(index='modification', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows according to modification_order\n",
    "pivot_df = pivot_df.reindex(negation_order)\n",
    "p_values = p_values.reindex(negation_order)\n",
    "significance = significance.reindex(negation_order)\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(row[col], significance.loc[idx, col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open('sentiment_analysis_negation_type_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to sentiment_analysis_negation_type_results_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
