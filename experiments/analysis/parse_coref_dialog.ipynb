{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1203, 1292, 1393, 85, 185, 837, 783, 830, 356, 518, 315, 1492, 849, 232, 734, 671, 1486, 1478, 894, 601, 312, 1154, 211, 1225, 438, 817, 1451, 117, 89, 189, 37, 828, 965, 405, 794, 1386, 1339, 989, 1300, 1104, 1400, 27, 55, 534, 1381, 724, 547, 884, 285, 130, 124, 747, 1449, 1012, 114, 1219, 1056, 207, 274, 679, 845, 1433, 2, 553, 131, 1353, 1084, 684, 121, 436, 1232, 821, 1448, 120, 1141, 1191, 1312, 159, 77, 155, 87, 963, 376, 796, 198, 1187, 1308, 887, 902, 602, 964, 659, 871, 800, 1223, 772, 323, 1056, 1066, 1012]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import decimal\n",
    "\n",
    "# Load derivation data to get indices\n",
    "derivation_path = Path('../data/modified_data/coref/derivation_100.json')\n",
    "with open(derivation_path) as f:\n",
    "    derivation_data = json.load(f)\n",
    "\n",
    "# Extract indices from derivation data\n",
    "derivation_indices = [item['index'] for item in derivation_data]\n",
    "print(derivation_indices)\n",
    "# Create mapping between modification types and their filenames\n",
    "modification_mapping = {\n",
    "    'derivation': 'derivation'\n",
    "}\n",
    "\n",
    "# Base path for results\n",
    "base_path = Path('../pretrained_language_models/coreference_resolution/tmp')\n",
    "\n",
    "# Create index mapping for each model and modification\n",
    "index_mappings = {}\n",
    "for model in ['bert-base-cased', 'gpt2', 't5-base']:\n",
    "    index_mappings[model] = {}\n",
    "    model_path = base_path / f'{model}_results'\n",
    "    \n",
    "    for mod_type, filename in modification_mapping.items():\n",
    "        csv_path = model_path / f'{filename}_predictions.csv'\n",
    "        if csv_path.exists():\n",
    "            df = pd.read_csv(csv_path)\n",
    "            # Replace indices with derivation indices\n",
    "            df['index'] = derivation_indices[:len(df)]\n",
    "            # Create new file with replaced indices\n",
    "            output_path = model_path / f'{filename}_predictions.csv'\n",
    "            df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results by modification and model:\n",
      "              model              modification original_acc modified_acc  \\\n",
      "0   bert-base-cased             temporal_bias        0.870        0.870   \n",
      "1              gpt2             temporal_bias        0.850        0.830   \n",
      "2           t5-base             temporal_bias        0.810        0.760   \n",
      "3   bert-base-cased         active_to_passive        0.870        0.900   \n",
      "4              gpt2         active_to_passive        0.840        0.870   \n",
      "5           t5-base         active_to_passive        0.850        0.820   \n",
      "6   bert-base-cased                derivation        0.882        0.882   \n",
      "7              gpt2                derivation        0.828        0.849   \n",
      "8           t5-base                derivation        0.849        0.839   \n",
      "9   bert-base-cased                  negation        0.890        0.670   \n",
      "10             gpt2                  negation        0.880        0.620   \n",
      "11          t5-base                  negation        0.900        0.680   \n",
      "12  bert-base-cased                 sentiment        0.920        0.880   \n",
      "13             gpt2                 sentiment        0.870        0.850   \n",
      "14          t5-base                 sentiment        0.840        0.810   \n",
      "15  bert-base-cased                 dialectal        0.850        0.810   \n",
      "16             gpt2                 dialectal        0.840        0.820   \n",
      "17          t5-base                 dialectal        0.880        0.840   \n",
      "18  bert-base-cased          grammatical_role        0.853        0.765   \n",
      "19             gpt2          grammatical_role        0.838        0.794   \n",
      "20          t5-base          grammatical_role        0.912        0.838   \n",
      "21  bert-base-cased               punctuation        0.860        0.840   \n",
      "22             gpt2               punctuation        0.900        0.880   \n",
      "23          t5-base               punctuation        0.860        0.860   \n",
      "24  bert-base-cased               length_bias        0.840        0.800   \n",
      "25             gpt2               length_bias        0.850        0.830   \n",
      "26          t5-base               length_bias        0.840        0.810   \n",
      "27  bert-base-cased  coordinating_conjunction        0.890        0.880   \n",
      "28             gpt2  coordinating_conjunction        0.900        0.880   \n",
      "29          t5-base  coordinating_conjunction        0.800        0.830   \n",
      "30  bert-base-cased       concept_replacement        0.890        0.860   \n",
      "31             gpt2       concept_replacement        0.900        0.850   \n",
      "32          t5-base       concept_replacement        0.870        0.850   \n",
      "33  bert-base-cased                 typo_bias        0.840        0.880   \n",
      "34             gpt2                 typo_bias        0.890        0.870   \n",
      "35          t5-base                 typo_bias        0.840        0.820   \n",
      "36  bert-base-cased             compound_word        0.890        0.850   \n",
      "37             gpt2             compound_word        0.860        0.860   \n",
      "38          t5-base             compound_word        0.810        0.820   \n",
      "39  bert-base-cased            capitalization        0.927        0.927   \n",
      "40             gpt2            capitalization        0.906        0.865   \n",
      "41          t5-base            capitalization        0.792        0.792   \n",
      "42  bert-base-cased                 discourse        0.885        0.943   \n",
      "43             gpt2                 discourse        0.920        0.897   \n",
      "44          t5-base                 discourse        0.943        0.931   \n",
      "45  bert-base-cased         geographical_bias        0.826        0.728   \n",
      "46             gpt2         geographical_bias        0.826        0.728   \n",
      "47          t5-base         geographical_bias        0.750        0.728   \n",
      "48  bert-base-cased                    casual        0.900        0.820   \n",
      "49             gpt2                    casual        0.930        0.850   \n",
      "50          t5-base                    casual        0.820        0.880   \n",
      "\n",
      "   percentage_diff p_value significance significant  \n",
      "0              0.0   1.000           ns          No  \n",
      "1             -2.4   0.414           ns          No  \n",
      "2             -6.2   0.059            .          No  \n",
      "3              3.4   0.317           ns          No  \n",
      "4              3.6   0.366           ns          No  \n",
      "5             -3.5   0.257           ns          No  \n",
      "6              0.0   1.000           ns          No  \n",
      "7              2.6   0.157           ns          No  \n",
      "8             -1.3   0.317           ns          No  \n",
      "9            -24.7   0.000           **         Yes  \n",
      "10           -29.5   0.000           **         Yes  \n",
      "11           -24.4   0.000           **         Yes  \n",
      "12            -4.3   0.248           ns          No  \n",
      "13            -2.3   0.564           ns          No  \n",
      "14            -3.6   0.366           ns          No  \n",
      "15            -4.7   0.317           ns          No  \n",
      "16            -2.4   0.480           ns          No  \n",
      "17            -4.5   0.157           ns          No  \n",
      "18           -10.3   0.058            .          No  \n",
      "19            -5.3   0.317           ns          No  \n",
      "20            -8.1   0.025            *         Yes  \n",
      "21            -2.3   0.317           ns          No  \n",
      "22            -2.2   0.157           ns          No  \n",
      "23             0.0   1.000           ns          No  \n",
      "24            -4.8   0.206           ns          No  \n",
      "25            -2.4   0.414           ns          No  \n",
      "26            -3.6   0.180           ns          No  \n",
      "27            -1.1   0.655           ns          No  \n",
      "28            -2.2   0.317           ns          No  \n",
      "29             3.7   0.257           ns          No  \n",
      "30            -3.4   0.257           ns          No  \n",
      "31            -5.6   0.025            *         Yes  \n",
      "32            -2.3   0.317           ns          No  \n",
      "33             4.8   0.102           ns          No  \n",
      "34            -2.2   0.414           ns          No  \n",
      "35            -2.4   0.317           ns          No  \n",
      "36            -4.5   0.102           ns          No  \n",
      "37             0.0   1.000           ns          No  \n",
      "38             1.2   0.564           ns          No  \n",
      "39             0.0   1.000           ns          No  \n",
      "40            -4.6   0.102           ns          No  \n",
      "41             0.0   1.000           ns          No  \n",
      "42             6.5   0.025            *         Yes  \n",
      "43            -2.5   0.480           ns          No  \n",
      "44            -1.2   0.564           ns          No  \n",
      "45           -11.8   0.020            *         Yes  \n",
      "46           -11.8   0.020            *         Yes  \n",
      "47            -2.9   0.637           ns          No  \n",
      "48            -8.9   0.021            *         Yes  \n",
      "49            -8.6   0.011            *         Yes  \n",
      "50             7.3   0.083            .          No  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Base path for results\n",
    "base_path = Path('../pretrained_language_models/dialogue_contradiction_detection/tmp')\n",
    "\n",
    "# Function to load and convert predictions to binary\n",
    "def load_predictions(filepath):\n",
    "    with open(filepath) as f:\n",
    "        preds = json.load(f)\n",
    "    for pred in preds.items():\n",
    "        if pred[1] == \"contradictory\":\n",
    "            preds[pred[0]] = 1\n",
    "        elif pred[1] == \"not contradictory\":\n",
    "            preds[pred[0]] = 0\n",
    "    return preds\n",
    "\n",
    "# Load original model predictions\n",
    "model_orig_preds = {}\n",
    "model_names = ['bert-base-cased', 'gpt2', 't5-base']\n",
    "\n",
    "for model in model_names:\n",
    "    filepath = base_path / f'{model}_results/{model}_predictions.json'\n",
    "    model_orig_preds[model] = load_predictions(filepath)\n",
    "\n",
    "# Load predictions for each modification\n",
    "modifications = []\n",
    "for model in model_names:\n",
    "    mod_path = base_path / f'{model}_results'\n",
    "    if mod_path.exists():\n",
    "        # Get all CSV files containing predictions\n",
    "        modifications.extend([f.stem for f in mod_path.glob('*_predictions.csv')])\n",
    "modifications = list(set(modifications))  # Remove duplicates\n",
    "modifications = [mod.replace('_predictions', '') for mod in modifications]\n",
    "\n",
    "# Load negation types from GPT4 results\n",
    "gpt4_negation_df = pd.read_csv('../eval/results/dialogue/gpt4o-0shot-negation_100.csv')\n",
    "negation_types = gpt4_negation_df['type'].tolist()\n",
    "\n",
    "# Sanity check negation types\n",
    "valid_types = {'absolute', 'double', 'lexical', 'approximate', 'verbal'}\n",
    "for neg_type in negation_types:\n",
    "    if neg_type not in valid_types:\n",
    "        print(f\"WARNING: Invalid negation type found: {neg_type}\")\n",
    "\n",
    "# Create results list to store accuracy and statistical test results\n",
    "results_rows = []\n",
    "negation_results_rows = []\n",
    "\n",
    "for mod in modifications:\n",
    "    for model in model_names:\n",
    "        # Get original predictions\n",
    "        orig_preds = model_orig_preds[model]\n",
    "        \n",
    "        # Get modified predictions from CSV file\n",
    "        mod_filepath = base_path / f'{model}_results/{mod}_predictions.csv'\n",
    "        if mod_filepath.exists():\n",
    "            mod_df = pd.read_csv(mod_filepath)\n",
    "            # Calculate accuracies\n",
    "            orig_correct = 0\n",
    "            mod_correct = 0\n",
    "            total = 0\n",
    "            orig_list = []\n",
    "            mod_list = []\n",
    "            \n",
    "            # Track results by negation type if this is negation mod\n",
    "            if mod == 'negation':\n",
    "                results_by_type = {\n",
    "                    'absolute': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'double': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'lexical': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'approximate': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'verbal': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []}\n",
    "                }\n",
    "            \n",
    "            # Load labels from eval/results/dialogue CSV\n",
    "            eval_filepath = Path(f'../data/modified_data/dialogue/{mod}_100.json')\n",
    "            if eval_filepath.exists():\n",
    "                label_df = json.load(open(eval_filepath))\n",
    "            if len(mod_df) != len(label_df):\n",
    "                print(mod, model, len(mod_df), len(label_df))\n",
    "                \n",
    "            for idx, row in mod_df.iterrows():\n",
    "                # Get ground truth labels from JSON in order\n",
    "                orig_label = label_df[idx][1]['original_label'] if 'original_label' in label_df[idx][1] else label_df[idx][1]['label']\n",
    "                mod_label = label_df[idx][1]['modified_label'] if 'modified_label' in label_df[idx][1] else label_df[idx][1]['label']\n",
    "                \n",
    "                # Get predictions from CSV\n",
    "                orig_pred = row['original'] if 'original' in row else row['original_pred']\n",
    "                mod_pred = row['modified'] if 'modified' in row else row['modified_pred']\n",
    "                \n",
    "                # Compare predictions with ground truth\n",
    "                orig_correct_bool = orig_pred == orig_label\n",
    "                mod_correct_bool = mod_pred == mod_label\n",
    "                \n",
    "                if orig_correct_bool:\n",
    "                    orig_correct += 1\n",
    "                    if mod == 'negation':\n",
    "                        if idx >= len(negation_types):\n",
    "                            print(f\"WARNING: Index {idx} exceeds negation types list length\")\n",
    "                            continue\n",
    "                        results_by_type[negation_types[idx]]['orig_correct'] += 1\n",
    "                if mod_correct_bool:\n",
    "                    mod_correct += 1\n",
    "                    if mod == 'negation':\n",
    "                        if idx >= len(negation_types):\n",
    "                            continue\n",
    "                        results_by_type[negation_types[idx]]['mod_correct'] += 1\n",
    "                        \n",
    "                if mod == 'negation':\n",
    "                    if idx >= len(negation_types):\n",
    "                        continue\n",
    "                    results_by_type[negation_types[idx]]['total'] += 1\n",
    "                    results_by_type[negation_types[idx]]['orig_binary'].append(1 if orig_correct_bool else 0)\n",
    "                    results_by_type[negation_types[idx]]['mod_binary'].append(1 if mod_correct_bool else 0)\n",
    "                    \n",
    "                orig_list.append(orig_pred)\n",
    "                mod_list.append(mod_pred)\n",
    "                total += 1\n",
    "                \n",
    "            orig_acc = orig_correct / total if total > 0 else 0\n",
    "            mod_acc = mod_correct / total if total > 0 else 0\n",
    "            pct_diff = ((mod_acc - orig_acc) / orig_acc) * 100 if orig_acc > 0 else 0\n",
    "            \n",
    "            orig_list_binary = [1 if pred == label else 0 for pred, label in zip(orig_list, [label_df[i][1]['original_label'] if 'original_label' in label_df[i][1] else label_df[i][1]['label'] for i in range(len(orig_list))])]\n",
    "            mod_list_binary = [1 if pred == label else 0 for pred, label in zip(mod_list, [label_df[i][1]['modified_label'] if 'modified_label' in label_df[i][1] else label_df[i][1]['label'] for i in range(len(mod_list))])]\n",
    "\n",
    "            # Perform paired t-test on the raw predictions (0/1)\n",
    "            try:\n",
    "                _, p_value_mw = stats.mannwhitneyu(orig_list_binary, mod_list_binary)\n",
    "                _, p_value_w = stats.wilcoxon(orig_list_binary, mod_list_binary)\n",
    "                p_value = min(p_value_mw, p_value_w)  # Use the more conservative p-value\n",
    "            except ValueError:\n",
    "                # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "                p_value = 1.0\n",
    "                \n",
    "            # Add significance level\n",
    "            if p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif p_value < 0.05:\n",
    "                significance = '*'\n",
    "            elif p_value < 0.1:\n",
    "                significance = '.'\n",
    "            else:\n",
    "                significance = 'ns'\n",
    "            \n",
    "            row = {\n",
    "                'model': model,\n",
    "                'modification': mod,\n",
    "                'original_acc': decimal.Decimal(orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'modified_acc': decimal.Decimal(mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'percentage_diff': decimal.Decimal(pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'p_value': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'significance': significance,\n",
    "                'significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "            }\n",
    "            results_rows.append(row)\n",
    "            \n",
    "            # Add rows for each negation type if this is negation mod\n",
    "            if mod == 'negation':\n",
    "                for neg_type, results in results_by_type.items():\n",
    "                    type_orig_acc = results['orig_correct'] / results['total'] if results['total'] > 0 else 0\n",
    "                    type_mod_acc = results['mod_correct'] / results['total'] if results['total'] > 0 else 0\n",
    "                    type_pct_diff = ((type_mod_acc - type_orig_acc) / type_orig_acc) * 100 if type_orig_acc > 0 else 0\n",
    "\n",
    "                    try:\n",
    "                        _, p_value_mw = stats.mannwhitneyu(results['orig_binary'], results['mod_binary'])\n",
    "                        _, p_value_w = stats.wilcoxon(results['orig_binary'], results['mod_binary'])\n",
    "                        p_value = min(p_value_mw, p_value_w)  # Use the more conservative p-value\n",
    "                    except ValueError:\n",
    "                        # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "                        p_value = 1.0\n",
    "                        \n",
    "                    # Add significance level\n",
    "                    if p_value < 0.01:\n",
    "                        significance = '**'\n",
    "                    elif p_value < 0.05:\n",
    "                        significance = '*'\n",
    "                    elif p_value < 0.1:\n",
    "                        significance = '.'\n",
    "                    else:\n",
    "                        significance = 'ns'\n",
    "\n",
    "                    type_row = {\n",
    "                        'model': model,\n",
    "                        'modification': neg_type,\n",
    "                        'original_acc': decimal.Decimal(type_orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'modified_acc': decimal.Decimal(type_mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'pct_diff': decimal.Decimal(type_pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'wilcoxon_pvalue': decimal.Decimal(p_value_w).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'mannwhitney_pvalue': decimal.Decimal(p_value_mw).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'pvalue': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'significance': significance,\n",
    "                        'significant': p_value < 0.05\n",
    "                    }\n",
    "                    negation_results_rows.append(type_row)\n",
    "\n",
    "# Convert to dataframes\n",
    "results_df = pd.DataFrame(results_rows)\n",
    "negation_results_df = pd.DataFrame(negation_results_rows)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(base_path / 'dialogue_plm_results.csv', index=False)\n",
    "negation_results_df.to_csv(base_path / 'dialogue_plm_negation_results.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"Results by modification and model:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results by modification and model:\n",
      "\n",
      "Negation Results by type and model:\n"
     ]
    }
   ],
   "source": [
    "# Also analyze results from eval/results/dialogue/\n",
    "eval_base_path = Path('../eval/results/dialogue')\n",
    "eval_results_rows = []\n",
    "eval_negation_results_rows = []\n",
    "\n",
    "for mod in os.listdir(eval_base_path):\n",
    "    if not mod.endswith('_100.csv'):\n",
    "        continue\n",
    "    model = mod.split('-0shot-')[0]\n",
    "    if model == 'mixtral':\n",
    "        continue\n",
    "    mod = mod.split('-0shot-')[1].replace('_100.csv', '')\n",
    "    # Load predictions from CSV\n",
    "    eval_filepath = eval_base_path / f'{model}-0shot-{mod}_100.csv'\n",
    "    if not eval_filepath.exists():\n",
    "        continue\n",
    "    df = pd.read_csv(eval_filepath)\n",
    "    \n",
    "    compare_file = Path(f'../data/modified_data/dialogue/{mod}_100.json')\n",
    "    if not compare_file.exists():\n",
    "        continue\n",
    "    compare_df = json.load(open(compare_file))\n",
    "    if len(df) != len(compare_df):\n",
    "        print(f\"Warning: Length mismatch for {mod} {model}\")\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    orig_correct = sum(df['original_pred'] == df['original_label'])\n",
    "    mod_correct = sum(df['modified_pred'] == df['modified_label'])\n",
    "    total = len(df)\n",
    "        \n",
    "    orig_acc = orig_correct / total if total > 0 else 0\n",
    "    mod_acc = mod_correct / total if total > 0 else 0\n",
    "    pct_diff = ((mod_acc - orig_acc) / orig_acc) * 100 if orig_acc > 0 else 0\n",
    "    \n",
    "    # Convert predictions to binary (0/1) based on correctness for t-test\n",
    "    orig_binary = (df['original_pred'] == df['original_label']).astype(int)\n",
    "    mod_binary = (df['modified_pred'] == df['modified_label']).astype(int)\n",
    "    \n",
    "    # Perform paired t-test on binary correctness values\n",
    "    try:\n",
    "        _, p_value_mw = stats.mannwhitneyu(orig_binary, mod_binary)\n",
    "        _, p_value_wilc = stats.wilcoxon(orig_binary, mod_binary)\n",
    "        p_value = min(p_value_mw, p_value_wilc)  # Use most conservative p-value\n",
    "    except ValueError:\n",
    "        # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "        p_value = 1.0\n",
    "        \n",
    "    # Add significance level\n",
    "    if p_value < 0.01:\n",
    "        significance = '**'\n",
    "    elif p_value < 0.05:\n",
    "        significance = '*'\n",
    "    elif p_value < 0.1:\n",
    "        significance = '.'\n",
    "    else:\n",
    "        significance = 'ns'\n",
    "    \n",
    "    row = {\n",
    "        'model': model,\n",
    "        'modification': mod,\n",
    "        'original_acc': decimal.Decimal(orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'modified_acc': decimal.Decimal(mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'percentage_diff': decimal.Decimal(pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'p_value': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'significance': significance,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "    eval_results_rows.append(row)\n",
    "    \n",
    "    # Add rows for each negation type if this is negation mod\n",
    "    if mod == 'negation':\n",
    "        # Verify negation types match expected values\n",
    "        expected_types = {'absolute', 'double', 'lexical', 'approximate', 'verbal'}\n",
    "        actual_types = set(df['type'].unique())\n",
    "        if actual_types != expected_types:\n",
    "            print(f\"Warning: Unexpected negation types for {model}\")\n",
    "            print(f\"Expected: {expected_types}\")\n",
    "            print(f\"Found: {actual_types}\")\n",
    "            \n",
    "        for neg_type in df['type'].unique():\n",
    "            type_df = df[df['type'] == neg_type]\n",
    "            \n",
    "            type_orig_correct = sum(type_df['original_pred'] == type_df['original_label'])\n",
    "            type_mod_correct = sum(type_df['modified_pred'] == type_df['modified_label'])\n",
    "            type_total = len(type_df)\n",
    "            \n",
    "            type_orig_acc = type_orig_correct / type_total if type_total > 0 else 0\n",
    "            type_mod_acc = type_mod_correct / type_total if type_total > 0 else 0\n",
    "            type_pct_diff = ((type_mod_acc - type_orig_acc) / type_orig_acc) * 100 if type_orig_acc > 0 else 0\n",
    "            \n",
    "            # Statistical tests for this negation type\n",
    "            type_orig_binary = (type_df['original_pred'] == type_df['original_label']).astype(int)\n",
    "            type_mod_binary = (type_df['modified_pred'] == type_df['modified_label']).astype(int)\n",
    "            \n",
    "            try:\n",
    "                _, type_p_value_mw = stats.mannwhitneyu(type_orig_binary, type_mod_binary)\n",
    "                _, type_p_value_wilc = stats.wilcoxon(type_orig_binary, type_mod_binary)\n",
    "                type_p_value = min(type_p_value_mw, type_p_value_wilc)\n",
    "            except ValueError:\n",
    "                type_p_value = 1.0\n",
    "                \n",
    "            # Add significance level\n",
    "            if type_p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif type_p_value < 0.05:\n",
    "                significance = '*'\n",
    "            elif type_p_value < 0.1:\n",
    "                significance = '.'\n",
    "            else:\n",
    "                significance = 'ns'\n",
    "                \n",
    "            type_row = {\n",
    "                'model': model,\n",
    "                'modification': neg_type,\n",
    "                'original_acc': decimal.Decimal(type_orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'modified_acc': decimal.Decimal(type_mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'pct_diff': decimal.Decimal(type_pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'wilcoxon_pvalue': decimal.Decimal(type_p_value_wilc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'mannwhitney_pvalue': decimal.Decimal(type_p_value_mw).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'pvalue': decimal.Decimal(type_p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'significance': significance,\n",
    "                'significant': type_p_value < 0.05\n",
    "            }\n",
    "            eval_negation_results_rows.append(type_row)\n",
    "\n",
    "# Convert to dataframes\n",
    "eval_results_df = pd.DataFrame(eval_results_rows)\n",
    "eval_negation_results_df = pd.DataFrame(eval_negation_results_rows)\n",
    "\n",
    "# Save results\n",
    "eval_results_df.to_csv(base_path / 'dialogue_llm_results.csv', index=False)\n",
    "eval_negation_results_df.to_csv(base_path / 'dialogue_llm_negation_results.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nEvaluation Results by modification and model:\")\n",
    "# print(eval_results_df)\n",
    "print(\"\\nNegation Results by type and model:\")\n",
    "# print(eval_negation_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Results:\n",
      "              model         modification  original_acc  modified_acc  \\\n",
      "54  bert-base-cased    active_to_passive         0.870         0.900   \n",
      "90  bert-base-cased       capitalization         0.927         0.927   \n",
      "99  bert-base-cased               casual         0.900         0.820   \n",
      "87  bert-base-cased        compound_word         0.890         0.850   \n",
      "81  bert-base-cased  concept_replacement         0.890         0.860   \n",
      "..              ...                  ...           ...           ...   \n",
      "62          t5-base             negation         0.900         0.680   \n",
      "74          t5-base          punctuation         0.860         0.860   \n",
      "65          t5-base            sentiment         0.840         0.810   \n",
      "53          t5-base        temporal_bias         0.810         0.760   \n",
      "86          t5-base            typo_bias         0.840         0.820   \n",
      "\n",
      "    percentage_diff  p_value significance significant  \n",
      "54              3.4    0.317           ns          No  \n",
      "90              0.0    1.000           ns          No  \n",
      "99             -8.9    0.021            *         Yes  \n",
      "87             -4.5    0.102           ns          No  \n",
      "81             -3.4    0.257           ns          No  \n",
      "..              ...      ...          ...         ...  \n",
      "62            -24.4    0.000           **         Yes  \n",
      "74              0.0    1.000           ns          No  \n",
      "65             -3.6    0.366           ns          No  \n",
      "53             -6.2    0.059            .          No  \n",
      "86             -2.4    0.317           ns          No  \n",
      "\n",
      "[102 rows x 8 columns]\n",
      "\n",
      "Combined Negation Results:\n",
      "                model modification  original_acc  modified_acc  pct_diff  \\\n",
      "15    bert-base-cased     absolute         0.846         0.615     -27.3   \n",
      "18    bert-base-cased  approximate         1.000         0.731     -26.9   \n",
      "16    bert-base-cased       double         0.636         0.727      14.3   \n",
      "17    bert-base-cased      lexical         0.938         0.656     -30.0   \n",
      "19    bert-base-cased       verbal         0.833         0.611     -26.7   \n",
      "8   claude-3-5-sonnet     absolute         1.000         0.692     -30.8   \n",
      "9   claude-3-5-sonnet  approximate         1.000         0.769     -23.1   \n",
      "6   claude-3-5-sonnet       double         1.000         0.636     -36.4   \n",
      "5   claude-3-5-sonnet      lexical         0.875         0.594     -32.1   \n",
      "7   claude-3-5-sonnet       verbal         0.889         0.833      -6.2   \n",
      "20               gpt2     absolute         0.846         0.538     -36.4   \n",
      "23               gpt2  approximate         0.923         0.692     -25.0   \n",
      "21               gpt2       double         0.909         0.636     -30.0   \n",
      "22               gpt2      lexical         0.906         0.594     -34.5   \n",
      "24               gpt2       verbal         0.778         0.611     -21.4   \n",
      "13              gpt4o     absolute         0.923         0.615     -33.3   \n",
      "14              gpt4o  approximate         0.923         0.769     -16.7   \n",
      "11              gpt4o       double         0.818         0.636     -22.2   \n",
      "10              gpt4o      lexical         0.844         0.531     -37.0   \n",
      "12              gpt4o       verbal         0.944         1.000       5.9   \n",
      "3               llama     absolute         1.000         0.615     -38.5   \n",
      "4               llama  approximate         0.962         0.769     -20.0   \n",
      "1               llama       double         0.909         0.455     -50.0   \n",
      "0               llama      lexical         0.875         0.563     -35.7   \n",
      "2               llama       verbal         1.000         0.722     -27.8   \n",
      "25            t5-base     absolute         0.923         0.538     -41.7   \n",
      "28            t5-base  approximate         0.923         0.731     -20.8   \n",
      "26            t5-base       double         0.818         0.636     -22.2   \n",
      "27            t5-base      lexical         0.938         0.656     -30.0   \n",
      "29            t5-base       verbal         0.833         0.778      -6.7   \n",
      "\n",
      "    wilcoxon_pvalue  mannwhitney_pvalue  pvalue significance  significant  \n",
      "15            0.180               0.205   0.180           ns        False  \n",
      "18            0.008               0.005   0.005           **         True  \n",
      "16            0.564               0.684   0.564           ns        False  \n",
      "17            0.003               0.006   0.003           **         True  \n",
      "19            0.102               0.148   0.102           ns        False  \n",
      "8             0.046               0.037   0.037            *         True  \n",
      "9             0.014               0.010   0.010            *         True  \n",
      "6             0.046               0.035   0.035            *         True  \n",
      "5             0.007               0.012   0.007           **         True  \n",
      "7             0.655               0.654   0.654           ns        False  \n",
      "20            0.102               0.102   0.102           ns        False  \n",
      "23            0.058               0.038   0.038            *         True  \n",
      "21            0.180               0.148   0.148           ns        False  \n",
      "22            0.004               0.004   0.004           **         True  \n",
      "24            0.257               0.294   0.257           ns        False  \n",
      "13            0.102               0.073   0.073            .        False  \n",
      "14            0.102               0.132   0.102           ns        False  \n",
      "11            0.414               0.372   0.372           ns        False  \n",
      "10            0.004               0.008   0.004           **         True  \n",
      "12            0.317               0.345   0.317           ns        False  \n",
      "3             0.025               0.016   0.016            *         True  \n",
      "4             0.025               0.046   0.025            *         True  \n",
      "1             0.025               0.028   0.025            *         True  \n",
      "0             0.002               0.006   0.002           **         True  \n",
      "2             0.025               0.019   0.019            *         True  \n",
      "25            0.025               0.033   0.025            *         True  \n",
      "28            0.059               0.072   0.059            .        False  \n",
      "26            0.157               0.372   0.157           ns        False  \n",
      "27            0.007               0.006   0.006           **         True  \n",
      "29            0.705               0.695   0.695           ns        False  \n"
     ]
    }
   ],
   "source": [
    "# Load LLM and PLM results\n",
    "base_path = Path('../pretrained_language_models/dialogue_contradiction_detection/tmp')\n",
    "llm_results = pd.read_csv(base_path / 'dialogue_llm_results.csv')\n",
    "plm_results = pd.read_csv(base_path / 'dialogue_plm_results.csv')\n",
    "\n",
    "# Load LLM and PLM negation results\n",
    "llm_neg_results = pd.read_csv(base_path / 'dialogue_llm_negation_results.csv')\n",
    "plm_neg_results = pd.read_csv(base_path / 'dialogue_plm_negation_results.csv')\n",
    "\n",
    "# Combine the regular results\n",
    "combined_results = pd.concat([llm_results, plm_results], ignore_index=True)\n",
    "\n",
    "# Combine the negation results\n",
    "combined_neg_results = pd.concat([llm_neg_results, plm_neg_results], ignore_index=True)\n",
    "\n",
    "# Sort by model and modification\n",
    "combined_results = combined_results.sort_values(['model', 'modification'])\n",
    "combined_neg_results = combined_neg_results.sort_values(['model', 'modification'])\n",
    "\n",
    "# Save combined results\n",
    "combined_results.to_csv(base_path / 'dialogue_combined_results.csv', index=False)\n",
    "combined_neg_results.to_csv(base_path / 'dialogue_combined_negation_results.csv', index=False)\n",
    "\n",
    "print(\"\\nCombined Results:\")\n",
    "print(combined_results)\n",
    "print(\"\\nCombined Negation Results:\")\n",
    "print(combined_neg_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table saved to .tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "base_path = Path('../pretrained_language_models/dialogue_contradiction_detection/tmp')\n",
    "modification_order =[\"B: Tem\", \"B: Geo\", \"B: Len\", \"O: Spell\",\"O: Cap\",\"O: Punc\",\n",
    "\"M: Deri\",\n",
    "\"M: Com\",\n",
    "\"Sx: Voice\",\n",
    "\"Sx: Gra\",\n",
    "\"Sx: Conj\",\n",
    "\"Sm: Con\",\n",
    "\"P: Neg\",\n",
    "\"P: Disc\",\n",
    "\"P: Senti\",\n",
    "\"G: Cas\",\n",
    "\"G: Dial\",]\n",
    "# Read the combined results\n",
    "df = pd.read_csv(base_path / 'dialogue_combined_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "mod_mapping = {\n",
    "    'temporal_bias': 'B: Tem',\n",
    "    'geographical_bias': 'B: Geo', \n",
    "    'length_bias': 'B: Len',\n",
    "    'typo_bias': 'O: Spell',\n",
    "    'capitalization': 'O: Cap',\n",
    "    'punctuation': 'O: Punc',\n",
    "    'derivation': 'M: Deri',\n",
    "    'compound_word': 'M: Com',\n",
    "    'active_to_passive': 'Sx: Voice',\n",
    "    'grammatical_role': 'Sx: Gra',\n",
    "    'coordinating_conjunction': 'Sx: Conj',\n",
    "    'concept_replacement': 'Sm: Con',\n",
    "    'negation': 'P: Neg',\n",
    "    'discourse': 'P: Disc',\n",
    "    'sentiment': 'P: Senti',\n",
    "    'casual': 'G: Cas',\n",
    "    'dialectal': 'G: Dial'\n",
    "}\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Define model order\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='percentage_diff')\n",
    "p_values = df.pivot(index='modification', columns='model', values='p_value')\n",
    "\n",
    "# Reorder rows and columns\n",
    "pivot_df = pivot_df.reindex(modification_order, axis=0)\n",
    "pivot_df = pivot_df.reindex(model_order, axis=1)\n",
    "p_values = p_values.reindex(modification_order, axis=0)\n",
    "p_values = p_values.reindex(model_order, axis=1)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, p_val):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if p_val < 0.01:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif p_val < 0.05:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif p_val < 0.1:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if p_val < 0.01:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif p_val < 0.05:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif p_val < 0.1:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(pivot_df.columns) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in pivot_df.columns]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(val, p_values.loc[idx, col]) for col, val in row.items()]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open(base_path / 'dialogue_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to .tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to dialogue_results_df.csv and dialogue_negation_results_df.csv\n"
     ]
    }
   ],
   "source": [
    "# Load results from CSV\n",
    "base_path = Path('../pretrained_language_models/dialogue_contradiction_detection/tmp')\n",
    "\n",
    "results_df = pd.read_csv(base_path / 'dialogue_combined_results.csv')\n",
    "\n",
    "# Create lists of unique modifications and models\n",
    "modification_order = list(mod_mapping.keys())\n",
    "negation_order = ['verbal', 'lexical', 'double', 'approximate', 'absolute']\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Create empty DataFrame with multi-level columns\n",
    "columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "results_df_pivot = pd.DataFrame(index=modification_order, columns=columns)\n",
    "\n",
    "\n",
    "\n",
    "# Fill DataFrame\n",
    "for mod in modification_order:\n",
    "    for model in model_order:\n",
    "        row = results_df[(results_df['modification'] == mod) & (results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            results_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'diff')] = row['percentage_diff'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "\n",
    "# Save to CSV\n",
    "results_df_pivot.to_csv(base_path / 'dialogue_results_df.csv')\n",
    "\n",
    "# Save negation results to separate CSV\n",
    "# Load negation results from CSV\n",
    "negation_results_df = pd.read_csv(base_path / 'dialogue_combined_negation_results.csv')\n",
    "\n",
    "# Create empty DataFrame with multi-level columns for negation results\n",
    "negation_df_pivot = pd.DataFrame(index=['negation'], columns=columns)\n",
    "\n",
    "# Fill DataFrame with negation results\n",
    "for mod in negation_order:  \n",
    "    for model in model_order:\n",
    "        row = negation_results_df[(negation_results_df['modification'] == mod) & (negation_results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            negation_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            negation_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0] \n",
    "            negation_df_pivot.loc[mod, (model, 'diff')] = row['pct_diff'].values[0]\n",
    "\n",
    "# Save negation results to CSV\n",
    "negation_df_pivot.to_csv(base_path / 'dialogue_negation_results_df.csv')\n",
    "\n",
    "\n",
    "print(\"Results saved to dialogue_results_df.csv and dialogue_negation_results_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hungthinht/miniconda3/lib/python3.12/site-packages/scipy/stats/_wilcoxon.py:199: UserWarning: Sample size too small for normal approximation.\n",
      "  temp = _wilcoxon_iv(x, y, zero_method, correction, alternative, method, axis)\n",
      "/Users/hungthinht/miniconda3/lib/python3.12/site-packages/scipy/stats/_wilcoxon.py:199: UserWarning: Sample size too small for normal approximation.\n",
      "  temp = _wilcoxon_iv(x, y, zero_method, correction, alternative, method, axis)\n",
      "/Users/hungthinht/miniconda3/lib/python3.12/site-packages/scipy/stats/_wilcoxon.py:199: UserWarning: Sample size too small for normal approximation.\n",
      "  temp = _wilcoxon_iv(x, y, zero_method, correction, alternative, method, axis)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Base path for results\n",
    "base_path = Path('../pretrained_language_models/coreference_resolution/tmp')\n",
    "\n",
    "# Function to load and convert predictions to binary\n",
    "def load_predictions(filepath):\n",
    "    with open(filepath) as f:\n",
    "        preds = json.load(f)\n",
    "    return preds\n",
    "\n",
    "# Load original model predictions\n",
    "model_orig_preds = {}\n",
    "model_names = ['bert-base-cased', 'gpt2', 't5-base']\n",
    "\n",
    "for model in model_names:\n",
    "    filepath = base_path / f'{model}_results/{model}_predictions.json'\n",
    "    model_orig_preds[model] = load_predictions(filepath)\n",
    "\n",
    "# Load predictions for each modification\n",
    "modifications = []\n",
    "for model in model_names:\n",
    "    mod_path = base_path / f'{model}_results'\n",
    "    if mod_path.exists():\n",
    "        # Get all CSV files containing predictions\n",
    "        modifications.extend([f.stem for f in mod_path.glob('*_predictions.csv')])\n",
    "modifications = list(set(modifications))  # Remove duplicates\n",
    "modifications = [mod.replace('_predictions', '') for mod in modifications]\n",
    "\n",
    "# Load negation types from GPT4 results\n",
    "gpt4_negation_df = pd.read_csv('../eval/results/coref/gpt4o-0shot-negation_100.csv')\n",
    "negation_types = gpt4_negation_df['type'].tolist()\n",
    "\n",
    "# Sanity check negation types\n",
    "valid_types = {'absolute', 'double', 'lexical', 'approximate', 'verbal'}\n",
    "for neg_type in negation_types:\n",
    "    if neg_type not in valid_types:\n",
    "        print(f\"WARNING: Invalid negation type found: {neg_type}\")\n",
    "\n",
    "# Create results list to store accuracy and statistical test results\n",
    "results_rows = []\n",
    "negation_results_rows = []\n",
    "\n",
    "for mod in modifications:\n",
    "    for model in model_names:\n",
    "        # Get original predictions\n",
    "        orig_preds = model_orig_preds[model]\n",
    "        \n",
    "        # Get modified predictions from CSV file\n",
    "        mod_filepath = base_path / f'{model}_results/{mod}_predictions.csv'\n",
    "        if mod_filepath.exists():\n",
    "            mod_df = pd.read_csv(mod_filepath)\n",
    "            # Calculate accuracies\n",
    "            orig_correct = 0\n",
    "            mod_correct = 0\n",
    "            total = 0\n",
    "            orig_list = []\n",
    "            mod_list = []\n",
    "            \n",
    "            # Track results by negation type if this is negation mod\n",
    "            if mod == 'negation':\n",
    "                results_by_type = {\n",
    "                    'absolute': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'double': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'lexical': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'approximate': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'verbal': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []}\n",
    "                }\n",
    "            \n",
    "            # Load labels from eval/results/dialogue CSV\n",
    "            eval_filepath = Path(f'../data/modified_data/coref/{mod}_100.json')\n",
    "            if eval_filepath.exists():\n",
    "                label_df = json.load(open(eval_filepath))\n",
    "            if len(mod_df) != len(label_df):\n",
    "                print(mod, model)\n",
    "                \n",
    "            for idx, row in mod_df.iterrows():\n",
    "                # Get ground truth labels from JSON in order\n",
    "                orig_label = label_df[idx]['original_label'] if 'original_label' in label_df[idx] else label_df[idx]['label']\n",
    "                mod_label = label_df[idx]['modified_label'] if 'modified_label' in label_df[idx] else label_df[idx]['label']\n",
    "                \n",
    "                # Get predictions from CSV\n",
    "                orig_pred = row['original'] if 'original' in row else row['original_pred']\n",
    "                mod_pred = row['modified'] if 'modified' in row else row['modified_pred']\n",
    "                \n",
    "                # Compare predictions with ground truth\n",
    "                orig_correct_bool = orig_pred == orig_label\n",
    "                mod_correct_bool = mod_pred == mod_label\n",
    "                \n",
    "                if orig_correct_bool:\n",
    "                    orig_correct += 1\n",
    "                    if mod == 'negation':\n",
    "                        if idx >= len(negation_types):\n",
    "                            print(f\"WARNING: Index {idx} exceeds negation types list length\")\n",
    "                            continue\n",
    "                        results_by_type[negation_types[idx]]['orig_correct'] += 1\n",
    "                if mod_correct_bool:\n",
    "                    mod_correct += 1\n",
    "                    if mod == 'negation':\n",
    "                        if idx >= len(negation_types):\n",
    "                            continue\n",
    "                        results_by_type[negation_types[idx]]['mod_correct'] += 1\n",
    "                        \n",
    "                if mod == 'negation':\n",
    "                    if idx >= len(negation_types):\n",
    "                        continue\n",
    "                    results_by_type[negation_types[idx]]['total'] += 1\n",
    "                    results_by_type[negation_types[idx]]['orig_binary'].append(1 if orig_correct_bool else 0)\n",
    "                    results_by_type[negation_types[idx]]['mod_binary'].append(1 if mod_correct_bool else 0)\n",
    "                    \n",
    "                orig_list.append(orig_pred)\n",
    "                mod_list.append(mod_pred)\n",
    "                total += 1\n",
    "                \n",
    "            orig_acc = orig_correct / total if total > 0 else 0\n",
    "            mod_acc = mod_correct / total if total > 0 else 0\n",
    "            pct_diff = ((mod_acc - orig_acc) / orig_acc) * 100 if orig_acc > 0 else 0\n",
    "            \n",
    "            orig_list_binary = [1 if pred == label else 0 for pred, label in zip(orig_list, [label_df[i]['original_label'] if 'original_label' in label_df[i] else label_df[i]['label'] for i in range(len(orig_list))])]\n",
    "            mod_list_binary = [1 if pred == label else 0 for pred, label in zip(mod_list, [label_df[i]['modified_label'] if 'modified_label' in label_df[i] else label_df[i]['label'] for i in range(len(mod_list))])]\n",
    "\n",
    "            # Perform paired t-test on the raw predictions (0/1)\n",
    "            try:\n",
    "                _, p_value_mw = stats.mannwhitneyu(orig_list_binary, mod_list_binary)\n",
    "                _, p_value_w = stats.wilcoxon(orig_list_binary, mod_list_binary)\n",
    "                p_value = min(p_value_mw, p_value_w)  # Use the more conservative p-value\n",
    "            except ValueError:\n",
    "                # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "                p_value = 1.0\n",
    "                \n",
    "            # Add significance level\n",
    "            if p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif p_value < 0.05:\n",
    "                significance = '*'\n",
    "            elif p_value < 0.1:\n",
    "                significance = '.'\n",
    "            else:\n",
    "                significance = 'ns'\n",
    "            \n",
    "            row = {\n",
    "                'model': model,\n",
    "                'modification': mod,\n",
    "                'original_acc': decimal.Decimal(orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'modified_acc': decimal.Decimal(mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'percentage_diff': decimal.Decimal(pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'p_value': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'significance': significance,\n",
    "                'significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "            }\n",
    "            results_rows.append(row)\n",
    "            \n",
    "            # Add rows for each negation type if this is negation mod\n",
    "            if mod == 'negation':\n",
    "                for neg_type, results in results_by_type.items():\n",
    "                    type_orig_acc = results['orig_correct'] / results['total'] if results['total'] > 0 else 0\n",
    "                    type_mod_acc = results['mod_correct'] / results['total'] if results['total'] > 0 else 0\n",
    "                    type_pct_diff = ((type_mod_acc - type_orig_acc) / type_orig_acc) * 100 if type_orig_acc > 0 else 0\n",
    "\n",
    "                    try:\n",
    "                        _, p_value_mw = stats.mannwhitneyu(results['orig_binary'], results['mod_binary'])\n",
    "                        _, p_value_w = stats.wilcoxon(results['orig_binary'], results['mod_binary'])\n",
    "                        p_value = min(p_value_mw, p_value_w)  # Use the more conservative p-value\n",
    "                    except ValueError:\n",
    "                        # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "                        p_value = 1.0\n",
    "                        \n",
    "                    # Add significance level\n",
    "                    if p_value < 0.01:\n",
    "                        significance = '**'\n",
    "                    elif p_value < 0.05:\n",
    "                        significance = '*'\n",
    "                    elif p_value < 0.1:\n",
    "                        significance = '.'\n",
    "                    else:\n",
    "                        significance = 'ns'\n",
    "\n",
    "                    type_row = {\n",
    "                        'model': model,\n",
    "                        'modification': neg_type,\n",
    "                        'original_acc': decimal.Decimal(type_orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'modified_acc': decimal.Decimal(type_mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'pct_diff': decimal.Decimal(type_pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'wilcoxon_pvalue': decimal.Decimal(p_value_w).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'mannwhitney_pvalue': decimal.Decimal(p_value_mw).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'pvalue': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'significance': significance,\n",
    "                        'significant': p_value < 0.05\n",
    "                    }\n",
    "                    negation_results_rows.append(type_row)\n",
    "\n",
    "# Convert to dataframes\n",
    "results_df = pd.DataFrame(results_rows)\n",
    "negation_results_df = pd.DataFrame(negation_results_rows)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(base_path / 'coreference_plm_results.csv', index=False)\n",
    "negation_results_df.to_csv(base_path / 'coreference_plm_negation_results.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "# print(\"Results by modification and model:\")\n",
    "# print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hungthinht/miniconda3/lib/python3.12/site-packages/scipy/stats/_wilcoxon.py:199: UserWarning: Sample size too small for normal approximation.\n",
      "  temp = _wilcoxon_iv(x, y, zero_method, correction, alternative, method, axis)\n",
      "/Users/hungthinht/miniconda3/lib/python3.12/site-packages/scipy/stats/_wilcoxon.py:199: UserWarning: Sample size too small for normal approximation.\n",
      "  temp = _wilcoxon_iv(x, y, zero_method, correction, alternative, method, axis)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results by modification and model:\n",
      "\n",
      "Negation Results by type and model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hungthinht/miniconda3/lib/python3.12/site-packages/scipy/stats/_wilcoxon.py:199: UserWarning: Sample size too small for normal approximation.\n",
      "  temp = _wilcoxon_iv(x, y, zero_method, correction, alternative, method, axis)\n"
     ]
    }
   ],
   "source": [
    "# Also analyze results from eval/results/dialogue/\n",
    "eval_base_path = Path('../eval/results/coref')\n",
    "eval_results_rows = []\n",
    "eval_negation_results_rows = []\n",
    "\n",
    "for mod in os.listdir(eval_base_path):\n",
    "    if not mod.endswith('_100.csv'):\n",
    "        continue\n",
    "    model = mod.split('-0shot-')[0]\n",
    "    if model == 'mixtral':\n",
    "        continue\n",
    "    mod = mod.split('-0shot-')[1].replace('_100.csv', '')\n",
    "    # Load predictions from CSV\n",
    "    eval_filepath = eval_base_path / f'{model}-0shot-{mod}_100.csv'\n",
    "    if not eval_filepath.exists():\n",
    "        continue\n",
    "    df = pd.read_csv(eval_filepath)\n",
    "    \n",
    "    compare_file = Path(f'../data/modified_data/coref/{mod}_100.json')\n",
    "    if not compare_file.exists():\n",
    "        continue\n",
    "    compare_df = json.load(open(compare_file))\n",
    "    if len(df) != len(compare_df):\n",
    "        print(f\"Warning: Length mismatch for {mod} {model}\")\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    orig_correct = sum(df['original_pred'] == df['original_label'])\n",
    "    mod_correct = sum(df['modified_pred'] == df['modified_label'])\n",
    "    total = len(df)\n",
    "        \n",
    "    orig_acc = orig_correct / total if total > 0 else 0\n",
    "    mod_acc = mod_correct / total if total > 0 else 0\n",
    "    pct_diff = ((mod_acc - orig_acc) / orig_acc) * 100 if orig_acc > 0 else 0\n",
    "    \n",
    "    # Convert predictions to binary (0/1) based on correctness for t-test\n",
    "    orig_binary = (df['original_pred'] == df['original_label']).astype(int)\n",
    "    mod_binary = (df['modified_pred'] == df['modified_label']).astype(int)\n",
    "    \n",
    "    # Perform paired t-test on binary correctness values\n",
    "    try:\n",
    "        _, p_value_mw = stats.mannwhitneyu(orig_binary, mod_binary)\n",
    "        _, p_value_wilc = stats.wilcoxon(orig_binary, mod_binary)\n",
    "        p_value = min(p_value_mw, p_value_wilc)  # Use most conservative p-value\n",
    "    except ValueError:\n",
    "        # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "        p_value = 1.0\n",
    "        \n",
    "    # Add significance level\n",
    "    if p_value < 0.01:\n",
    "        significance = '**'\n",
    "    elif p_value < 0.05:\n",
    "        significance = '*'\n",
    "    elif p_value < 0.1:\n",
    "        significance = '.'\n",
    "    else:\n",
    "        significance = 'ns'\n",
    "    \n",
    "    row = {\n",
    "        'model': model,\n",
    "        'modification': mod,\n",
    "        'original_acc': decimal.Decimal(orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'modified_acc': decimal.Decimal(mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'percentage_diff': decimal.Decimal(pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'p_value': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'significance': significance,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "    eval_results_rows.append(row)\n",
    "    \n",
    "    # Add rows for each negation type if this is negation mod\n",
    "    if mod == 'negation':\n",
    "        # Verify negation types match expected values\n",
    "        expected_types = {'absolute', 'double', 'lexical', 'approximate', 'verbal'}\n",
    "        actual_types = set(df['type'].unique())\n",
    "        if actual_types != expected_types:\n",
    "            print(f\"Warning: Unexpected negation types for {model}\")\n",
    "            print(f\"Expected: {expected_types}\")\n",
    "            print(f\"Found: {actual_types}\")\n",
    "            \n",
    "        for neg_type in df['type'].unique():\n",
    "            type_df = df[df['type'] == neg_type]\n",
    "            \n",
    "            type_orig_correct = sum(type_df['original_pred'] == type_df['original_label'])\n",
    "            type_mod_correct = sum(type_df['modified_pred'] == type_df['modified_label'])\n",
    "            type_total = len(type_df)\n",
    "            \n",
    "            type_orig_acc = type_orig_correct / type_total if type_total > 0 else 0\n",
    "            type_mod_acc = type_mod_correct / type_total if type_total > 0 else 0\n",
    "            type_pct_diff = ((type_mod_acc - type_orig_acc) / type_orig_acc) * 100 if type_orig_acc > 0 else 0\n",
    "            \n",
    "            # Statistical tests for this negation type\n",
    "            type_orig_binary = (type_df['original_pred'] == type_df['original_label']).astype(int)\n",
    "            type_mod_binary = (type_df['modified_pred'] == type_df['modified_label']).astype(int)\n",
    "            \n",
    "            try:\n",
    "                _, type_p_value_mw = stats.mannwhitneyu(type_orig_binary, type_mod_binary)\n",
    "                _, type_p_value_wilc = stats.wilcoxon(type_orig_binary, type_mod_binary)\n",
    "                type_p_value = min(type_p_value_mw, type_p_value_wilc)\n",
    "            except ValueError:\n",
    "                type_p_value = 1.0\n",
    "                \n",
    "            # Add significance level\n",
    "            if type_p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif type_p_value < 0.05:\n",
    "                significance = '*'\n",
    "            elif type_p_value < 0.1:\n",
    "                significance = '.'\n",
    "            else:\n",
    "                significance = 'ns'\n",
    "                \n",
    "            type_row = {\n",
    "                'model': model,\n",
    "                'modification': neg_type,\n",
    "                'original_acc': decimal.Decimal(type_orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'modified_acc': decimal.Decimal(type_mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'pct_diff': decimal.Decimal(type_pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'wilcoxon_pvalue': decimal.Decimal(type_p_value_wilc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'mannwhitney_pvalue': decimal.Decimal(type_p_value_mw).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'pvalue': decimal.Decimal(type_p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'significance': significance,\n",
    "                'significant': type_p_value < 0.05\n",
    "            }\n",
    "            eval_negation_results_rows.append(type_row)\n",
    "\n",
    "# Convert to dataframes\n",
    "eval_results_df = pd.DataFrame(eval_results_rows)\n",
    "eval_negation_results_df = pd.DataFrame(eval_negation_results_rows)\n",
    "\n",
    "# Save results\n",
    "eval_results_df.to_csv(base_path / 'coreference_llm_results.csv', index=False)\n",
    "eval_negation_results_df.to_csv(base_path / 'coreference_llm_negation_results.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nEvaluation Results by modification and model:\")\n",
    "# print(eval_results_df)\n",
    "print(\"\\nNegation Results by type and model:\")\n",
    "# print(eval_negation_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Results:\n",
      "              model         modification  original_acc  modified_acc  \\\n",
      "54  bert-base-cased    active_to_passive         0.495         0.474   \n",
      "90  bert-base-cased       capitalization         0.535         0.485   \n",
      "99  bert-base-cased               casual         0.550         0.430   \n",
      "87  bert-base-cased        compound_word         0.604         0.604   \n",
      "81  bert-base-cased  concept_replacement         0.440         0.420   \n",
      "..              ...                  ...           ...           ...   \n",
      "62          t5-base             negation         0.592         0.571   \n",
      "74          t5-base          punctuation         0.586         0.596   \n",
      "65          t5-base            sentiment         0.640         0.640   \n",
      "53          t5-base        temporal_bias         0.600         0.580   \n",
      "86          t5-base            typo_bias         0.633         0.643   \n",
      "\n",
      "    percentage_diff  p_value significance significant  \n",
      "54             -4.3    0.732           ns          No  \n",
      "90             -9.4    0.197           ns          No  \n",
      "99            -21.8    0.003           **         Yes  \n",
      "87              0.0    1.000           ns          No  \n",
      "81             -4.5    0.480           ns          No  \n",
      "..              ...      ...          ...         ...  \n",
      "62             -3.4    0.683           ns          No  \n",
      "74              1.7    0.317           ns          No  \n",
      "65              0.0    1.000           ns          No  \n",
      "53             -3.3    0.414           ns          No  \n",
      "86              1.6    0.705           ns          No  \n",
      "\n",
      "[102 rows x 8 columns]\n",
      "\n",
      "Combined Negation Results:\n",
      "                model modification  original_acc  modified_acc  pct_diff  \\\n",
      "15    bert-base-cased     absolute         0.750         0.625     -16.7   \n",
      "18    bert-base-cased  approximate         0.484         0.419     -13.3   \n",
      "16    bert-base-cased       double         0.600         0.600       0.0   \n",
      "17    bert-base-cased      lexical         0.500         0.458      -8.3   \n",
      "19    bert-base-cased       verbal         0.550         0.600       9.1   \n",
      "9   claude-3-5-sonnet     absolute         0.875         0.875       0.0   \n",
      "8   claude-3-5-sonnet  approximate         0.677         0.774      14.3   \n",
      "7   claude-3-5-sonnet       double         0.667         0.667       0.0   \n",
      "6   claude-3-5-sonnet      lexical         0.917         0.833      -9.1   \n",
      "5   claude-3-5-sonnet       verbal         0.850         0.600     -29.4   \n",
      "20               gpt2     absolute         0.625         0.875      40.0   \n",
      "23               gpt2  approximate         0.516         0.452     -12.5   \n",
      "21               gpt2       double         0.467         0.533      14.3   \n",
      "22               gpt2      lexical         0.542         0.458     -15.4   \n",
      "24               gpt2       verbal         0.450         0.450       0.0   \n",
      "14              gpt4o     absolute         0.875         1.000      14.3   \n",
      "13              gpt4o  approximate         0.774         0.645     -16.7   \n",
      "12              gpt4o       double         0.733         0.667      -9.1   \n",
      "11              gpt4o      lexical         0.833         0.792      -5.0   \n",
      "10              gpt4o       verbal         0.900         0.600     -33.3   \n",
      "4               llama     absolute         0.625         0.875      40.0   \n",
      "3               llama  approximate         0.806         0.774      -4.0   \n",
      "2               llama       double         0.667         0.667       0.0   \n",
      "1               llama      lexical         0.875         0.750     -14.3   \n",
      "0               llama       verbal         0.850         0.550     -35.3   \n",
      "25            t5-base     absolute         0.625         0.875      40.0   \n",
      "28            t5-base  approximate         0.516         0.484      -6.2   \n",
      "26            t5-base       double         0.467         0.600      28.6   \n",
      "27            t5-base      lexical         0.625         0.542     -13.3   \n",
      "29            t5-base       verbal         0.750         0.600     -20.0   \n",
      "\n",
      "    wilcoxon_pvalue  mannwhitney_pvalue  pvalue significance  significant  \n",
      "15            0.317               0.648   0.317           ns        False  \n",
      "18            0.527               0.618   0.527           ns        False  \n",
      "16            1.000               1.000   1.000           ns        False  \n",
      "17            0.655               0.784   0.655           ns        False  \n",
      "19            0.655               0.764   0.655           ns        False  \n",
      "9             1.000               1.000   1.000           ns        False  \n",
      "8             0.317               0.402   0.317           ns        False  \n",
      "7             1.000               1.000   1.000           ns        False  \n",
      "6             0.317               0.398   0.317           ns        False  \n",
      "5             0.059               0.083   0.059            .        False  \n",
      "20            0.317               0.295   0.295           ns        False  \n",
      "23            0.480               0.620   0.480           ns        False  \n",
      "21            0.564               0.738   0.564           ns        False  \n",
      "22            0.317               0.576   0.317           ns        False  \n",
      "24            1.000               1.000   1.000           ns        False  \n",
      "14            0.317               0.382   0.317           ns        False  \n",
      "13            0.157               0.271   0.157           ns        False  \n",
      "12            0.564               0.715   0.564           ns        False  \n",
      "11            0.564               0.726   0.564           ns        False  \n",
      "10            0.014               0.032   0.014            *         True  \n",
      "4             0.157               0.295   0.157           ns        False  \n",
      "3             0.705               0.765   0.705           ns        False  \n",
      "2             1.000               1.000   1.000           ns        False  \n",
      "1             0.083               0.279   0.083            .        False  \n",
      "0             0.014               0.043   0.014            *         True  \n",
      "25            0.157               0.295   0.157           ns        False  \n",
      "28            0.705               0.807   0.705           ns        False  \n",
      "26            0.317               0.487   0.317           ns        False  \n",
      "27            0.414               0.570   0.414           ns        False  \n",
      "29            0.180               0.325   0.180           ns        False  \n"
     ]
    }
   ],
   "source": [
    "# Load LLM and PLM results\n",
    "llm_results = pd.read_csv(base_path / 'coreference_llm_results.csv')\n",
    "plm_results = pd.read_csv(base_path / 'coreference_plm_results.csv')\n",
    "\n",
    "# Load LLM and PLM negation results\n",
    "llm_neg_results = pd.read_csv(base_path / 'coreference_llm_negation_results.csv')\n",
    "plm_neg_results = pd.read_csv(base_path / 'coreference_plm_negation_results.csv')\n",
    "\n",
    "# Combine the regular results\n",
    "combined_results = pd.concat([llm_results, plm_results], ignore_index=True)\n",
    "\n",
    "# Combine the negation results\n",
    "combined_neg_results = pd.concat([llm_neg_results, plm_neg_results], ignore_index=True)\n",
    "\n",
    "# Sort by model and modification\n",
    "combined_results = combined_results.sort_values(['model', 'modification'])\n",
    "combined_neg_results = combined_neg_results.sort_values(['model', 'modification'])\n",
    "\n",
    "# Save combined results\n",
    "combined_results.to_csv(base_path / 'coreference_combined_results.csv', index=False)\n",
    "combined_neg_results.to_csv(base_path / 'coreference_combined_negation_results.csv', index=False)\n",
    "\n",
    "print(\"\\nCombined Results:\")\n",
    "print(combined_results)\n",
    "print(\"\\nCombined Negation Results:\")\n",
    "print(combined_neg_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table saved to .tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "base_path = Path('../pretrained_language_models/coreference_resolution/tmp')\n",
    "modification_order =[\"B: Tem\", \"B: Geo\", \"B: Len\", \"O: Spell\",\"O: Cap\",\"O: Punc\",\n",
    "\"M: Deri\",\n",
    "\"M: Com\",\n",
    "\"Sx: Voice\",\n",
    "\"Sx: Gra\",\n",
    "\"Sx: Conj\",\n",
    "\"Sm: Con\",\n",
    "\"P: Neg\",\n",
    "\"P: Disc\",\n",
    "\"P: Senti\",\n",
    "\"G: Cas\",\n",
    "\"G: Dial\",]\n",
    "# Read the combined results\n",
    "df = pd.read_csv(base_path / 'coreference_combined_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "mod_mapping = {\n",
    "    'temporal_bias': 'B: Tem',\n",
    "    'geographical_bias': 'B: Geo', \n",
    "    'length_bias': 'B: Len',\n",
    "    'typo_bias': 'O: Spell',\n",
    "    'capitalization': 'O: Cap',\n",
    "    'punctuation': 'O: Punc',\n",
    "    'derivation': 'M: Deri',\n",
    "    'compound_word': 'M: Com',\n",
    "    'active_to_passive': 'Sx: Voice',\n",
    "    'grammatical_role': 'Sx: Gra',\n",
    "    'coordinating_conjunction': 'Sx: Conj',\n",
    "    'concept_replacement': 'Sm: Con',\n",
    "    'negation': 'P: Neg',\n",
    "    'discourse': 'P: Disc',\n",
    "    'sentiment': 'P: Senti',\n",
    "    'casual': 'G: Cas',\n",
    "    'dialectal': 'G: Dial'\n",
    "}\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Define model order\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='percentage_diff')\n",
    "p_values = df.pivot(index='modification', columns='model', values='p_value')\n",
    "\n",
    "# Reorder rows and columns\n",
    "pivot_df = pivot_df.reindex(modification_order, axis=0)\n",
    "pivot_df = pivot_df.reindex(model_order, axis=1)\n",
    "p_values = p_values.reindex(modification_order, axis=0)\n",
    "p_values = p_values.reindex(model_order, axis=1)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, p_val):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if p_val < 0.01:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif p_val < 0.05:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif p_val < 0.1:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if p_val < 0.01:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif p_val < 0.05:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif p_val < 0.1:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(pivot_df.columns) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in pivot_df.columns]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(val, p_values.loc[idx, col]) for col, val in row.items()]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open(base_path / 'coreference_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to .tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to coreference_results_df.csv and coreference_negation_results_df.csv\n"
     ]
    }
   ],
   "source": [
    "# Load results from CSV\n",
    "base_path = Path('../pretrained_language_models/coreference_resolution/tmp')\n",
    "\n",
    "results_df = pd.read_csv(base_path / 'coreference_combined_results.csv')\n",
    "\n",
    "# Create lists of unique modifications and models\n",
    "modification_order = list(mod_mapping.keys())\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "negation_order = ['verbal', 'lexical', 'double', 'approximate', 'absolute']\n",
    "# Create empty DataFrame with multi-level columns\n",
    "columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "results_df_pivot = pd.DataFrame(index=modification_order, columns=columns)\n",
    "\n",
    "# Fill DataFrame\n",
    "for mod in modification_order:\n",
    "    for model in model_order:\n",
    "        row = results_df[(results_df['modification'] == mod) & (results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            results_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'diff')] = row['percentage_diff'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "\n",
    "# Save to CSV\n",
    "results_df_pivot.to_csv(base_path / 'coreference_results_df.csv')\n",
    "\n",
    "# Save negation results to separate CSV\n",
    "# Load negation results from CSV\n",
    "negation_results_df = pd.read_csv(base_path / 'coreference_combined_negation_results.csv')\n",
    "\n",
    "# Create empty DataFrame with multi-level columns for negation results\n",
    "negation_df_pivot = pd.DataFrame(index=['negation'], columns=columns)\n",
    "\n",
    "# Fill DataFrame with negation results\n",
    "for mod in negation_order:\n",
    "    for model in model_order:\n",
    "        row = negation_results_df[(negation_results_df['modification'] == mod) & (negation_results_df['model'] == model)]\n",
    "        if not row.empty:   \n",
    "            negation_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            negation_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0] \n",
    "            negation_df_pivot.loc[mod, (model, 'diff')] = row['pct_diff'].values[0]\n",
    "\n",
    "# Save negation results to CSV\n",
    "negation_df_pivot.to_csv(base_path / 'coreference_negation_results_df.csv')\n",
    "\n",
    "\n",
    "print(\"Results saved to coreference_results_df.csv and coreference_negation_results_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model         bert-base-cased  claude-3-5-sonnet  gpt2  gpt4o  llama  t5-base\n",
      "modification                                                                 \n",
      "Verbal                  -26.7               -6.2 -21.4    5.9  -27.8     -6.7\n",
      "Lexical                 -30.0              -32.1 -34.5  -37.0  -35.7    -30.0\n",
      "Double                   14.3              -36.4 -30.0  -22.2  -50.0    -22.2\n",
      "Approximate             -26.9              -23.1 -25.0  -16.7  -20.0    -20.8\n",
      "Absolute                -27.3              -30.8 -36.4  -33.3  -38.5    -41.7\n",
      "LaTeX table saved to ner_results_table.tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "base_path = Path('dialogue_contradiction_detection/tmp')\n",
    "df = pd.read_csv(base_path / 'dialogue_combined_negation_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "negation_order= ['Verbal', 'Lexical', 'Double', 'Approximate', 'Absolute']\n",
    "mod_mapping = {\n",
    "    'verbal': 'Verbal',\n",
    "    'lexical': 'Lexical',\n",
    "    'double': 'Double',\n",
    "    'approximate': 'Approximate',\n",
    "    'absolute': 'Absolute',\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='pct_diff')\n",
    "p_values = df.pivot(index='modification', columns='model', values='pvalue')\n",
    "significance = df.pivot(index='modification', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows according to modification_order\n",
    "pivot_df = pivot_df.reindex(negation_order)\n",
    "p_values = p_values.reindex(negation_order)\n",
    "significance = significance.reindex(negation_order)\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(row[col], significance.loc[idx, col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open('dialogue_negation_type_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to ner_results_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model         bert-base-cased  claude-3-5-sonnet  gpt2  gpt4o  llama  t5-base\n",
      "modification                                                                 \n",
      "Verbal                    9.1              -29.4   0.0  -33.3  -35.3    -20.0\n",
      "Lexical                  -8.3               -9.1 -15.4   -5.0  -14.3    -13.3\n",
      "Double                    0.0                0.0  14.3   -9.1    0.0     28.6\n",
      "Approximate             -13.3               14.3 -12.5  -16.7   -4.0     -6.2\n",
      "Absolute                -16.7                0.0  40.0   14.3   40.0     40.0\n",
      "LaTeX table saved to coreference_negation_type_results_table.tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "base_path = Path('coreference_resolution/tmp')\n",
    "df = pd.read_csv(base_path / 'coreference_combined_negation_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "negation_order= ['Verbal', 'Lexical', 'Double', 'Approximate', 'Absolute']\n",
    "mod_mapping = {\n",
    "    'verbal': 'Verbal',\n",
    "    'lexical': 'Lexical',\n",
    "    'double': 'Double',\n",
    "    'approximate': 'Approximate',\n",
    "    'absolute': 'Absolute',\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='pct_diff')\n",
    "p_values = df.pivot(index='modification', columns='model', values='pvalue')\n",
    "significance = df.pivot(index='modification', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows according to modification_order\n",
    "pivot_df = pivot_df.reindex(negation_order)\n",
    "p_values = p_values.reindex(negation_order)\n",
    "significance = significance.reindex(negation_order)\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(row[col], significance.loc[idx, col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open(base_path / 'coreference_negation_type_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to coreference_negation_type_results_table.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
