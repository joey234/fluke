{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import decimal\n",
    "\n",
    "# Load derivation data to get indices\n",
    "derivation_path = Path('../data/modified_data/coref/derivation_100.json')\n",
    "with open(derivation_path) as f:\n",
    "    derivation_data = json.load(f)\n",
    "\n",
    "# Extract indices from derivation data\n",
    "derivation_indices = [item['index'] for item in derivation_data]\n",
    "print(derivation_indices)\n",
    "# Create mapping between modification types and their filenames\n",
    "modification_mapping = {\n",
    "    'derivation': 'derivation'\n",
    "}\n",
    "\n",
    "# Base path for results\n",
    "base_path = Path('../pretrained_language_models/coreference_resolution/tmp')\n",
    "\n",
    "# Create index mapping for each model and modification\n",
    "index_mappings = {}\n",
    "for model in ['bert-base-cased', 'gpt2', 't5-base']:\n",
    "    index_mappings[model] = {}\n",
    "    model_path = base_path / f'{model}_results'\n",
    "    \n",
    "    for mod_type, filename in modification_mapping.items():\n",
    "        csv_path = model_path / f'{filename}_predictions.csv'\n",
    "        if csv_path.exists():\n",
    "            df = pd.read_csv(csv_path)\n",
    "            # Replace indices with derivation indices\n",
    "            df['index'] = derivation_indices[:len(df)]\n",
    "            # Create new file with replaced indices\n",
    "            output_path = model_path / f'{filename}_predictions.csv'\n",
    "            df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Base path for results\n",
    "base_path = Path('../pretrained_language_models/dialogue_contradiction_detection/tmp')\n",
    "\n",
    "# Function to load and convert predictions to binary\n",
    "def load_predictions(filepath):\n",
    "    with open(filepath) as f:\n",
    "        preds = json.load(f)\n",
    "    for pred in preds.items():\n",
    "        if pred[1] == \"contradictory\":\n",
    "            preds[pred[0]] = 1\n",
    "        elif pred[1] == \"not contradictory\":\n",
    "            preds[pred[0]] = 0\n",
    "    return preds\n",
    "\n",
    "# Load original model predictions\n",
    "model_orig_preds = {}\n",
    "model_names = ['bert-base-cased', 'gpt2', 't5-base']\n",
    "\n",
    "for model in model_names:\n",
    "    filepath = base_path / f'{model}_results/{model}_predictions.json'\n",
    "    model_orig_preds[model] = load_predictions(filepath)\n",
    "\n",
    "# Load predictions for each modification\n",
    "modifications = []\n",
    "for model in model_names:\n",
    "    mod_path = base_path / f'{model}_results'\n",
    "    if mod_path.exists():\n",
    "        # Get all CSV files containing predictions\n",
    "        modifications.extend([f.stem for f in mod_path.glob('*_predictions.csv')])\n",
    "modifications = list(set(modifications))  # Remove duplicates\n",
    "modifications = [mod.replace('_predictions', '') for mod in modifications]\n",
    "\n",
    "# Load negation types from GPT4 results\n",
    "gpt4_negation_df = pd.read_csv('../eval/results/dialogue/gpt4o-0shot-negation_100.csv')\n",
    "negation_types = gpt4_negation_df['type'].tolist()\n",
    "\n",
    "# Sanity check negation types\n",
    "valid_types = {'absolute', 'double', 'lexical', 'approximate', 'verbal'}\n",
    "for neg_type in negation_types:\n",
    "    if neg_type not in valid_types:\n",
    "        print(f\"WARNING: Invalid negation type found: {neg_type}\")\n",
    "\n",
    "# Create results list to store accuracy and statistical test results\n",
    "results_rows = []\n",
    "negation_results_rows = []\n",
    "\n",
    "for mod in modifications:\n",
    "    for model in model_names:\n",
    "        # Get original predictions\n",
    "        orig_preds = model_orig_preds[model]\n",
    "        \n",
    "        # Get modified predictions from CSV file\n",
    "        mod_filepath = base_path / f'{model}_results/{mod}_predictions.csv'\n",
    "        if mod_filepath.exists():\n",
    "            mod_df = pd.read_csv(mod_filepath)\n",
    "            # Calculate accuracies\n",
    "            orig_correct = 0\n",
    "            mod_correct = 0\n",
    "            total = 0\n",
    "            orig_list = []\n",
    "            mod_list = []\n",
    "            \n",
    "            # Track results by negation type if this is negation mod\n",
    "            if mod == 'negation':\n",
    "                results_by_type = {\n",
    "                    'absolute': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'double': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'lexical': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'approximate': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'verbal': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []}\n",
    "                }\n",
    "            \n",
    "            # Load labels from eval/results/dialogue CSV\n",
    "            eval_filepath = Path(f'../data/modified_data/dialogue/{mod}_100.json')\n",
    "            if eval_filepath.exists():\n",
    "                label_df = json.load(open(eval_filepath))\n",
    "            if len(mod_df) != len(label_df):\n",
    "                print(mod, model, len(mod_df), len(label_df))\n",
    "                \n",
    "            for idx, row in mod_df.iterrows():\n",
    "                # Get ground truth labels from JSON in order\n",
    "                orig_label = label_df[idx][1]['original_label'] if 'original_label' in label_df[idx][1] else label_df[idx][1]['label']\n",
    "                mod_label = label_df[idx][1]['modified_label'] if 'modified_label' in label_df[idx][1] else label_df[idx][1]['label']\n",
    "                \n",
    "                # Get predictions from CSV\n",
    "                orig_pred = row['original'] if 'original' in row else row['original_pred']\n",
    "                mod_pred = row['modified'] if 'modified' in row else row['modified_pred']\n",
    "                \n",
    "                # Compare predictions with ground truth\n",
    "                orig_correct_bool = orig_pred == orig_label\n",
    "                mod_correct_bool = mod_pred == mod_label\n",
    "                \n",
    "                if orig_correct_bool:\n",
    "                    orig_correct += 1\n",
    "                    if mod == 'negation':\n",
    "                        if idx >= len(negation_types):\n",
    "                            print(f\"WARNING: Index {idx} exceeds negation types list length\")\n",
    "                            continue\n",
    "                        results_by_type[negation_types[idx]]['orig_correct'] += 1\n",
    "                if mod_correct_bool:\n",
    "                    mod_correct += 1\n",
    "                    if mod == 'negation':\n",
    "                        if idx >= len(negation_types):\n",
    "                            continue\n",
    "                        results_by_type[negation_types[idx]]['mod_correct'] += 1\n",
    "                        \n",
    "                if mod == 'negation':\n",
    "                    if idx >= len(negation_types):\n",
    "                        continue\n",
    "                    results_by_type[negation_types[idx]]['total'] += 1\n",
    "                    results_by_type[negation_types[idx]]['orig_binary'].append(1 if orig_correct_bool else 0)\n",
    "                    results_by_type[negation_types[idx]]['mod_binary'].append(1 if mod_correct_bool else 0)\n",
    "                    \n",
    "                orig_list.append(orig_pred)\n",
    "                mod_list.append(mod_pred)\n",
    "                total += 1\n",
    "                \n",
    "            orig_acc = orig_correct / total if total > 0 else 0\n",
    "            mod_acc = mod_correct / total if total > 0 else 0\n",
    "            pct_diff = ((mod_acc - orig_acc) / orig_acc) * 100 if orig_acc > 0 else 0\n",
    "            \n",
    "            orig_list_binary = [1 if pred == label else 0 for pred, label in zip(orig_list, [label_df[i][1]['original_label'] if 'original_label' in label_df[i][1] else label_df[i][1]['label'] for i in range(len(orig_list))])]\n",
    "            mod_list_binary = [1 if pred == label else 0 for pred, label in zip(mod_list, [label_df[i][1]['modified_label'] if 'modified_label' in label_df[i][1] else label_df[i][1]['label'] for i in range(len(mod_list))])]\n",
    "\n",
    "            # Perform paired t-test on the raw predictions (0/1)\n",
    "            try:\n",
    "                _, p_value_mw = stats.mannwhitneyu(orig_list_binary, mod_list_binary)\n",
    "                _, p_value_w = stats.wilcoxon(orig_list_binary, mod_list_binary)\n",
    "                p_value = min(p_value_mw, p_value_w)  # Use the more conservative p-value\n",
    "            except ValueError:\n",
    "                # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "                p_value = 1.0\n",
    "                \n",
    "            # Add significance level\n",
    "            if p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif p_value < 0.05:\n",
    "                significance = '*'\n",
    "            elif p_value < 0.1:\n",
    "                significance = '.'\n",
    "            else:\n",
    "                significance = 'ns'\n",
    "            \n",
    "            row = {\n",
    "                'model': model,\n",
    "                'modification': mod,\n",
    "                'original_acc': decimal.Decimal(orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'modified_acc': decimal.Decimal(mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'percentage_diff': decimal.Decimal(pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'p_value': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'significance': significance,\n",
    "                'significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "            }\n",
    "            results_rows.append(row)\n",
    "            \n",
    "            # Add rows for each negation type if this is negation mod\n",
    "            if mod == 'negation':\n",
    "                for neg_type, results in results_by_type.items():\n",
    "                    type_orig_acc = results['orig_correct'] / results['total'] if results['total'] > 0 else 0\n",
    "                    type_mod_acc = results['mod_correct'] / results['total'] if results['total'] > 0 else 0\n",
    "                    type_pct_diff = ((type_mod_acc - type_orig_acc) / type_orig_acc) * 100 if type_orig_acc > 0 else 0\n",
    "\n",
    "                    try:\n",
    "                        _, p_value_mw = stats.mannwhitneyu(results['orig_binary'], results['mod_binary'])\n",
    "                        _, p_value_w = stats.wilcoxon(results['orig_binary'], results['mod_binary'])\n",
    "                        p_value = min(p_value_mw, p_value_w)  # Use the more conservative p-value\n",
    "                    except ValueError:\n",
    "                        # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "                        p_value = 1.0\n",
    "                        \n",
    "                    # Add significance level\n",
    "                    if p_value < 0.01:\n",
    "                        significance = '**'\n",
    "                    elif p_value < 0.05:\n",
    "                        significance = '*'\n",
    "                    elif p_value < 0.1:\n",
    "                        significance = '.'\n",
    "                    else:\n",
    "                        significance = 'ns'\n",
    "\n",
    "                    type_row = {\n",
    "                        'model': model,\n",
    "                        'modification': neg_type,\n",
    "                        'original_acc': decimal.Decimal(type_orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'modified_acc': decimal.Decimal(type_mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'pct_diff': decimal.Decimal(type_pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'wilcoxon_pvalue': decimal.Decimal(p_value_w).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'mannwhitney_pvalue': decimal.Decimal(p_value_mw).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'pvalue': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'significance': significance,\n",
    "                        'significant': p_value < 0.05\n",
    "                    }\n",
    "                    negation_results_rows.append(type_row)\n",
    "\n",
    "# Convert to dataframes\n",
    "results_df = pd.DataFrame(results_rows)\n",
    "negation_results_df = pd.DataFrame(negation_results_rows)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(base_path / 'dialogue_plm_results.csv', index=False)\n",
    "negation_results_df.to_csv(base_path / 'dialogue_plm_negation_results.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"Results by modification and model:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also analyze results from eval/results/dialogue/\n",
    "eval_base_path = Path('../eval/results/dialogue')\n",
    "eval_results_rows = []\n",
    "eval_negation_results_rows = []\n",
    "\n",
    "for mod in os.listdir(eval_base_path):\n",
    "    if not mod.endswith('_100.csv'):\n",
    "        continue\n",
    "    model = mod.split('-0shot-')[0]\n",
    "    if model == 'mixtral':\n",
    "        continue\n",
    "    mod = mod.split('-0shot-')[1].replace('_100.csv', '')\n",
    "    # Load predictions from CSV\n",
    "    eval_filepath = eval_base_path / f'{model}-0shot-{mod}_100.csv'\n",
    "    if not eval_filepath.exists():\n",
    "        continue\n",
    "    df = pd.read_csv(eval_filepath)\n",
    "    \n",
    "    compare_file = Path(f'../data/modified_data/dialogue/{mod}_100.json')\n",
    "    if not compare_file.exists():\n",
    "        continue\n",
    "    compare_df = json.load(open(compare_file))\n",
    "    if len(df) != len(compare_df):\n",
    "        print(f\"Warning: Length mismatch for {mod} {model}\")\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    orig_correct = sum(df['original_pred'] == df['original_label'])\n",
    "    mod_correct = sum(df['modified_pred'] == df['modified_label'])\n",
    "    total = len(df)\n",
    "        \n",
    "    orig_acc = orig_correct / total if total > 0 else 0\n",
    "    mod_acc = mod_correct / total if total > 0 else 0\n",
    "    pct_diff = ((mod_acc - orig_acc) / orig_acc) * 100 if orig_acc > 0 else 0\n",
    "    \n",
    "    # Convert predictions to binary (0/1) based on correctness for t-test\n",
    "    orig_binary = (df['original_pred'] == df['original_label']).astype(int)\n",
    "    mod_binary = (df['modified_pred'] == df['modified_label']).astype(int)\n",
    "    \n",
    "    # Perform paired t-test on binary correctness values\n",
    "    try:\n",
    "        _, p_value_mw = stats.mannwhitneyu(orig_binary, mod_binary)\n",
    "        _, p_value_wilc = stats.wilcoxon(orig_binary, mod_binary)\n",
    "        p_value = min(p_value_mw, p_value_wilc)  # Use most conservative p-value\n",
    "    except ValueError:\n",
    "        # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "        p_value = 1.0\n",
    "        \n",
    "    # Add significance level\n",
    "    if p_value < 0.01:\n",
    "        significance = '**'\n",
    "    elif p_value < 0.05:\n",
    "        significance = '*'\n",
    "    elif p_value < 0.1:\n",
    "        significance = '.'\n",
    "    else:\n",
    "        significance = 'ns'\n",
    "    \n",
    "    row = {\n",
    "        'model': model,\n",
    "        'modification': mod,\n",
    "        'original_acc': decimal.Decimal(orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'modified_acc': decimal.Decimal(mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'percentage_diff': decimal.Decimal(pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'p_value': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'significance': significance,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "    eval_results_rows.append(row)\n",
    "    \n",
    "    # Add rows for each negation type if this is negation mod\n",
    "    if mod == 'negation':\n",
    "        # Verify negation types match expected values\n",
    "        expected_types = {'absolute', 'double', 'lexical', 'approximate', 'verbal'}\n",
    "        actual_types = set(df['type'].unique())\n",
    "        if actual_types != expected_types:\n",
    "            print(f\"Warning: Unexpected negation types for {model}\")\n",
    "            print(f\"Expected: {expected_types}\")\n",
    "            print(f\"Found: {actual_types}\")\n",
    "            \n",
    "        for neg_type in df['type'].unique():\n",
    "            type_df = df[df['type'] == neg_type]\n",
    "            \n",
    "            type_orig_correct = sum(type_df['original_pred'] == type_df['original_label'])\n",
    "            type_mod_correct = sum(type_df['modified_pred'] == type_df['modified_label'])\n",
    "            type_total = len(type_df)\n",
    "            \n",
    "            type_orig_acc = type_orig_correct / type_total if type_total > 0 else 0\n",
    "            type_mod_acc = type_mod_correct / type_total if type_total > 0 else 0\n",
    "            type_pct_diff = ((type_mod_acc - type_orig_acc) / type_orig_acc) * 100 if type_orig_acc > 0 else 0\n",
    "            \n",
    "            # Statistical tests for this negation type\n",
    "            type_orig_binary = (type_df['original_pred'] == type_df['original_label']).astype(int)\n",
    "            type_mod_binary = (type_df['modified_pred'] == type_df['modified_label']).astype(int)\n",
    "            \n",
    "            try:\n",
    "                _, type_p_value_mw = stats.mannwhitneyu(type_orig_binary, type_mod_binary)\n",
    "                _, type_p_value_wilc = stats.wilcoxon(type_orig_binary, type_mod_binary)\n",
    "                type_p_value = min(type_p_value_mw, type_p_value_wilc)\n",
    "            except ValueError:\n",
    "                type_p_value = 1.0\n",
    "                \n",
    "            # Add significance level\n",
    "            if type_p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif type_p_value < 0.05:\n",
    "                significance = '*'\n",
    "            elif type_p_value < 0.1:\n",
    "                significance = '.'\n",
    "            else:\n",
    "                significance = 'ns'\n",
    "                \n",
    "            type_row = {\n",
    "                'model': model,\n",
    "                'modification': neg_type,\n",
    "                'original_acc': decimal.Decimal(type_orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'modified_acc': decimal.Decimal(type_mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'pct_diff': decimal.Decimal(type_pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'wilcoxon_pvalue': decimal.Decimal(type_p_value_wilc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'mannwhitney_pvalue': decimal.Decimal(type_p_value_mw).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'pvalue': decimal.Decimal(type_p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'significance': significance,\n",
    "                'significant': type_p_value < 0.05\n",
    "            }\n",
    "            eval_negation_results_rows.append(type_row)\n",
    "\n",
    "# Convert to dataframes\n",
    "eval_results_df = pd.DataFrame(eval_results_rows)\n",
    "eval_negation_results_df = pd.DataFrame(eval_negation_results_rows)\n",
    "\n",
    "# Save results\n",
    "eval_results_df.to_csv(base_path / 'dialogue_llm_results.csv', index=False)\n",
    "eval_negation_results_df.to_csv(base_path / 'dialogue_llm_negation_results.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nEvaluation Results by modification and model:\")\n",
    "# print(eval_results_df)\n",
    "print(\"\\nNegation Results by type and model:\")\n",
    "# print(eval_negation_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM and PLM results\n",
    "base_path = Path('../pretrained_language_models/dialogue_contradiction_detection/tmp')\n",
    "llm_results = pd.read_csv(base_path / 'dialogue_llm_results.csv')\n",
    "plm_results = pd.read_csv(base_path / 'dialogue_plm_results.csv')\n",
    "\n",
    "# Load LLM and PLM negation results\n",
    "llm_neg_results = pd.read_csv(base_path / 'dialogue_llm_negation_results.csv')\n",
    "plm_neg_results = pd.read_csv(base_path / 'dialogue_plm_negation_results.csv')\n",
    "\n",
    "# Combine the regular results\n",
    "combined_results = pd.concat([llm_results, plm_results], ignore_index=True)\n",
    "\n",
    "# Combine the negation results\n",
    "combined_neg_results = pd.concat([llm_neg_results, plm_neg_results], ignore_index=True)\n",
    "\n",
    "# Sort by model and modification\n",
    "combined_results = combined_results.sort_values(['model', 'modification'])\n",
    "combined_neg_results = combined_neg_results.sort_values(['model', 'modification'])\n",
    "\n",
    "# Save combined results\n",
    "combined_results.to_csv(base_path / 'dialogue_combined_results.csv', index=False)\n",
    "combined_neg_results.to_csv(base_path / 'dialogue_combined_negation_results.csv', index=False)\n",
    "\n",
    "print(\"\\nCombined Results:\")\n",
    "print(combined_results)\n",
    "print(\"\\nCombined Negation Results:\")\n",
    "print(combined_neg_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "base_path = Path('../pretrained_language_models/dialogue_contradiction_detection/tmp')\n",
    "modification_order =[\"B: Tem\", \"B: Geo\", \"B: Len\", \"O: Spell\",\"O: Cap\",\"O: Punc\",\n",
    "\"M: Deri\",\n",
    "\"M: Com\",\n",
    "\"Sx: Voice\",\n",
    "\"Sx: Gra\",\n",
    "\"Sx: Conj\",\n",
    "\"Sm: Con\",\n",
    "\"P: Neg\",\n",
    "\"P: Disc\",\n",
    "\"P: Senti\",\n",
    "\"G: Cas\",\n",
    "\"G: Dial\",]\n",
    "# Read the combined results\n",
    "df = pd.read_csv(base_path / 'dialogue_combined_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "mod_mapping = {\n",
    "    'temporal_bias': 'B: Tem',\n",
    "    'geographical_bias': 'B: Geo', \n",
    "    'length_bias': 'B: Len',\n",
    "    'typo_bias': 'O: Spell',\n",
    "    'capitalization': 'O: Cap',\n",
    "    'punctuation': 'O: Punc',\n",
    "    'derivation': 'M: Deri',\n",
    "    'compound_word': 'M: Com',\n",
    "    'active_to_passive': 'Sx: Voice',\n",
    "    'grammatical_role': 'Sx: Gra',\n",
    "    'coordinating_conjunction': 'Sx: Conj',\n",
    "    'concept_replacement': 'Sm: Con',\n",
    "    'negation': 'P: Neg',\n",
    "    'discourse': 'P: Disc',\n",
    "    'sentiment': 'P: Senti',\n",
    "    'casual': 'G: Cas',\n",
    "    'dialectal': 'G: Dial'\n",
    "}\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Define model order\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='percentage_diff')\n",
    "p_values = df.pivot(index='modification', columns='model', values='p_value')\n",
    "\n",
    "# Reorder rows and columns\n",
    "pivot_df = pivot_df.reindex(modification_order, axis=0)\n",
    "pivot_df = pivot_df.reindex(model_order, axis=1)\n",
    "p_values = p_values.reindex(modification_order, axis=0)\n",
    "p_values = p_values.reindex(model_order, axis=1)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, p_val):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if p_val < 0.01:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif p_val < 0.05:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif p_val < 0.1:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if p_val < 0.01:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif p_val < 0.05:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif p_val < 0.1:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(pivot_df.columns) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in pivot_df.columns]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(val, p_values.loc[idx, col]) for col, val in row.items()]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open(base_path / 'dialogue_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to .tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from CSV\n",
    "base_path = Path('../pretrained_language_models/dialogue_contradiction_detection/tmp')\n",
    "\n",
    "results_df = pd.read_csv(base_path / 'dialogue_combined_results.csv')\n",
    "\n",
    "# Create lists of unique modifications and models\n",
    "modification_order = list(mod_mapping.keys())\n",
    "negation_order = ['verbal', 'lexical', 'double', 'approximate', 'absolute']\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Create empty DataFrame with multi-level columns\n",
    "columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "results_df_pivot = pd.DataFrame(index=modification_order, columns=columns)\n",
    "\n",
    "\n",
    "\n",
    "# Fill DataFrame\n",
    "for mod in modification_order:\n",
    "    for model in model_order:\n",
    "        row = results_df[(results_df['modification'] == mod) & (results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            results_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'diff')] = row['percentage_diff'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "\n",
    "# Save to CSV\n",
    "results_df_pivot.to_csv(base_path / 'dialogue_results_df.csv')\n",
    "\n",
    "# Save negation results to separate CSV\n",
    "# Load negation results from CSV\n",
    "negation_results_df = pd.read_csv(base_path / 'dialogue_combined_negation_results.csv')\n",
    "\n",
    "# Create empty DataFrame with multi-level columns for negation results\n",
    "negation_df_pivot = pd.DataFrame(index=['negation'], columns=columns)\n",
    "\n",
    "# Fill DataFrame with negation results\n",
    "for mod in negation_order:  \n",
    "    for model in model_order:\n",
    "        row = negation_results_df[(negation_results_df['modification'] == mod) & (negation_results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            negation_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            negation_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0] \n",
    "            negation_df_pivot.loc[mod, (model, 'diff')] = row['pct_diff'].values[0]\n",
    "\n",
    "# Save negation results to CSV\n",
    "negation_df_pivot.to_csv(base_path / 'dialogue_negation_results_df.csv')\n",
    "\n",
    "\n",
    "print(\"Results saved to dialogue_results_df.csv and dialogue_negation_results_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Base path for results\n",
    "base_path = Path('../pretrained_language_models/coreference_resolution/tmp')\n",
    "\n",
    "# Function to load and convert predictions to binary\n",
    "def load_predictions(filepath):\n",
    "    with open(filepath) as f:\n",
    "        preds = json.load(f)\n",
    "    return preds\n",
    "\n",
    "# Load original model predictions\n",
    "model_orig_preds = {}\n",
    "model_names = ['bert-base-cased', 'gpt2', 't5-base']\n",
    "\n",
    "for model in model_names:\n",
    "    filepath = base_path / f'{model}_results/{model}_predictions.json'\n",
    "    model_orig_preds[model] = load_predictions(filepath)\n",
    "\n",
    "# Load predictions for each modification\n",
    "modifications = []\n",
    "for model in model_names:\n",
    "    mod_path = base_path / f'{model}_results'\n",
    "    if mod_path.exists():\n",
    "        # Get all CSV files containing predictions\n",
    "        modifications.extend([f.stem for f in mod_path.glob('*_predictions.csv')])\n",
    "modifications = list(set(modifications))  # Remove duplicates\n",
    "modifications = [mod.replace('_predictions', '') for mod in modifications]\n",
    "\n",
    "# Load negation types from GPT4 results\n",
    "gpt4_negation_df = pd.read_csv('../eval/results/coref/gpt4o-0shot-negation_100.csv')\n",
    "negation_types = gpt4_negation_df['type'].tolist()\n",
    "\n",
    "# Sanity check negation types\n",
    "valid_types = {'absolute', 'double', 'lexical', 'approximate', 'verbal'}\n",
    "for neg_type in negation_types:\n",
    "    if neg_type not in valid_types:\n",
    "        print(f\"WARNING: Invalid negation type found: {neg_type}\")\n",
    "\n",
    "# Create results list to store accuracy and statistical test results\n",
    "results_rows = []\n",
    "negation_results_rows = []\n",
    "\n",
    "for mod in modifications:\n",
    "    for model in model_names:\n",
    "        # Get original predictions\n",
    "        orig_preds = model_orig_preds[model]\n",
    "        \n",
    "        # Get modified predictions from CSV file\n",
    "        mod_filepath = base_path / f'{model}_results/{mod}_predictions.csv'\n",
    "        if mod_filepath.exists():\n",
    "            mod_df = pd.read_csv(mod_filepath)\n",
    "            # Calculate accuracies\n",
    "            orig_correct = 0\n",
    "            mod_correct = 0\n",
    "            total = 0\n",
    "            orig_list = []\n",
    "            mod_list = []\n",
    "            \n",
    "            # Track results by negation type if this is negation mod\n",
    "            if mod == 'negation':\n",
    "                results_by_type = {\n",
    "                    'absolute': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'double': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'lexical': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'approximate': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'verbal': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []}\n",
    "                }\n",
    "            \n",
    "            # Load labels from eval/results/dialogue CSV\n",
    "            eval_filepath = Path(f'../data/modified_data/coref/{mod}_100.json')\n",
    "            if eval_filepath.exists():\n",
    "                label_df = json.load(open(eval_filepath))\n",
    "            if len(mod_df) != len(label_df):\n",
    "                print(mod, model)\n",
    "                \n",
    "            for idx, row in mod_df.iterrows():\n",
    "                # Get ground truth labels from JSON in order\n",
    "                orig_label = label_df[idx]['original_label'] if 'original_label' in label_df[idx] else label_df[idx]['label']\n",
    "                mod_label = label_df[idx]['modified_label'] if 'modified_label' in label_df[idx] else label_df[idx]['label']\n",
    "                \n",
    "                # Get predictions from CSV\n",
    "                orig_pred = row['original'] if 'original' in row else row['original_pred']\n",
    "                mod_pred = row['modified'] if 'modified' in row else row['modified_pred']\n",
    "                \n",
    "                # Compare predictions with ground truth\n",
    "                orig_correct_bool = orig_pred == orig_label\n",
    "                mod_correct_bool = mod_pred == mod_label\n",
    "                \n",
    "                if orig_correct_bool:\n",
    "                    orig_correct += 1\n",
    "                    if mod == 'negation':\n",
    "                        if idx >= len(negation_types):\n",
    "                            print(f\"WARNING: Index {idx} exceeds negation types list length\")\n",
    "                            continue\n",
    "                        results_by_type[negation_types[idx]]['orig_correct'] += 1\n",
    "                if mod_correct_bool:\n",
    "                    mod_correct += 1\n",
    "                    if mod == 'negation':\n",
    "                        if idx >= len(negation_types):\n",
    "                            continue\n",
    "                        results_by_type[negation_types[idx]]['mod_correct'] += 1\n",
    "                        \n",
    "                if mod == 'negation':\n",
    "                    if idx >= len(negation_types):\n",
    "                        continue\n",
    "                    results_by_type[negation_types[idx]]['total'] += 1\n",
    "                    results_by_type[negation_types[idx]]['orig_binary'].append(1 if orig_correct_bool else 0)\n",
    "                    results_by_type[negation_types[idx]]['mod_binary'].append(1 if mod_correct_bool else 0)\n",
    "                    \n",
    "                orig_list.append(orig_pred)\n",
    "                mod_list.append(mod_pred)\n",
    "                total += 1\n",
    "                \n",
    "            orig_acc = orig_correct / total if total > 0 else 0\n",
    "            mod_acc = mod_correct / total if total > 0 else 0\n",
    "            pct_diff = ((mod_acc - orig_acc) / orig_acc) * 100 if orig_acc > 0 else 0\n",
    "            \n",
    "            orig_list_binary = [1 if pred == label else 0 for pred, label in zip(orig_list, [label_df[i]['original_label'] if 'original_label' in label_df[i] else label_df[i]['label'] for i in range(len(orig_list))])]\n",
    "            mod_list_binary = [1 if pred == label else 0 for pred, label in zip(mod_list, [label_df[i]['modified_label'] if 'modified_label' in label_df[i] else label_df[i]['label'] for i in range(len(mod_list))])]\n",
    "\n",
    "            # Perform paired t-test on the raw predictions (0/1)\n",
    "            try:\n",
    "                _, p_value_mw = stats.mannwhitneyu(orig_list_binary, mod_list_binary)\n",
    "                _, p_value_w = stats.wilcoxon(orig_list_binary, mod_list_binary)\n",
    "                p_value = min(p_value_mw, p_value_w)  # Use the more conservative p-value\n",
    "            except ValueError:\n",
    "                # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "                p_value = 1.0\n",
    "                \n",
    "            # Add significance level\n",
    "            if p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif p_value < 0.05:\n",
    "                significance = '*'\n",
    "            elif p_value < 0.1:\n",
    "                significance = '.'\n",
    "            else:\n",
    "                significance = 'ns'\n",
    "            \n",
    "            row = {\n",
    "                'model': model,\n",
    "                'modification': mod,\n",
    "                'original_acc': decimal.Decimal(orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'modified_acc': decimal.Decimal(mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'percentage_diff': decimal.Decimal(pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'p_value': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'significance': significance,\n",
    "                'significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "            }\n",
    "            results_rows.append(row)\n",
    "            \n",
    "            # Add rows for each negation type if this is negation mod\n",
    "            if mod == 'negation':\n",
    "                for neg_type, results in results_by_type.items():\n",
    "                    type_orig_acc = results['orig_correct'] / results['total'] if results['total'] > 0 else 0\n",
    "                    type_mod_acc = results['mod_correct'] / results['total'] if results['total'] > 0 else 0\n",
    "                    type_pct_diff = ((type_mod_acc - type_orig_acc) / type_orig_acc) * 100 if type_orig_acc > 0 else 0\n",
    "\n",
    "                    try:\n",
    "                        _, p_value_mw = stats.mannwhitneyu(results['orig_binary'], results['mod_binary'])\n",
    "                        _, p_value_w = stats.wilcoxon(results['orig_binary'], results['mod_binary'])\n",
    "                        p_value = min(p_value_mw, p_value_w)  # Use the more conservative p-value\n",
    "                    except ValueError:\n",
    "                        # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "                        p_value = 1.0\n",
    "                        \n",
    "                    # Add significance level\n",
    "                    if p_value < 0.01:\n",
    "                        significance = '**'\n",
    "                    elif p_value < 0.05:\n",
    "                        significance = '*'\n",
    "                    elif p_value < 0.1:\n",
    "                        significance = '.'\n",
    "                    else:\n",
    "                        significance = 'ns'\n",
    "\n",
    "                    type_row = {\n",
    "                        'model': model,\n",
    "                        'modification': neg_type,\n",
    "                        'original_acc': decimal.Decimal(type_orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'modified_acc': decimal.Decimal(type_mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'pct_diff': decimal.Decimal(type_pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'wilcoxon_pvalue': decimal.Decimal(p_value_w).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'mannwhitney_pvalue': decimal.Decimal(p_value_mw).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'pvalue': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'significance': significance,\n",
    "                        'significant': p_value < 0.05\n",
    "                    }\n",
    "                    negation_results_rows.append(type_row)\n",
    "\n",
    "# Convert to dataframes\n",
    "results_df = pd.DataFrame(results_rows)\n",
    "negation_results_df = pd.DataFrame(negation_results_rows)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(base_path / 'coreference_plm_results.csv', index=False)\n",
    "negation_results_df.to_csv(base_path / 'coreference_plm_negation_results.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "# print(\"Results by modification and model:\")\n",
    "# print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also analyze results from eval/results/dialogue/\n",
    "eval_base_path = Path('../eval/results/coref')\n",
    "eval_results_rows = []\n",
    "eval_negation_results_rows = []\n",
    "\n",
    "for mod in os.listdir(eval_base_path):\n",
    "    if not mod.endswith('_100.csv'):\n",
    "        continue\n",
    "    model = mod.split('-0shot-')[0]\n",
    "    if model == 'mixtral':\n",
    "        continue\n",
    "    mod = mod.split('-0shot-')[1].replace('_100.csv', '')\n",
    "    # Load predictions from CSV\n",
    "    eval_filepath = eval_base_path / f'{model}-0shot-{mod}_100.csv'\n",
    "    if not eval_filepath.exists():\n",
    "        continue\n",
    "    df = pd.read_csv(eval_filepath)\n",
    "    \n",
    "    compare_file = Path(f'../data/modified_data/coref/{mod}_100.json')\n",
    "    if not compare_file.exists():\n",
    "        continue\n",
    "    compare_df = json.load(open(compare_file))\n",
    "    if len(df) != len(compare_df):\n",
    "        print(f\"Warning: Length mismatch for {mod} {model}\")\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    orig_correct = sum(df['original_pred'] == df['original_label'])\n",
    "    mod_correct = sum(df['modified_pred'] == df['modified_label'])\n",
    "    total = len(df)\n",
    "        \n",
    "    orig_acc = orig_correct / total if total > 0 else 0\n",
    "    mod_acc = mod_correct / total if total > 0 else 0\n",
    "    pct_diff = ((mod_acc - orig_acc) / orig_acc) * 100 if orig_acc > 0 else 0\n",
    "    \n",
    "    # Convert predictions to binary (0/1) based on correctness for t-test\n",
    "    orig_binary = (df['original_pred'] == df['original_label']).astype(int)\n",
    "    mod_binary = (df['modified_pred'] == df['modified_label']).astype(int)\n",
    "    \n",
    "    # Perform paired t-test on binary correctness values\n",
    "    try:\n",
    "        _, p_value_mw = stats.mannwhitneyu(orig_binary, mod_binary)\n",
    "        _, p_value_wilc = stats.wilcoxon(orig_binary, mod_binary)\n",
    "        p_value = min(p_value_mw, p_value_wilc)  # Use most conservative p-value\n",
    "    except ValueError:\n",
    "        # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "        p_value = 1.0\n",
    "        \n",
    "    # Add significance level\n",
    "    if p_value < 0.01:\n",
    "        significance = '**'\n",
    "    elif p_value < 0.05:\n",
    "        significance = '*'\n",
    "    elif p_value < 0.1:\n",
    "        significance = '.'\n",
    "    else:\n",
    "        significance = 'ns'\n",
    "    \n",
    "    row = {\n",
    "        'model': model,\n",
    "        'modification': mod,\n",
    "        'original_acc': decimal.Decimal(orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'modified_acc': decimal.Decimal(mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'percentage_diff': decimal.Decimal(pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'p_value': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'significance': significance,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "    eval_results_rows.append(row)\n",
    "    \n",
    "    # Add rows for each negation type if this is negation mod\n",
    "    if mod == 'negation':\n",
    "        # Verify negation types match expected values\n",
    "        expected_types = {'absolute', 'double', 'lexical', 'approximate', 'verbal'}\n",
    "        actual_types = set(df['type'].unique())\n",
    "        if actual_types != expected_types:\n",
    "            print(f\"Warning: Unexpected negation types for {model}\")\n",
    "            print(f\"Expected: {expected_types}\")\n",
    "            print(f\"Found: {actual_types}\")\n",
    "            \n",
    "        for neg_type in df['type'].unique():\n",
    "            type_df = df[df['type'] == neg_type]\n",
    "            \n",
    "            type_orig_correct = sum(type_df['original_pred'] == type_df['original_label'])\n",
    "            type_mod_correct = sum(type_df['modified_pred'] == type_df['modified_label'])\n",
    "            type_total = len(type_df)\n",
    "            \n",
    "            type_orig_acc = type_orig_correct / type_total if type_total > 0 else 0\n",
    "            type_mod_acc = type_mod_correct / type_total if type_total > 0 else 0\n",
    "            type_pct_diff = ((type_mod_acc - type_orig_acc) / type_orig_acc) * 100 if type_orig_acc > 0 else 0\n",
    "            \n",
    "            # Statistical tests for this negation type\n",
    "            type_orig_binary = (type_df['original_pred'] == type_df['original_label']).astype(int)\n",
    "            type_mod_binary = (type_df['modified_pred'] == type_df['modified_label']).astype(int)\n",
    "            \n",
    "            try:\n",
    "                _, type_p_value_mw = stats.mannwhitneyu(type_orig_binary, type_mod_binary)\n",
    "                _, type_p_value_wilc = stats.wilcoxon(type_orig_binary, type_mod_binary)\n",
    "                type_p_value = min(type_p_value_mw, type_p_value_wilc)\n",
    "            except ValueError:\n",
    "                type_p_value = 1.0\n",
    "                \n",
    "            # Add significance level\n",
    "            if type_p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif type_p_value < 0.05:\n",
    "                significance = '*'\n",
    "            elif type_p_value < 0.1:\n",
    "                significance = '.'\n",
    "            else:\n",
    "                significance = 'ns'\n",
    "                \n",
    "            type_row = {\n",
    "                'model': model,\n",
    "                'modification': neg_type,\n",
    "                'original_acc': decimal.Decimal(type_orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'modified_acc': decimal.Decimal(type_mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'pct_diff': decimal.Decimal(type_pct_diff).quantize(decimal.Decimal('0.1'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'wilcoxon_pvalue': decimal.Decimal(type_p_value_wilc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'mannwhitney_pvalue': decimal.Decimal(type_p_value_mw).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'pvalue': decimal.Decimal(type_p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'significance': significance,\n",
    "                'significant': type_p_value < 0.05\n",
    "            }\n",
    "            eval_negation_results_rows.append(type_row)\n",
    "\n",
    "# Convert to dataframes\n",
    "eval_results_df = pd.DataFrame(eval_results_rows)\n",
    "eval_negation_results_df = pd.DataFrame(eval_negation_results_rows)\n",
    "\n",
    "# Save results\n",
    "eval_results_df.to_csv(base_path / 'coreference_llm_results.csv', index=False)\n",
    "eval_negation_results_df.to_csv(base_path / 'coreference_llm_negation_results.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nEvaluation Results by modification and model:\")\n",
    "# print(eval_results_df)\n",
    "print(\"\\nNegation Results by type and model:\")\n",
    "# print(eval_negation_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM and PLM results\n",
    "llm_results = pd.read_csv(base_path / 'coreference_llm_results.csv')\n",
    "plm_results = pd.read_csv(base_path / 'coreference_plm_results.csv')\n",
    "\n",
    "# Load LLM and PLM negation results\n",
    "llm_neg_results = pd.read_csv(base_path / 'coreference_llm_negation_results.csv')\n",
    "plm_neg_results = pd.read_csv(base_path / 'coreference_plm_negation_results.csv')\n",
    "\n",
    "# Combine the regular results\n",
    "combined_results = pd.concat([llm_results, plm_results], ignore_index=True)\n",
    "\n",
    "# Combine the negation results\n",
    "combined_neg_results = pd.concat([llm_neg_results, plm_neg_results], ignore_index=True)\n",
    "\n",
    "# Sort by model and modification\n",
    "combined_results = combined_results.sort_values(['model', 'modification'])\n",
    "combined_neg_results = combined_neg_results.sort_values(['model', 'modification'])\n",
    "\n",
    "# Save combined results\n",
    "combined_results.to_csv(base_path / 'coreference_combined_results.csv', index=False)\n",
    "combined_neg_results.to_csv(base_path / 'coreference_combined_negation_results.csv', index=False)\n",
    "\n",
    "print(\"\\nCombined Results:\")\n",
    "print(combined_results)\n",
    "print(\"\\nCombined Negation Results:\")\n",
    "print(combined_neg_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "base_path = Path('../pretrained_language_models/coreference_resolution/tmp')\n",
    "modification_order =[\"B: Tem\", \"B: Geo\", \"B: Len\", \"O: Spell\",\"O: Cap\",\"O: Punc\",\n",
    "\"M: Deri\",\n",
    "\"M: Com\",\n",
    "\"Sx: Voice\",\n",
    "\"Sx: Gra\",\n",
    "\"Sx: Conj\",\n",
    "\"Sm: Con\",\n",
    "\"P: Neg\",\n",
    "\"P: Disc\",\n",
    "\"P: Senti\",\n",
    "\"G: Cas\",\n",
    "\"G: Dial\",]\n",
    "# Read the combined results\n",
    "df = pd.read_csv(base_path / 'coreference_combined_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "mod_mapping = {\n",
    "    'temporal_bias': 'B: Tem',\n",
    "    'geographical_bias': 'B: Geo', \n",
    "    'length_bias': 'B: Len',\n",
    "    'typo_bias': 'O: Spell',\n",
    "    'capitalization': 'O: Cap',\n",
    "    'punctuation': 'O: Punc',\n",
    "    'derivation': 'M: Deri',\n",
    "    'compound_word': 'M: Com',\n",
    "    'active_to_passive': 'Sx: Voice',\n",
    "    'grammatical_role': 'Sx: Gra',\n",
    "    'coordinating_conjunction': 'Sx: Conj',\n",
    "    'concept_replacement': 'Sm: Con',\n",
    "    'negation': 'P: Neg',\n",
    "    'discourse': 'P: Disc',\n",
    "    'sentiment': 'P: Senti',\n",
    "    'casual': 'G: Cas',\n",
    "    'dialectal': 'G: Dial'\n",
    "}\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Define model order\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='percentage_diff')\n",
    "p_values = df.pivot(index='modification', columns='model', values='p_value')\n",
    "\n",
    "# Reorder rows and columns\n",
    "pivot_df = pivot_df.reindex(modification_order, axis=0)\n",
    "pivot_df = pivot_df.reindex(model_order, axis=1)\n",
    "p_values = p_values.reindex(modification_order, axis=0)\n",
    "p_values = p_values.reindex(model_order, axis=1)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, p_val):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if p_val < 0.01:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif p_val < 0.05:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif p_val < 0.1:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if p_val < 0.01:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif p_val < 0.05:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif p_val < 0.1:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(pivot_df.columns) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in pivot_df.columns]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(val, p_values.loc[idx, col]) for col, val in row.items()]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open(base_path / 'coreference_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to .tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from CSV\n",
    "base_path = Path('../pretrained_language_models/coreference_resolution/tmp')\n",
    "\n",
    "results_df = pd.read_csv(base_path / 'coreference_combined_results.csv')\n",
    "\n",
    "# Create lists of unique modifications and models\n",
    "modification_order = list(mod_mapping.keys())\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "negation_order = ['verbal', 'lexical', 'double', 'approximate', 'absolute']\n",
    "# Create empty DataFrame with multi-level columns\n",
    "columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "results_df_pivot = pd.DataFrame(index=modification_order, columns=columns)\n",
    "\n",
    "# Fill DataFrame\n",
    "for mod in modification_order:\n",
    "    for model in model_order:\n",
    "        row = results_df[(results_df['modification'] == mod) & (results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            results_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'diff')] = row['percentage_diff'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "\n",
    "# Save to CSV\n",
    "results_df_pivot.to_csv(base_path / 'coreference_results_df.csv')\n",
    "\n",
    "# Save negation results to separate CSV\n",
    "# Load negation results from CSV\n",
    "negation_results_df = pd.read_csv(base_path / 'coreference_combined_negation_results.csv')\n",
    "\n",
    "# Create empty DataFrame with multi-level columns for negation results\n",
    "negation_df_pivot = pd.DataFrame(index=['negation'], columns=columns)\n",
    "\n",
    "# Fill DataFrame with negation results\n",
    "for mod in negation_order:\n",
    "    for model in model_order:\n",
    "        row = negation_results_df[(negation_results_df['modification'] == mod) & (negation_results_df['model'] == model)]\n",
    "        if not row.empty:   \n",
    "            negation_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            negation_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0] \n",
    "            negation_df_pivot.loc[mod, (model, 'diff')] = row['pct_diff'].values[0]\n",
    "\n",
    "# Save negation results to CSV\n",
    "negation_df_pivot.to_csv(base_path / 'coreference_negation_results_df.csv')\n",
    "\n",
    "\n",
    "print(\"Results saved to coreference_results_df.csv and coreference_negation_results_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "base_path = Path('dialogue_contradiction_detection/tmp')\n",
    "df = pd.read_csv(base_path / 'dialogue_combined_negation_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "negation_order= ['Verbal', 'Lexical', 'Double', 'Approximate', 'Absolute']\n",
    "mod_mapping = {\n",
    "    'verbal': 'Verbal',\n",
    "    'lexical': 'Lexical',\n",
    "    'double': 'Double',\n",
    "    'approximate': 'Approximate',\n",
    "    'absolute': 'Absolute',\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='pct_diff')\n",
    "p_values = df.pivot(index='modification', columns='model', values='pvalue')\n",
    "significance = df.pivot(index='modification', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows according to modification_order\n",
    "pivot_df = pivot_df.reindex(negation_order)\n",
    "p_values = p_values.reindex(negation_order)\n",
    "significance = significance.reindex(negation_order)\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(row[col], significance.loc[idx, col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open('dialogue_negation_type_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to ner_results_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "base_path = Path('coreference_resolution/tmp')\n",
    "df = pd.read_csv(base_path / 'coreference_combined_negation_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "negation_order= ['Verbal', 'Lexical', 'Double', 'Approximate', 'Absolute']\n",
    "mod_mapping = {\n",
    "    'verbal': 'Verbal',\n",
    "    'lexical': 'Lexical',\n",
    "    'double': 'Double',\n",
    "    'approximate': 'Approximate',\n",
    "    'absolute': 'Absolute',\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='pct_diff')\n",
    "p_values = df.pivot(index='modification', columns='model', values='pvalue')\n",
    "significance = df.pivot(index='modification', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows according to modification_order\n",
    "pivot_df = pivot_df.reindex(negation_order)\n",
    "p_values = p_values.reindex(negation_order)\n",
    "significance = significance.reindex(negation_order)\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(row[col], significance.loc[idx, col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open(base_path / 'coreference_negation_type_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to coreference_negation_type_results_table.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
