{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def parse_ner_predictions(input_file):\n",
    "    \"\"\"Parse NER predictions into structured format\"\"\"\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    parsed_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Get the original text, tokens and labels\n",
    "        text = item['text']\n",
    "        tokens = item['tokenized_text']\n",
    "        gold_labels = item['gold_label']\n",
    "        pred_labels = item['prediction']\n",
    "        \n",
    "        # Sanity check that lengths match\n",
    "        if len(tokens) != len(gold_labels) or len(tokens) != len(pred_labels):\n",
    "            raise ValueError(f\"Mismatched lengths in {input_file}: tokens={len(tokens)}, gold={len(gold_labels)}, pred={len(pred_labels)}\")\n",
    "        \n",
    "        # Build gold and prediction lists\n",
    "        gold = []\n",
    "        pred = []\n",
    "        \n",
    "        # Track multi-token entities\n",
    "        curr_gold_entity = ''\n",
    "        curr_gold_text = []\n",
    "        curr_pred_entity = ''\n",
    "        curr_pred_text = []\n",
    "        \n",
    "        for token, gold_label, pred_label in zip(tokens, gold_labels, pred_labels):\n",
    "            # Handle gold labels\n",
    "            if gold_label.startswith('B-'):\n",
    "                if curr_gold_entity:\n",
    "                    gold.append({'text': ' '.join(curr_gold_text), 'value': curr_gold_entity.upper()})\n",
    "                curr_gold_entity = gold_label[2:]\n",
    "                curr_gold_text = [token]\n",
    "            elif gold_label.startswith('I-'):\n",
    "                if curr_gold_entity == gold_label[2:]:\n",
    "                    curr_gold_text.append(token)\n",
    "            elif gold_label == 'O':\n",
    "                if curr_gold_entity:\n",
    "                    gold.append({'text': ' '.join(curr_gold_text), 'value': curr_gold_entity.upper()})\n",
    "                    curr_gold_entity = ''\n",
    "                    curr_gold_text = []\n",
    "            \n",
    "            # Handle predicted labels\n",
    "            if pred_label.startswith('B-'):\n",
    "                if curr_pred_entity:\n",
    "                    pred.append({'text': ' '.join(curr_pred_text), 'value': curr_pred_entity.upper()})\n",
    "                curr_pred_entity = pred_label[2:]\n",
    "                curr_pred_text = [token]\n",
    "            elif pred_label.startswith('I-'):\n",
    "                if curr_pred_entity == pred_label[2:]:\n",
    "                    curr_pred_text.append(token)\n",
    "            elif pred_label == 'O':\n",
    "                if curr_pred_entity:\n",
    "                    pred.append({'text': ' '.join(curr_pred_text), 'value': curr_pred_entity.upper()})\n",
    "                    curr_pred_entity = ''\n",
    "                    curr_pred_text = []\n",
    "        \n",
    "        # Add any remaining entities\n",
    "        if curr_gold_entity:\n",
    "            gold.append({'text': ' '.join(curr_gold_text), 'value': curr_gold_entity.upper()})\n",
    "        if curr_pred_entity:\n",
    "            pred.append({'text': ' '.join(curr_pred_text), 'value': curr_pred_entity.upper()})\n",
    "            \n",
    "        parsed_item = {\n",
    "            'text': text,\n",
    "            'gold': gold,\n",
    "            'prediction': pred\n",
    "        }\n",
    "        parsed_data.append(parsed_item)\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "def process_ner_files(model_name):\n",
    "    \"\"\"Process all NER files for a given model\"\"\"\n",
    "    input_dir = f'NER_{model_name}'\n",
    "    output_dir = f'parsed_NER_{model_name}'\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            \n",
    "            parsed_data = parse_ner_predictions(input_path)\n",
    "            \n",
    "            # Sanity check number of entities matches between input and parsed\n",
    "            with open(input_path, 'r') as f:\n",
    "                input_data = json.load(f)\n",
    "            total_gold_count_input = 0    \n",
    "            total_pred_count_input = 0\n",
    "            total_gold_count_parsed = 0\n",
    "            total_pred_count_parsed = 0\n",
    "            for i, (input_item, parsed_item) in enumerate(zip(input_data, parsed_data)):\n",
    "                gold_count = sum(1 for label in input_item['gold_label'] if label.startswith('B-'))\n",
    "                pred_count = sum(1 for label in input_item['prediction'] if label.startswith('B-'))\n",
    "                total_gold_count_input += gold_count\n",
    "                total_pred_count_input += pred_count\n",
    "                total_gold_count_parsed += len(parsed_item['gold'])\n",
    "                total_pred_count_parsed += len(parsed_item['prediction'])\n",
    "                if len(parsed_item['gold']) != gold_count:\n",
    "                    raise ValueError(f\"Mismatch in gold entities for item {i} in {filename}: {len(parsed_item['gold'])} vs {gold_count}\")\n",
    "                if len(parsed_item['prediction']) != pred_count:\n",
    "                    raise ValueError(f\"Mismatch in predicted entities for item {i} in {filename}: {len(parsed_item['prediction'])} vs {pred_count}\")\n",
    "            if filename == 'bert_active_to_passive_ori.json':\n",
    "                # Count entities by class for gold labels\n",
    "                input_gold_class_counts = {}\n",
    "                input_pred_class_counts = {}\n",
    "                parsed_gold_class_counts = {}\n",
    "                parsed_pred_class_counts = {}\n",
    "                \n",
    "                # Count from input data\n",
    "                for item in input_data:\n",
    "                    for label in item['gold_label']:\n",
    "                        if label.startswith('B-'):\n",
    "                            entity_class = label[2:]\n",
    "                            input_gold_class_counts[entity_class] = input_gold_class_counts.get(entity_class, 0) + 1\n",
    "                    for label in item['prediction']:\n",
    "                        if label.startswith('B-'):\n",
    "                            entity_class = label[2:]\n",
    "                            input_pred_class_counts[entity_class] = input_pred_class_counts.get(entity_class, 0) + 1\n",
    "                \n",
    "                # Count from parsed data            \n",
    "                for item in parsed_data:\n",
    "                    for entity in item['gold']:\n",
    "                        entity_class = entity['value']\n",
    "                        parsed_gold_class_counts[entity_class] = parsed_gold_class_counts.get(entity_class, 0) + 1\n",
    "                    for entity in item['prediction']:\n",
    "                        entity_class = entity['value'] \n",
    "                        parsed_pred_class_counts[entity_class] = parsed_pred_class_counts.get(entity_class, 0) + 1\n",
    "                \n",
    "                print(\"Input gold entity counts by class:\")\n",
    "                for entity_class, count in input_gold_class_counts.items():\n",
    "                    print(f\"{entity_class}: {count}\")\n",
    "                print(\"\\nInput predicted entity counts by class:\")\n",
    "                for entity_class, count in input_pred_class_counts.items():\n",
    "                    print(f\"{entity_class}: {count}\")\n",
    "                print(\"\\nParsed gold entity counts by class:\")\n",
    "                for entity_class, count in parsed_gold_class_counts.items():\n",
    "                    print(f\"{entity_class}: {count}\")\n",
    "                print(\"\\nParsed predicted entity counts by class:\")\n",
    "                for entity_class, count in parsed_pred_class_counts.items():\n",
    "                    print(f\"{entity_class}: {count}\")\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(parsed_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_ner_files('BERT')\n",
    "# process_ner_files('GPT2')\n",
    "# process_ner_files('T5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import ast\n",
    "import difflib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_f1_and_counts(example):\n",
    "    true_entities = []\n",
    "    pred_entities = []\n",
    "    \n",
    "    # Get entities from appropriate field names\n",
    "    gold_entities = example['gold']\n",
    "    pred_entities_raw = example['prediction']\n",
    "    \n",
    "    # Handle empty case\n",
    "    if not gold_entities and not pred_entities_raw:\n",
    "        return 0.0, {}\n",
    "\n",
    "    # Process gold entities\n",
    "    for entity in gold_entities:\n",
    "        if isinstance(entity, str):\n",
    "            entity = ast.literal_eval(entity)\n",
    "        # Store as tuple of (text, value, class) to handle duplicates\n",
    "        if entity.get('text') is not None and entity.get('value') is not None:\n",
    "            true_entities.append((entity['text'], entity['value'], entity['value']))\n",
    "        elif entity.get('text') is not None and entity.get('class') is not None:\n",
    "            true_entities.append((entity['text'], entity['class'], entity['class']))\n",
    "        else:\n",
    "            # Handle dictionary format entities\n",
    "            for key, value in entity.items():\n",
    "                if isinstance(value, str):\n",
    "                    true_entities.append((key, value, value))\n",
    "\n",
    "    # Process predicted entities\n",
    "    for entity in pred_entities_raw:\n",
    "        if isinstance(entity, str):\n",
    "            entity = ast.literal_eval(entity)\n",
    "        if entity.get('text') is not None:\n",
    "            pred_entities.append((entity['text'], entity['value'], entity['value']))\n",
    "        else:\n",
    "            for key, value in entity.items():\n",
    "                if isinstance(value, str):\n",
    "                    pred_entities.append((key, value, value))\n",
    "    \n",
    "    # Calculate per-class counts\n",
    "    class_counts = {}\n",
    "    # Get unique classes from both true and predicted entities to ensure complete coverage\n",
    "    classes = set(e[2] for e in true_entities) | set(e[2] for e in pred_entities)\n",
    "    \n",
    "    for cls in classes:\n",
    "        # Get entities for this class\n",
    "        true_cls = [e for e in true_entities if e[2] == cls]\n",
    "        pred_cls = [e for e in pred_entities if e[2] == cls]\n",
    "        \n",
    "        # Calculate counts for this class allowing for duplicates\n",
    "        tp = sum(1 for t in true_cls if t in pred_cls)\n",
    "        # Count false positives - predictions that don't match any gold entity\n",
    "        fp = len(pred_cls) - tp\n",
    "        # Count false negatives - gold entities that weren't predicted\n",
    "        fn = len(true_cls) - tp\n",
    "        \n",
    "        class_counts[cls] = (tp, fp, fn)\n",
    "    \n",
    "    # Calculate overall F1 for the example\n",
    "    total_tp = sum(counts[0] for counts in class_counts.values())\n",
    "    total_fp = sum(counts[1] for counts in class_counts.values())\n",
    "    total_fn = sum(counts[2] for counts in class_counts.values())\n",
    "    \n",
    "    # Handle edge case where no true positives\n",
    "    if total_tp == 0:\n",
    "        return 0.0, class_counts\n",
    "        \n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0, class_counts\n",
    "        \n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1, class_counts\n",
    "\n",
    "def get_f1_scores_and_counts(data):\n",
    "    if not data:\n",
    "        return [], {}\n",
    "        \n",
    "    scores_and_counts = [get_example_f1_and_counts(example) for example in data]\n",
    "    f1_scores = [score for score, _ in scores_and_counts]\n",
    "    \n",
    "    # Combine per-class counts across all examples\n",
    "    class_counts = {}\n",
    "    for _, example_counts in scores_and_counts:\n",
    "        for cls, (tp, fp, fn) in example_counts.items():\n",
    "            if cls not in class_counts:\n",
    "                class_counts[cls] = [0, 0, 0]\n",
    "            class_counts[cls][0] += tp  # Add true positives\n",
    "            class_counts[cls][1] += fp  # Add false positives\n",
    "            class_counts[cls][2] += fn  # Add false negatives\n",
    "            \n",
    "    return f1_scores, class_counts\n",
    "\n",
    "def calculate_micro_f1(counts):\n",
    "    if isinstance(counts, tuple):\n",
    "        tp, fp, fn = counts\n",
    "        if tp == 0:\n",
    "            return 0.0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        # Calculate micro F1 across all classes\n",
    "        total_tp = sum(counts[cls][0] for cls in counts)\n",
    "        total_fp = sum(counts[cls][1] for cls in counts)\n",
    "        total_fn = sum(counts[cls][2] for cls in counts)\n",
    "        \n",
    "        if total_tp == 0:\n",
    "            return {'micro_f1': 0.0, 'support': 0}\n",
    "            \n",
    "        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            micro_f1 = 0.0\n",
    "        else:\n",
    "            micro_f1 = 2 * (precision * recall) / (precision + recall)\n",
    "            \n",
    "        # Calculate per-class metrics\n",
    "        class_f1s = {'micro_f1': micro_f1}\n",
    "        total_support = 0\n",
    "        \n",
    "        for cls, (tp, fp, fn) in counts.items():\n",
    "            support = tp + fn  # Support is true positives + false negatives\n",
    "            total_support += support\n",
    "            \n",
    "            if tp == 0:\n",
    "                class_f1s[cls] = {'f1': 0.0, 'support': support}\n",
    "                continue\n",
    "                \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            \n",
    "            if precision + recall == 0:\n",
    "                class_f1s[cls] = {'f1': 0.0, 'support': support}\n",
    "            else:\n",
    "                f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                class_f1s[cls] = {'f1': f1, 'support': support}\n",
    "        \n",
    "        class_f1s['support'] = total_support\n",
    "        return class_f1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ast\n",
    "def load_json_file(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Get list of all modifications from filenames\n",
    "modifications = set()\n",
    "models = ['GPT2', 'BERT', 'T5']\n",
    "for model in models:\n",
    "    model_dir = f'parsed_NER_{model}'\n",
    "    for filename in os.listdir(model_dir):\n",
    "        if filename.startswith(f'{model.lower()}_') and filename.endswith('_ori.json'):\n",
    "            mod = filename.replace(f'{model.lower()}_', '').replace('_ori.json', '')\n",
    "            modifications.add(mod)\n",
    "modifications = list(modifications)\n",
    "\n",
    "# Load original and modified files for each model\n",
    "ori_files = {model: {} for model in models}\n",
    "modif_files = {model: {} for model in models}\n",
    "\n",
    "for model in models:\n",
    "    model_dir = f'parsed_NER_{model}'\n",
    "    for modification in modifications:\n",
    "        # Load original files\n",
    "        ori_filepath = f'{model_dir}/{model.lower()}_{modification}_ori.json'\n",
    "        ori_files[model][modification] = load_json_file(ori_filepath)\n",
    "        \n",
    "        # Load modified files\n",
    "        modif_filepath = f'{model_dir}/{model.lower()}_{modification}_modif.json'\n",
    "        modif_files[model][modification] = load_json_file(modif_filepath)\n",
    "\n",
    "\n",
    "# Calculate and store F1 scores for each model\n",
    "results = []\n",
    "negation_type_results = []  # For storing negation type breakdown\n",
    "\n",
    "for model in models:\n",
    "    for modification in modifications:\n",
    "        compare_file = Path(f'../../data/modified_data/ner/{modification}_100.json')\n",
    "        if not compare_file.exists():\n",
    "            continue\n",
    "        compare_df = json.load(open(compare_file))\n",
    "        if len(compare_df) != len(ori_files[model][modification]):\n",
    "            print('mismatch',modification, model)\n",
    "        ori_f1_scores, ori_counts = get_f1_scores_and_counts(ori_files[model][modification])\n",
    "        modif_f1_scores, modif_counts = get_f1_scores_and_counts(modif_files[model][modification])\n",
    "        # Calculate mean F1 scores\n",
    "        ori_mean_f1 = np.mean(ori_f1_scores)\n",
    "        modif_mean_f1 = np.mean(modif_f1_scores)\n",
    "        # Calculate micro F1 scores\n",
    "        ori_micro_f1 = calculate_micro_f1(ori_counts)\n",
    "        modif_micro_f1 = calculate_micro_f1(modif_counts)\n",
    "        # Calculate percentage change\n",
    "        mean_f1_pct_change = ((modif_mean_f1 - ori_mean_f1) / ori_mean_f1) * 100\n",
    "        micro_f1_pct_change = ((modif_micro_f1['micro_f1'] - ori_micro_f1['micro_f1']) / ori_micro_f1['micro_f1']) * 100\n",
    "        \n",
    "        # Perform paired t-test on per-example F1 scores\n",
    "        _, p_wilcoxon = stats.wilcoxon(ori_f1_scores, modif_f1_scores)\n",
    "        _, p_mannwhitney = stats.mannwhitneyu(ori_f1_scores, modif_f1_scores)\n",
    "        p_value = min(p_wilcoxon, p_mannwhitney)\n",
    "        \n",
    "        # Determine significance level\n",
    "        if p_value < 0.01:\n",
    "            significance = \"**\"\n",
    "        elif p_value < 0.05:\n",
    "            significance = \"*\"\n",
    "        elif p_value < 0.1:\n",
    "            significance = \".\"\n",
    "        else:\n",
    "            significance = \"ns\"\n",
    "        \n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'modification': modification,\n",
    "            'original_mean_f1': ori_mean_f1,\n",
    "            'modified_mean_f1': modif_mean_f1,\n",
    "            'mean_f1_pct_change': mean_f1_pct_change,\n",
    "            'original_micro_f1': ori_micro_f1,\n",
    "            'modified_micro_f1': modif_micro_f1,\n",
    "            'micro_f1_pct_change': micro_f1_pct_change,\n",
    "            'p_value': p_value,\n",
    "            'significance': significance\n",
    "        })\n",
    "        \n",
    "        # Additional analysis for negation types\n",
    "        if modification == 'negation':\n",
    "            # Load negation type information\n",
    "            negation_file = Path(f'../../data/modified_data/ner/negation_100.json')\n",
    "            negation_data = json.load(open(negation_file))\n",
    "            \n",
    "            # Group examples by negation type\n",
    "            type_results = {}\n",
    "            for idx, (ori_f1, mod_f1) in enumerate(zip(ori_f1_scores, modif_f1_scores)):\n",
    "                neg_type = negation_data[idx].get('subtype', 'unknown')\n",
    "                if neg_type not in type_results:\n",
    "                    type_results[neg_type] = {'ori_f1s': [], 'mod_f1s': []}\n",
    "                type_results[neg_type]['ori_f1s'].append(ori_f1)\n",
    "                type_results[neg_type]['mod_f1s'].append(mod_f1)\n",
    "            \n",
    "            # Calculate metrics for each negation type\n",
    "            for neg_type, scores in type_results.items():\n",
    "                ori_mean = np.mean(scores['ori_f1s'])\n",
    "                mod_mean = np.mean(scores['mod_f1s'])\n",
    "                pct_change = ((mod_mean - ori_mean) / ori_mean) * 100 if ori_mean > 0 else 0\n",
    "                \n",
    "                # Statistical tests\n",
    "                if len(scores['ori_f1s']) > 1:  # Only if we have enough samples\n",
    "                    _, p_wilcoxon = stats.wilcoxon(scores['ori_f1s'], scores['mod_f1s'])\n",
    "                    _, p_mannwhitney = stats.mannwhitneyu(scores['ori_f1s'], scores['mod_f1s'])\n",
    "                    p_value = min(p_wilcoxon, p_mannwhitney)\n",
    "                else:\n",
    "                    p_value = 1.0\n",
    "                \n",
    "                # Determine significance level for negation types\n",
    "                if p_value < 0.01:\n",
    "                    significance = \"**\"\n",
    "                elif p_value < 0.05:\n",
    "                    significance = \"*\"\n",
    "                elif p_value < 0.1:\n",
    "                    significance = \".\"\n",
    "                else:\n",
    "                    significance = \"ns\"\n",
    "                \n",
    "                negation_type_results.append({\n",
    "                    'model': model,\n",
    "                    'negation_type': neg_type,\n",
    "                    'original_mean_f1': ori_mean,\n",
    "                    'modified_mean_f1': mod_mean,\n",
    "                    'mean_f1_pct_change': pct_change,\n",
    "                    'sample_size': len(scores['ori_f1s']),\n",
    "                    'p_value': p_value,\n",
    "                    'significance': significance\n",
    "                })\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('ner_modification_results_plm.csv', index=False)\n",
    "\n",
    "# Save negation type results\n",
    "if negation_type_results:\n",
    "    df_negation = pd.DataFrame(negation_type_results)\n",
    "    df_negation.to_csv('ner_negation_type_results_plm.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_f1_and_counts_list(gold, pred):\n",
    "    # Convert string labels to lists if needed\n",
    "    if isinstance(gold, str):\n",
    "        gold = ast.literal_eval(gold)\n",
    "    if isinstance(pred, str):\n",
    "        pred = ast.literal_eval(pred)\n",
    "\n",
    "    # Standardize format to list of dicts with 'text' and 'value' keys\n",
    "    def standardize_format(data):\n",
    "        if isinstance(data, dict):\n",
    "            return [{'text': k, 'value': v} for k, v in data.items()]\n",
    "        elif isinstance(data, list) and len(data) > 0:\n",
    "            if isinstance(data[0], dict):\n",
    "                standardized = []\n",
    "                for item in data:\n",
    "                    if 'text' not in item:\n",
    "                        for text, value in item.items():\n",
    "                            standardized.append({'text': text, 'value': value})\n",
    "                    else:\n",
    "                        standardized.append(item)\n",
    "                return standardized\n",
    "        return data\n",
    "\n",
    "    gold = standardize_format(gold)\n",
    "    pred = standardize_format(pred)\n",
    "\n",
    "    # Calculate metrics by comparing each prediction against gold\n",
    "    tp = 0\n",
    "    gold_matched = [False] * len(gold)\n",
    "    pred_matched = [False] * len(pred)\n",
    "\n",
    "    # First pass - find exact matches\n",
    "    for i, p in enumerate(pred):\n",
    "        for j, g in enumerate(gold):\n",
    "            if not gold_matched[j] and not pred_matched[i]:\n",
    "                if p['text'] == g['text'] and p['value'] == g['value']:\n",
    "                    tp += 1\n",
    "                    gold_matched[j] = True\n",
    "                    pred_matched[i] = True\n",
    "\n",
    "    # Calculate false positives and false negatives\n",
    "    fp = len(pred) - tp  # Predictions that didn't match any gold\n",
    "    fn = len(gold) - tp  # Gold entities that weren't matched\n",
    "\n",
    "    # Calculate F1 score for this example\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return f1, (tp, fp, fn)\n",
    "\n",
    "def calculate_micro_f1_list(counts):\n",
    "    # Sum up all true positives, false positives, and false negatives\n",
    "    tp = sum(count[0] for count in counts)\n",
    "    fp = sum(count[1] for count in counts)\n",
    "    fn = sum(count[2] for count in counts)\n",
    "\n",
    "    # Calculate micro-averaged precision and recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    # Calculate micro F1\n",
    "    micro_f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return micro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and analyze modification results from ner directory\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "ner_results_dir = '../../eval/results/ner/'\n",
    "ner_results_files = glob.glob(os.path.join(ner_results_dir, '*.csv'))\n",
    "\n",
    "print(\"\\nAnalyzing results from ner directory:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create list to store results\n",
    "results_data = []\n",
    "negation_results_data = []\n",
    "\n",
    "for results_file in ner_results_files:\n",
    "    # Extract model and modification from filename\n",
    "    filename = os.path.basename(results_file)\n",
    "    print(filename)\n",
    "    if 'DP' in filename or 'ner' in filename:\n",
    "        continue\n",
    "    model = filename.split('-')[0]\n",
    "    # if model == 'gpt4o':\n",
    "    #     if 'new' not in filename:\n",
    "    #         continue\n",
    "    #     modification = filename.split('-')[2].replace('_100_new.csv', '')\n",
    "    # else:\n",
    "    modification = filename.split('-')[2].replace('_100_new.csv', '')\n",
    "    print(modification)\n",
    "    \n",
    "    print(f\"\\nResults from {filename}:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(results_file)\n",
    "    compare_file = Path(f'../../data/modified_data/ner/{modification}_100.json')\n",
    "    if not compare_file.exists():\n",
    "        continue\n",
    "    compare_df = json.load(open(compare_file))\n",
    "    if len(compare_df) != len(df):\n",
    "        print('mismatch',modification, model)\n",
    "    # Calculate macro F1 scores\n",
    "    # Get labels and predictions\n",
    "    # Get original and modified labels/predictions\n",
    "    ori_labels = df['original_label'].values\n",
    "    ori_preds = df['original_pred'].values\n",
    "    mod_labels = df['modified_label'].values\n",
    "    mod_preds = df['modified_pred'].values\n",
    "    # Calculate F1 scores using helper functions\n",
    "    ori_f1_scores = []\n",
    "    modif_f1_scores = []\n",
    "    print('original')\n",
    "    for l, p in tqdm(zip(ori_labels, ori_preds)):\n",
    "        f1, _ = get_example_f1_and_counts_list(l, p)\n",
    "        ori_f1_scores.append(f1)\n",
    "    print('modified')\n",
    "    for l, p in tqdm(zip(mod_labels, mod_preds)):\n",
    "        f1, _ = get_example_f1_and_counts_list(l, p)\n",
    "        modif_f1_scores.append(f1)\n",
    "    print('done calculating f1 scores')\n",
    "    # Calculate mean F1 scores\n",
    "    ori_mean_f1 = np.mean(ori_f1_scores)\n",
    "    modif_mean_f1 = np.mean(modif_f1_scores)\n",
    "    # Calculate percentage difference\n",
    "    mean_pct_diff = ((modif_mean_f1 - ori_mean_f1) / ori_mean_f1) * 100\n",
    "    # Perform t-test\n",
    "    _, p_value_mw = stats.mannwhitneyu(ori_f1_scores, modif_f1_scores, alternative='two-sided')\n",
    "    _, p_value_w = stats.wilcoxon(ori_f1_scores, modif_f1_scores)\n",
    "    p_value = min(p_value_mw, p_value_w)\n",
    "    \n",
    "    # Determine significance level\n",
    "    if p_value < 0.01:\n",
    "        significance = \"**\"\n",
    "    elif p_value < 0.05:\n",
    "        significance = \"*\"\n",
    "    elif p_value < 0.1:\n",
    "        significance = \".\"\n",
    "    else:\n",
    "        significance = \"ns\"\n",
    "        \n",
    "    print(f\"\\n{model} - {modification.upper()} Modification:\")\n",
    "    print(f\"Original Mean F1: {ori_mean_f1:.3f}\")\n",
    "    print(f\"Modified Mean F1: {modif_mean_f1:.3f}\")\n",
    "    print(f\"Mean F1 Percentage Change: {mean_pct_diff:.1f}%\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    print(f\"Significance: {significance}\")\n",
    "    \n",
    "    # For negation, get subtype results\n",
    "    if modification == 'negation':\n",
    "        for subtype in ['verbal', 'lexical', 'double', 'approximate', 'absolute']:\n",
    "            subtype_df = df[df['type'] == subtype]\n",
    "            if len(subtype_df) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate F1 scores for subtype\n",
    "            ori_subtype_f1 = []\n",
    "            mod_subtype_f1 = []\n",
    "            \n",
    "            for l, p in zip(subtype_df['original_label'], subtype_df['original_pred']):\n",
    "                f1, _ = get_example_f1_and_counts_list(l, p)\n",
    "                ori_subtype_f1.append(f1)\n",
    "                \n",
    "            for l, p in zip(subtype_df['modified_label'], subtype_df['modified_pred']):\n",
    "                f1, _ = get_example_f1_and_counts_list(l, p)\n",
    "                mod_subtype_f1.append(f1)\n",
    "                \n",
    "            # Calculate stats\n",
    "            ori_mean = np.mean(ori_subtype_f1)\n",
    "            mod_mean = np.mean(mod_subtype_f1)\n",
    "            pct_diff = ((mod_mean - ori_mean) / ori_mean) * 100\n",
    "            \n",
    "            # Statistical tests\n",
    "            _, p_mw = stats.mannwhitneyu(ori_subtype_f1, mod_subtype_f1, alternative='two-sided')\n",
    "            _, p_w = stats.wilcoxon(ori_subtype_f1, mod_subtype_f1)\n",
    "            p_val = min(p_mw, p_w)\n",
    "            \n",
    "            # Determine significance\n",
    "            if p_val < 0.001:\n",
    "                sig = '***'\n",
    "            elif p_val < 0.01:\n",
    "                sig = '**'\n",
    "            elif p_val < 0.05:\n",
    "                sig = '*'\n",
    "            elif p_val < 0.1:\n",
    "                sig = '.'\n",
    "            else:\n",
    "                sig = 'ns'\n",
    "                \n",
    "            # Store subtype results\n",
    "            # results_data.append({\n",
    "            #     'model': model,\n",
    "            #     'modification': f'negation_{subtype}',\n",
    "            #     'original_mean_f1': ori_mean,\n",
    "            #     'modified_mean_f1': mod_mean,\n",
    "            #     'mean_f1_pct_change': pct_diff,\n",
    "            #     'p_value': p_val,\n",
    "            #     'significance': sig\n",
    "            # })\n",
    "            \n",
    "            # Also store in negation results\n",
    "            negation_results_data.append({\n",
    "                'model': model,\n",
    "                'negation_type': subtype,\n",
    "                'original_mean_f1': ori_mean,\n",
    "                'modified_mean_f1': mod_mean,\n",
    "                'mean_f1_pct_change': pct_diff,\n",
    "                'sample_size': len(subtype_df),\n",
    "                'p_value': p_val,\n",
    "                'significance': sig\n",
    "            })\n",
    "    \n",
    "    # Store main results\n",
    "    results_data.append({\n",
    "        'model': model,\n",
    "        'modification': modification,\n",
    "        'original_mean_f1': ori_mean_f1,\n",
    "        'modified_mean_f1': modif_mean_f1,\n",
    "        'mean_f1_pct_change': mean_pct_diff,\n",
    "        'p_value': p_value,\n",
    "        'significance': significance\n",
    "    })\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv('ner_modification_results_llm.csv', index=False)\n",
    "print(\"\\nResults saved to ner_modification_results_llm.csv\")\n",
    "\n",
    "# Save negation results\n",
    "negation_results_df = pd.DataFrame(negation_results_data)\n",
    "negation_results_df.to_csv('ner_negation_type_results_llm.csv', index=False)\n",
    "print(\"\\nNegation results saved to ner_negation_type_results_llm.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read both CSV files\n",
    "llm_results = pd.read_csv('ner_modification_results_llm.csv')\n",
    "plm_results = pd.read_csv('ner_modification_results_plm.csv')\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_results = pd.concat([llm_results, plm_results], ignore_index=True)\n",
    "\n",
    "# Save combined results\n",
    "combined_results.to_csv('ner_modification_results_combined.csv', index=False)\n",
    "print(\"\\nCombined results saved to ner_modification_results_combined.csv\")\n",
    "\n",
    "negation_results_llm = pd.read_csv('ner_negation_type_results_llm.csv')\n",
    "negation_results_plm = pd.read_csv('ner_negation_type_results_plm.csv')\n",
    "\n",
    "combined_negation_results = pd.concat([negation_results_llm, negation_results_plm], ignore_index=True)\n",
    "\n",
    "combined_negation_results.to_csv('ner_negation_type_results_combined.csv', index=False)\n",
    "print(\"\\nCombined negation results saved to ner_negation_type_results_combined.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification_order =[\"B: Tem\", \"B: Geo\", \"B: Len\", \"O: Spell\",\"O: Cap\",\"O: Punc\",\n",
    "\"M: Deri\",\n",
    "\"M: Com\",\n",
    "\"Sx: Voice\",\n",
    "\"Sx: Gra\",\n",
    "\"Sx: Conj\",\n",
    "\"Sm: Con\",\n",
    "\"P: Neg\",\n",
    "\"P: Disc\",\n",
    "\"P: Senti\",\n",
    "\"G: Cas\",\n",
    "\"G: Dial\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "df = pd.read_csv('ner_modification_results_combined.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "mod_mapping = {\n",
    "    'temporal_bias': 'B: Tem',\n",
    "    'geographical_bias': 'B: Geo', \n",
    "    'length_bias': 'B: Len',\n",
    "    'typo_bias': 'O: Spell',\n",
    "    'capitalization': 'O: Cap',\n",
    "    'punctuation': 'O: Punc',\n",
    "    'derivation': 'M: Deri',\n",
    "    'compound_word': 'M: Com',\n",
    "    'active_to_passive': 'Sx: Voice',\n",
    "    'grammatical_role': 'Sx: Gra',\n",
    "    'coordinating_conjunction': 'Sx: Conj',\n",
    "    'concept_replacement': 'Sm: Con',\n",
    "    'negation': 'P: Neg',\n",
    "    'discourse': 'P: Disc',\n",
    "    'sentiment': 'P: Senti',\n",
    "    'casual': 'G: Cas',\n",
    "    'dialectal': 'G: Dial'\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "model_order = ['BERT', 'GPT2', 'T5', 'gpt4o', 'claude', 'llama']\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='mean_f1_pct_change')\n",
    "p_values = df.pivot(index='modification', columns='model', values='p_value')\n",
    "significance = df.pivot(index='modification', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows according to modification_order\n",
    "pivot_df = pivot_df.reindex(modification_order)\n",
    "p_values = p_values.reindex(modification_order)\n",
    "significance = significance.reindex(modification_order)\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(row[col], significance.loc[idx, col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open('ner_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to ner_results_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from CSV\n",
    "results_df = pd.read_csv('ner_modification_results_combined.csv')\n",
    "negation_results_df = pd.read_csv('ner_negation_type_results_combined.csv')\n",
    "# Create lists of unique modifications and models\n",
    "modification_order = list(mod_mapping.keys())\n",
    "model_order = ['BERT', 'GPT2', 'T5', 'gpt4o', 'claude', 'llama']\n",
    "negation_order = ['verbal', 'lexical', 'double', 'approximate', 'absolute']\n",
    "# Create empty DataFrame with multi-level columns\n",
    "columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "results_df_pivot = pd.DataFrame(index=modification_order, columns=columns)\n",
    "\n",
    "negation_columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "negation_results_df_pivot = pd.DataFrame(index=negation_order, columns=negation_columns)\n",
    "# Fill DataFrame\n",
    "for mod in modification_order:\n",
    "    for model in model_order:\n",
    "        row = results_df[(results_df['modification'] == mod) & (results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            results_df_pivot.loc[mod, (model, 'original')] = row['original_mean_f1'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'modified')] = row['modified_mean_f1'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'diff')] = row['mean_f1_pct_change'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "        \n",
    "for mod in negation_order:\n",
    "    for model in model_order:\n",
    "        row = negation_results_df[(negation_results_df['negation_type'] == mod) & (negation_results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            negation_results_df_pivot.loc[mod, (model, 'original')] = row['original_mean_f1'].values[0]\n",
    "            negation_results_df_pivot.loc[mod, (model, 'modified')] = row['modified_mean_f1'].values[0]\n",
    "            negation_results_df_pivot.loc[mod, (model, 'diff')] = row['mean_f1_pct_change'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "results_df_pivot.to_csv('ner_results_df.csv')\n",
    "negation_results_df_pivot.to_csv('ner_negation_results_df.csv')\n",
    "\n",
    "print(\"Results saved to ner_results_df.csv\")\n",
    "print(\"Negation results saved to ner_negation_results_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "df = pd.read_csv('ner_negation_type_results_combined.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "negation_order= ['Verbal', 'Lexical', 'Double', 'Approximate', 'Absolute']\n",
    "mod_mapping = {\n",
    "    'verbal': 'Verbal',\n",
    "    'lexical': 'Lexical',\n",
    "    'double': 'Double',\n",
    "    'approximate': 'Approximate',\n",
    "    'absolute': 'Absolute',\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "model_order = ['BERT', 'GPT2', 'T5', 'gpt4o', 'claude', 'llama']\n",
    "\n",
    "# Map the modification names\n",
    "df['negation_type'] = df['negation_type'].map(mod_mapping)\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='negation_type', columns='model', values='mean_f1_pct_change')\n",
    "p_values = df.pivot(index='negation_type', columns='model', values='p_value')\n",
    "significance = df.pivot(index='negation_type', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows according to modification_order\n",
    "pivot_df = pivot_df.reindex(negation_order)\n",
    "p_values = p_values.reindex(negation_order)\n",
    "significance = significance.reindex(negation_order)\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(row[col], significance.loc[idx, col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open('ner_negation_type_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to ner_results_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
