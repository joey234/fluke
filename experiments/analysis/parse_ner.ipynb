{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def parse_ner_predictions(input_file):\n",
    "    \"\"\"Parse NER predictions into structured format\"\"\"\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    parsed_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Get the original text, tokens and labels\n",
    "        text = item['text']\n",
    "        tokens = item['tokenized_text']\n",
    "        gold_labels = item['gold_label']\n",
    "        pred_labels = item['prediction']\n",
    "        \n",
    "        # Sanity check that lengths match\n",
    "        if len(tokens) != len(gold_labels) or len(tokens) != len(pred_labels):\n",
    "            raise ValueError(f\"Mismatched lengths in {input_file}: tokens={len(tokens)}, gold={len(gold_labels)}, pred={len(pred_labels)}\")\n",
    "        \n",
    "        # Build gold and prediction lists\n",
    "        gold = []\n",
    "        pred = []\n",
    "        \n",
    "        # Track multi-token entities\n",
    "        curr_gold_entity = ''\n",
    "        curr_gold_text = []\n",
    "        curr_pred_entity = ''\n",
    "        curr_pred_text = []\n",
    "        \n",
    "        for token, gold_label, pred_label in zip(tokens, gold_labels, pred_labels):\n",
    "            # Handle gold labels\n",
    "            if gold_label.startswith('B-'):\n",
    "                if curr_gold_entity:\n",
    "                    gold.append({'text': ' '.join(curr_gold_text), 'value': curr_gold_entity.upper()})\n",
    "                curr_gold_entity = gold_label[2:]\n",
    "                curr_gold_text = [token]\n",
    "            elif gold_label.startswith('I-'):\n",
    "                if curr_gold_entity == gold_label[2:]:\n",
    "                    curr_gold_text.append(token)\n",
    "            elif gold_label == 'O':\n",
    "                if curr_gold_entity:\n",
    "                    gold.append({'text': ' '.join(curr_gold_text), 'value': curr_gold_entity.upper()})\n",
    "                    curr_gold_entity = ''\n",
    "                    curr_gold_text = []\n",
    "            \n",
    "            # Handle predicted labels\n",
    "            if pred_label.startswith('B-'):\n",
    "                if curr_pred_entity:\n",
    "                    pred.append({'text': ' '.join(curr_pred_text), 'value': curr_pred_entity.upper()})\n",
    "                curr_pred_entity = pred_label[2:]\n",
    "                curr_pred_text = [token]\n",
    "            elif pred_label.startswith('I-'):\n",
    "                if curr_pred_entity == pred_label[2:]:\n",
    "                    curr_pred_text.append(token)\n",
    "            elif pred_label == 'O':\n",
    "                if curr_pred_entity:\n",
    "                    pred.append({'text': ' '.join(curr_pred_text), 'value': curr_pred_entity.upper()})\n",
    "                    curr_pred_entity = ''\n",
    "                    curr_pred_text = []\n",
    "        \n",
    "        # Add any remaining entities\n",
    "        if curr_gold_entity:\n",
    "            gold.append({'text': ' '.join(curr_gold_text), 'value': curr_gold_entity.upper()})\n",
    "        if curr_pred_entity:\n",
    "            pred.append({'text': ' '.join(curr_pred_text), 'value': curr_pred_entity.upper()})\n",
    "            \n",
    "        parsed_item = {\n",
    "            'text': text,\n",
    "            'gold': gold,\n",
    "            'prediction': pred\n",
    "        }\n",
    "        parsed_data.append(parsed_item)\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "def process_ner_files(model_name):\n",
    "    \"\"\"Process all NER files for a given model\"\"\"\n",
    "    input_dir = f'NER_{model_name}'\n",
    "    output_dir = f'parsed_NER_{model_name}'\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            \n",
    "            parsed_data = parse_ner_predictions(input_path)\n",
    "            \n",
    "            # Sanity check number of entities matches between input and parsed\n",
    "            with open(input_path, 'r') as f:\n",
    "                input_data = json.load(f)\n",
    "            total_gold_count_input = 0    \n",
    "            total_pred_count_input = 0\n",
    "            total_gold_count_parsed = 0\n",
    "            total_pred_count_parsed = 0\n",
    "            for i, (input_item, parsed_item) in enumerate(zip(input_data, parsed_data)):\n",
    "                gold_count = sum(1 for label in input_item['gold_label'] if label.startswith('B-'))\n",
    "                pred_count = sum(1 for label in input_item['prediction'] if label.startswith('B-'))\n",
    "                total_gold_count_input += gold_count\n",
    "                total_pred_count_input += pred_count\n",
    "                total_gold_count_parsed += len(parsed_item['gold'])\n",
    "                total_pred_count_parsed += len(parsed_item['prediction'])\n",
    "                if len(parsed_item['gold']) != gold_count:\n",
    "                    raise ValueError(f\"Mismatch in gold entities for item {i} in {filename}: {len(parsed_item['gold'])} vs {gold_count}\")\n",
    "                if len(parsed_item['prediction']) != pred_count:\n",
    "                    raise ValueError(f\"Mismatch in predicted entities for item {i} in {filename}: {len(parsed_item['prediction'])} vs {pred_count}\")\n",
    "            if filename == 'bert_active_to_passive_ori.json':\n",
    "                # Count entities by class for gold labels\n",
    "                input_gold_class_counts = {}\n",
    "                input_pred_class_counts = {}\n",
    "                parsed_gold_class_counts = {}\n",
    "                parsed_pred_class_counts = {}\n",
    "                \n",
    "                # Count from input data\n",
    "                for item in input_data:\n",
    "                    for label in item['gold_label']:\n",
    "                        if label.startswith('B-'):\n",
    "                            entity_class = label[2:]\n",
    "                            input_gold_class_counts[entity_class] = input_gold_class_counts.get(entity_class, 0) + 1\n",
    "                    for label in item['prediction']:\n",
    "                        if label.startswith('B-'):\n",
    "                            entity_class = label[2:]\n",
    "                            input_pred_class_counts[entity_class] = input_pred_class_counts.get(entity_class, 0) + 1\n",
    "                \n",
    "                # Count from parsed data            \n",
    "                for item in parsed_data:\n",
    "                    for entity in item['gold']:\n",
    "                        entity_class = entity['value']\n",
    "                        parsed_gold_class_counts[entity_class] = parsed_gold_class_counts.get(entity_class, 0) + 1\n",
    "                    for entity in item['prediction']:\n",
    "                        entity_class = entity['value'] \n",
    "                        parsed_pred_class_counts[entity_class] = parsed_pred_class_counts.get(entity_class, 0) + 1\n",
    "                \n",
    "                print(\"Input gold entity counts by class:\")\n",
    "                for entity_class, count in input_gold_class_counts.items():\n",
    "                    print(f\"{entity_class}: {count}\")\n",
    "                print(\"\\nInput predicted entity counts by class:\")\n",
    "                for entity_class, count in input_pred_class_counts.items():\n",
    "                    print(f\"{entity_class}: {count}\")\n",
    "                print(\"\\nParsed gold entity counts by class:\")\n",
    "                for entity_class, count in parsed_gold_class_counts.items():\n",
    "                    print(f\"{entity_class}: {count}\")\n",
    "                print(\"\\nParsed predicted entity counts by class:\")\n",
    "                for entity_class, count in parsed_pred_class_counts.items():\n",
    "                    print(f\"{entity_class}: {count}\")\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(parsed_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input gold entity counts by class:\n",
      "organization: 41\n",
      "location: 39\n",
      "person: 55\n",
      "art: 9\n",
      "product: 18\n",
      "building: 10\n",
      "other: 23\n",
      "event: 12\n",
      "\n",
      "Input predicted entity counts by class:\n",
      "organization: 35\n",
      "location: 43\n",
      "person: 57\n",
      "art: 16\n",
      "product: 14\n",
      "building: 14\n",
      "other: 20\n",
      "event: 11\n",
      "\n",
      "Parsed gold entity counts by class:\n",
      "ORGANIZATION: 41\n",
      "LOCATION: 39\n",
      "PERSON: 55\n",
      "ART: 9\n",
      "PRODUCT: 18\n",
      "BUILDING: 10\n",
      "OTHER: 23\n",
      "EVENT: 12\n",
      "\n",
      "Parsed predicted entity counts by class:\n",
      "ORGANIZATION: 35\n",
      "LOCATION: 43\n",
      "PERSON: 57\n",
      "ART: 16\n",
      "PRODUCT: 14\n",
      "BUILDING: 14\n",
      "OTHER: 20\n",
      "EVENT: 11\n"
     ]
    }
   ],
   "source": [
    "process_ner_files('BERT')\n",
    "# process_ner_files('GPT2')\n",
    "# process_ner_files('T5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import ast\n",
    "import difflib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_f1_and_counts(example):\n",
    "    true_entities = []\n",
    "    pred_entities = []\n",
    "    \n",
    "    # Get entities from appropriate field names\n",
    "    gold_entities = example['gold']\n",
    "    pred_entities_raw = example['prediction']\n",
    "    \n",
    "    # Handle empty case\n",
    "    if not gold_entities and not pred_entities_raw:\n",
    "        return 0.0, {}\n",
    "\n",
    "    # Process gold entities\n",
    "    for entity in gold_entities:\n",
    "        if isinstance(entity, str):\n",
    "            entity = ast.literal_eval(entity)\n",
    "        # Store as tuple of (text, value, class) to handle duplicates\n",
    "        if entity.get('text') is not None and entity.get('value') is not None:\n",
    "            true_entities.append((entity['text'], entity['value'], entity['value']))\n",
    "        elif entity.get('text') is not None and entity.get('class') is not None:\n",
    "            true_entities.append((entity['text'], entity['class'], entity['class']))\n",
    "        else:\n",
    "            # Handle dictionary format entities\n",
    "            for key, value in entity.items():\n",
    "                if isinstance(value, str):\n",
    "                    true_entities.append((key, value, value))\n",
    "\n",
    "    # Process predicted entities\n",
    "    for entity in pred_entities_raw:\n",
    "        if isinstance(entity, str):\n",
    "            entity = ast.literal_eval(entity)\n",
    "        if entity.get('text') is not None:\n",
    "            pred_entities.append((entity['text'], entity['value'], entity['value']))\n",
    "        else:\n",
    "            for key, value in entity.items():\n",
    "                if isinstance(value, str):\n",
    "                    pred_entities.append((key, value, value))\n",
    "    \n",
    "    # Calculate per-class counts\n",
    "    class_counts = {}\n",
    "    # Get unique classes from both true and predicted entities to ensure complete coverage\n",
    "    classes = set(e[2] for e in true_entities) | set(e[2] for e in pred_entities)\n",
    "    \n",
    "    for cls in classes:\n",
    "        # Get entities for this class\n",
    "        true_cls = [e for e in true_entities if e[2] == cls]\n",
    "        pred_cls = [e for e in pred_entities if e[2] == cls]\n",
    "        \n",
    "        # Calculate counts for this class allowing for duplicates\n",
    "        tp = sum(1 for t in true_cls if t in pred_cls)\n",
    "        # Count false positives - predictions that don't match any gold entity\n",
    "        fp = len(pred_cls) - tp\n",
    "        # Count false negatives - gold entities that weren't predicted\n",
    "        fn = len(true_cls) - tp\n",
    "        \n",
    "        class_counts[cls] = (tp, fp, fn)\n",
    "    \n",
    "    # Calculate overall F1 for the example\n",
    "    total_tp = sum(counts[0] for counts in class_counts.values())\n",
    "    total_fp = sum(counts[1] for counts in class_counts.values())\n",
    "    total_fn = sum(counts[2] for counts in class_counts.values())\n",
    "    \n",
    "    # Handle edge case where no true positives\n",
    "    if total_tp == 0:\n",
    "        return 0.0, class_counts\n",
    "        \n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0, class_counts\n",
    "        \n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1, class_counts\n",
    "\n",
    "def get_f1_scores_and_counts(data):\n",
    "    if not data:\n",
    "        return [], {}\n",
    "        \n",
    "    scores_and_counts = [get_example_f1_and_counts(example) for example in data]\n",
    "    f1_scores = [score for score, _ in scores_and_counts]\n",
    "    \n",
    "    # Combine per-class counts across all examples\n",
    "    class_counts = {}\n",
    "    for _, example_counts in scores_and_counts:\n",
    "        for cls, (tp, fp, fn) in example_counts.items():\n",
    "            if cls not in class_counts:\n",
    "                class_counts[cls] = [0, 0, 0]\n",
    "            class_counts[cls][0] += tp  # Add true positives\n",
    "            class_counts[cls][1] += fp  # Add false positives\n",
    "            class_counts[cls][2] += fn  # Add false negatives\n",
    "            \n",
    "    return f1_scores, class_counts\n",
    "\n",
    "def calculate_micro_f1(counts):\n",
    "    if isinstance(counts, tuple):\n",
    "        tp, fp, fn = counts\n",
    "        if tp == 0:\n",
    "            return 0.0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        # Calculate micro F1 across all classes\n",
    "        total_tp = sum(counts[cls][0] for cls in counts)\n",
    "        total_fp = sum(counts[cls][1] for cls in counts)\n",
    "        total_fn = sum(counts[cls][2] for cls in counts)\n",
    "        \n",
    "        if total_tp == 0:\n",
    "            return {'micro_f1': 0.0, 'support': 0}\n",
    "            \n",
    "        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            micro_f1 = 0.0\n",
    "        else:\n",
    "            micro_f1 = 2 * (precision * recall) / (precision + recall)\n",
    "            \n",
    "        # Calculate per-class metrics\n",
    "        class_f1s = {'micro_f1': micro_f1}\n",
    "        total_support = 0\n",
    "        \n",
    "        for cls, (tp, fp, fn) in counts.items():\n",
    "            support = tp + fn  # Support is true positives + false negatives\n",
    "            total_support += support\n",
    "            \n",
    "            if tp == 0:\n",
    "                class_f1s[cls] = {'f1': 0.0, 'support': support}\n",
    "                continue\n",
    "                \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            \n",
    "            if precision + recall == 0:\n",
    "                class_f1s[cls] = {'f1': 0.0, 'support': support}\n",
    "            else:\n",
    "                f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                class_f1s[cls] = {'f1': f1, 'support': support}\n",
    "        \n",
    "        class_f1s['support'] = total_support\n",
    "        return class_f1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mismatch derivation BERT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ast\n",
    "def load_json_file(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Get list of all modifications from filenames\n",
    "modifications = set()\n",
    "models = ['GPT2', 'BERT', 'T5']\n",
    "for model in models:\n",
    "    model_dir = f'parsed_NER_{model}'\n",
    "    for filename in os.listdir(model_dir):\n",
    "        if filename.startswith(f'{model.lower()}_') and filename.endswith('_ori.json'):\n",
    "            mod = filename.replace(f'{model.lower()}_', '').replace('_ori.json', '')\n",
    "            modifications.add(mod)\n",
    "modifications = list(modifications)\n",
    "\n",
    "# Load original and modified files for each model\n",
    "ori_files = {model: {} for model in models}\n",
    "modif_files = {model: {} for model in models}\n",
    "\n",
    "for model in models:\n",
    "    model_dir = f'parsed_NER_{model}'\n",
    "    for modification in modifications:\n",
    "        # Load original files\n",
    "        ori_filepath = f'{model_dir}/{model.lower()}_{modification}_ori.json'\n",
    "        ori_files[model][modification] = load_json_file(ori_filepath)\n",
    "        \n",
    "        # Load modified files\n",
    "        modif_filepath = f'{model_dir}/{model.lower()}_{modification}_modif.json'\n",
    "        modif_files[model][modification] = load_json_file(modif_filepath)\n",
    "\n",
    "\n",
    "# Calculate and store F1 scores for each model\n",
    "results = []\n",
    "negation_type_results = []  # For storing negation type breakdown\n",
    "\n",
    "for model in models:\n",
    "    for modification in modifications:\n",
    "        compare_file = Path(f'../../data/modified_data/ner/{modification}_100.json')\n",
    "        if not compare_file.exists():\n",
    "            continue\n",
    "        compare_df = json.load(open(compare_file))\n",
    "        if len(compare_df) != len(ori_files[model][modification]):\n",
    "            print('mismatch',modification, model)\n",
    "        ori_f1_scores, ori_counts = get_f1_scores_and_counts(ori_files[model][modification])\n",
    "        modif_f1_scores, modif_counts = get_f1_scores_and_counts(modif_files[model][modification])\n",
    "        # Calculate mean F1 scores\n",
    "        ori_mean_f1 = np.mean(ori_f1_scores)\n",
    "        modif_mean_f1 = np.mean(modif_f1_scores)\n",
    "        # Calculate micro F1 scores\n",
    "        ori_micro_f1 = calculate_micro_f1(ori_counts)\n",
    "        modif_micro_f1 = calculate_micro_f1(modif_counts)\n",
    "        # Calculate percentage change\n",
    "        mean_f1_pct_change = ((modif_mean_f1 - ori_mean_f1) / ori_mean_f1) * 100\n",
    "        micro_f1_pct_change = ((modif_micro_f1['micro_f1'] - ori_micro_f1['micro_f1']) / ori_micro_f1['micro_f1']) * 100\n",
    "        \n",
    "        # Perform paired t-test on per-example F1 scores\n",
    "        _, p_wilcoxon = stats.wilcoxon(ori_f1_scores, modif_f1_scores)\n",
    "        _, p_mannwhitney = stats.mannwhitneyu(ori_f1_scores, modif_f1_scores)\n",
    "        p_value = min(p_wilcoxon, p_mannwhitney)\n",
    "        \n",
    "        # Determine significance level\n",
    "        if p_value < 0.01:\n",
    "            significance = \"**\"\n",
    "        elif p_value < 0.05:\n",
    "            significance = \"*\"\n",
    "        elif p_value < 0.1:\n",
    "            significance = \".\"\n",
    "        else:\n",
    "            significance = \"ns\"\n",
    "        \n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'modification': modification,\n",
    "            'original_mean_f1': ori_mean_f1,\n",
    "            'modified_mean_f1': modif_mean_f1,\n",
    "            'mean_f1_pct_change': mean_f1_pct_change,\n",
    "            'original_micro_f1': ori_micro_f1,\n",
    "            'modified_micro_f1': modif_micro_f1,\n",
    "            'micro_f1_pct_change': micro_f1_pct_change,\n",
    "            'p_value': p_value,\n",
    "            'significance': significance\n",
    "        })\n",
    "        \n",
    "        # Additional analysis for negation types\n",
    "        if modification == 'negation':\n",
    "            # Load negation type information\n",
    "            negation_file = Path(f'../../data/modified_data/ner/negation_100.json')\n",
    "            negation_data = json.load(open(negation_file))\n",
    "            \n",
    "            # Group examples by negation type\n",
    "            type_results = {}\n",
    "            for idx, (ori_f1, mod_f1) in enumerate(zip(ori_f1_scores, modif_f1_scores)):\n",
    "                neg_type = negation_data[idx].get('subtype', 'unknown')\n",
    "                if neg_type not in type_results:\n",
    "                    type_results[neg_type] = {'ori_f1s': [], 'mod_f1s': []}\n",
    "                type_results[neg_type]['ori_f1s'].append(ori_f1)\n",
    "                type_results[neg_type]['mod_f1s'].append(mod_f1)\n",
    "            \n",
    "            # Calculate metrics for each negation type\n",
    "            for neg_type, scores in type_results.items():\n",
    "                ori_mean = np.mean(scores['ori_f1s'])\n",
    "                mod_mean = np.mean(scores['mod_f1s'])\n",
    "                pct_change = ((mod_mean - ori_mean) / ori_mean) * 100 if ori_mean > 0 else 0\n",
    "                \n",
    "                # Statistical tests\n",
    "                if len(scores['ori_f1s']) > 1:  # Only if we have enough samples\n",
    "                    _, p_wilcoxon = stats.wilcoxon(scores['ori_f1s'], scores['mod_f1s'])\n",
    "                    _, p_mannwhitney = stats.mannwhitneyu(scores['ori_f1s'], scores['mod_f1s'])\n",
    "                    p_value = min(p_wilcoxon, p_mannwhitney)\n",
    "                else:\n",
    "                    p_value = 1.0\n",
    "                \n",
    "                # Determine significance level for negation types\n",
    "                if p_value < 0.01:\n",
    "                    significance = \"**\"\n",
    "                elif p_value < 0.05:\n",
    "                    significance = \"*\"\n",
    "                elif p_value < 0.1:\n",
    "                    significance = \".\"\n",
    "                else:\n",
    "                    significance = \"ns\"\n",
    "                \n",
    "                negation_type_results.append({\n",
    "                    'model': model,\n",
    "                    'negation_type': neg_type,\n",
    "                    'original_mean_f1': ori_mean,\n",
    "                    'modified_mean_f1': mod_mean,\n",
    "                    'mean_f1_pct_change': pct_change,\n",
    "                    'sample_size': len(scores['ori_f1s']),\n",
    "                    'p_value': p_value,\n",
    "                    'significance': significance\n",
    "                })\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('ner_modification_results_plm.csv', index=False)\n",
    "\n",
    "# Save negation type results\n",
    "if negation_type_results:\n",
    "    df_negation = pd.DataFrame(negation_type_results)\n",
    "    df_negation.to_csv('ner_negation_type_results_plm.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_f1_and_counts_list(gold, pred):\n",
    "    # Convert string labels to lists if needed\n",
    "    if isinstance(gold, str):\n",
    "        gold = ast.literal_eval(gold)\n",
    "    if isinstance(pred, str):\n",
    "        pred = ast.literal_eval(pred)\n",
    "\n",
    "    # Standardize format to list of dicts with 'text' and 'value' keys\n",
    "    def standardize_format(data):\n",
    "        if isinstance(data, dict):\n",
    "            return [{'text': k, 'value': v} for k, v in data.items()]\n",
    "        elif isinstance(data, list) and len(data) > 0:\n",
    "            if isinstance(data[0], dict):\n",
    "                standardized = []\n",
    "                for item in data:\n",
    "                    if 'text' not in item:\n",
    "                        for text, value in item.items():\n",
    "                            standardized.append({'text': text, 'value': value})\n",
    "                    else:\n",
    "                        standardized.append(item)\n",
    "                return standardized\n",
    "        return data\n",
    "\n",
    "    gold = standardize_format(gold)\n",
    "    pred = standardize_format(pred)\n",
    "\n",
    "    # Calculate metrics by comparing each prediction against gold\n",
    "    tp = 0\n",
    "    gold_matched = [False] * len(gold)\n",
    "    pred_matched = [False] * len(pred)\n",
    "\n",
    "    # First pass - find exact matches\n",
    "    for i, p in enumerate(pred):\n",
    "        for j, g in enumerate(gold):\n",
    "            if not gold_matched[j] and not pred_matched[i]:\n",
    "                if p['text'] == g['text'] and p['value'] == g['value']:\n",
    "                    tp += 1\n",
    "                    gold_matched[j] = True\n",
    "                    pred_matched[i] = True\n",
    "\n",
    "    # Calculate false positives and false negatives\n",
    "    fp = len(pred) - tp  # Predictions that didn't match any gold\n",
    "    fn = len(gold) - tp  # Gold entities that weren't matched\n",
    "\n",
    "    # Calculate F1 score for this example\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return f1, (tp, fp, fn)\n",
    "\n",
    "def calculate_micro_f1_list(counts):\n",
    "    # Sum up all true positives, false positives, and false negatives\n",
    "    tp = sum(count[0] for count in counts)\n",
    "    fp = sum(count[1] for count in counts)\n",
    "    fn = sum(count[2] for count in counts)\n",
    "\n",
    "    # Calculate micro-averaged precision and recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    # Calculate micro F1\n",
    "    micro_f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return micro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing results from rahmad directory:\n",
      "--------------------------------------------------\n",
      "llama-0shot-concept_replacement_100.csv\n",
      "concept_replacement_100.csv\n",
      "\n",
      "Results from llama-0shot-concept_replacement_100.csv:\n",
      "==================================================\n",
      "claude-0shot-capitalization_100_new.csv\n",
      "capitalization\n",
      "\n",
      "Results from claude-0shot-capitalization_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 9730.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 20643.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - CAPITALIZATION Modification:\n",
      "Original Mean F1: 0.607\n",
      "Modified Mean F1: 0.564\n",
      "Mean F1 Percentage Change: -7.0%\n",
      "P-value: 0.0792\n",
      "Significance: .\n",
      "gpt4o-0shot-sentiment_100_new.csv\n",
      "sentiment\n",
      "\n",
      "Results from gpt4o-0shot-sentiment_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [00:00, 23041.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [00:00, 20022.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - SENTIMENT Modification:\n",
      "Original Mean F1: 0.570\n",
      "Modified Mean F1: 0.571\n",
      "Mean F1 Percentage Change: 0.1%\n",
      "P-value: 0.9547\n",
      "Significance: ns\n",
      "llama-0shot-negation_100.csv\n",
      "negation_100.csv\n",
      "\n",
      "Results from llama-0shot-negation_100.csv:\n",
      "==================================================\n",
      "llama-0shot-temporal_bias_100.csv\n",
      "temporal_bias_100.csv\n",
      "\n",
      "Results from llama-0shot-temporal_bias_100.csv:\n",
      "==================================================\n",
      "llama-0shot-discourse_100_compare.csv\n",
      "discourse_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-discourse_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-coordinating_conjunction_100.csv\n",
      "coordinating_conjunction_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-coordinating_conjunction_100.csv:\n",
      "==================================================\n",
      "llama-0shot-sentiment_100_compare.csv\n",
      "sentiment_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-sentiment_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-grammatical_role_100.csv\n",
      "grammatical_role_100.csv\n",
      "\n",
      "Results from llama-0shot-grammatical_role_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-grammatical_role_100.csv\n",
      "grammatical_role_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-grammatical_role_100.csv:\n",
      "==================================================\n",
      "claude-0shot-compound_word_100_compare.csv\n",
      "compound_word_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-compound_word_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-discourse_100.csv\n",
      "discourse_100.csv\n",
      "\n",
      "Results from llama-0shot-discourse_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-discourse_100_compare.csv\n",
      "discourse_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-discourse_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-sentiment_100_compare.csv\n",
      "sentiment_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-sentiment_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-derivation_100_compare.csv\n",
      "derivation_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-derivation_100_compare.csv:\n",
      "==================================================\n",
      "claude-DP.csv\n",
      "claude-0shot-casual_100.csv\n",
      "casual_100.csv\n",
      "\n",
      "Results from claude-0shot-casual_100.csv:\n",
      "==================================================\n",
      "llama-0shot-coordinating_conjunction_100.csv\n",
      "coordinating_conjunction_100.csv\n",
      "\n",
      "Results from llama-0shot-coordinating_conjunction_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-ner.csv\n",
      "claude-0shot-punctuation_100.csv\n",
      "punctuation_100.csv\n",
      "\n",
      "Results from claude-0shot-punctuation_100.csv:\n",
      "==================================================\n",
      "llama-0shot-length_bias_100_compare.csv\n",
      "length_bias_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-length_bias_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-sentiment_100.csv\n",
      "sentiment_100.csv\n",
      "\n",
      "Results from claude-0shot-sentiment_100.csv:\n",
      "==================================================\n",
      "claude-0shot-geographical_bias_100.csv\n",
      "geographical_bias_100.csv\n",
      "\n",
      "Results from claude-0shot-geographical_bias_100.csv:\n",
      "==================================================\n",
      "claude-0shot-length_bias_100.csv\n",
      "length_bias_100.csv\n",
      "\n",
      "Results from claude-0shot-length_bias_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-concept_replacement_100_compare.csv\n",
      "concept_replacement_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-concept_replacement_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-grammatical_role_100.csv\n",
      "grammatical_role_100.csv\n",
      "\n",
      "Results from claude-0shot-grammatical_role_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-punctuation_100_new.csv\n",
      "punctuation\n",
      "\n",
      "Results from gpt4o-0shot-punctuation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 21644.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 21881.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - PUNCTUATION Modification:\n",
      "Original Mean F1: 0.506\n",
      "Modified Mean F1: 0.471\n",
      "Mean F1 Percentage Change: -6.9%\n",
      "P-value: 0.0978\n",
      "Significance: .\n",
      "claude-0shot-compound_word_100.csv\n",
      "compound_word_100.csv\n",
      "\n",
      "Results from claude-0shot-compound_word_100.csv:\n",
      "==================================================\n",
      "llama-0shot-geographical_bias_100.csv\n",
      "geographical_bias_100.csv\n",
      "\n",
      "Results from llama-0shot-geographical_bias_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-coordinating_conjunction_100_compare.csv\n",
      "coordinating_conjunction_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-coordinating_conjunction_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-geographical_bias_100.csv\n",
      "geographical_bias_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-geographical_bias_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-capitalization_100.csv\n",
      "capitalization_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-capitalization_100.csv:\n",
      "==================================================\n",
      "claude-0shot-capitalization_100.csv\n",
      "capitalization_100.csv\n",
      "\n",
      "Results from claude-0shot-capitalization_100.csv:\n",
      "==================================================\n",
      "llama-0shot-punctuation_100.csv\n",
      "punctuation_100.csv\n",
      "\n",
      "Results from llama-0shot-punctuation_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-derivation_100_new.csv\n",
      "derivation\n",
      "\n",
      "Results from gpt4o-0shot-derivation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:00, 21909.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:00, 21644.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - DERIVATION Modification:\n",
      "Original Mean F1: 0.555\n",
      "Modified Mean F1: 0.555\n",
      "Mean F1 Percentage Change: -0.0%\n",
      "P-value: 0.9430\n",
      "Significance: ns\n",
      "claude-0shot-derivation_100_new.csv\n",
      "derivation\n",
      "\n",
      "Results from claude-0shot-derivation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:00, 24117.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:00, 22585.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - DERIVATION Modification:\n",
      "Original Mean F1: 0.563\n",
      "Modified Mean F1: 0.544\n",
      "Mean F1 Percentage Change: -3.5%\n",
      "P-value: 0.0679\n",
      "Significance: .\n",
      "claude-0shot-discourse_100_compare.csv\n",
      "discourse_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-discourse_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-compound_word_100_compare.csv\n",
      "compound_word_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-compound_word_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-sentiment_100_compare.csv\n",
      "sentiment_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-sentiment_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-casual_100_compare.csv\n",
      "casual_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-casual_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-punctuation_100_new.csv\n",
      "punctuation\n",
      "\n",
      "Results from claude-0shot-punctuation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 26924.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 26311.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - PUNCTUATION Modification:\n",
      "Original Mean F1: 0.454\n",
      "Modified Mean F1: 0.373\n",
      "Mean F1 Percentage Change: -17.8%\n",
      "P-value: 0.0007\n",
      "Significance: **\n",
      "llama-0shot-length_bias_100.csv\n",
      "length_bias_100.csv\n",
      "\n",
      "Results from llama-0shot-length_bias_100.csv:\n",
      "==================================================\n",
      "claude-0shot-casual_100_compare.csv\n",
      "casual_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-casual_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-discourse_100_new.csv\n",
      "discourse\n",
      "\n",
      "Results from gpt4o-0shot-discourse_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:00, 20631.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:00, 21414.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - DISCOURSE Modification:\n",
      "Original Mean F1: 0.549\n",
      "Modified Mean F1: 0.550\n",
      "Mean F1 Percentage Change: 0.2%\n",
      "P-value: 0.8125\n",
      "Significance: ns\n",
      "llama-0shot-capitalization_100.csv\n",
      "capitalization_100.csv\n",
      "\n",
      "Results from llama-0shot-capitalization_100.csv:\n",
      "==================================================\n",
      "claude-0shot-temporal_bias_100_new.csv\n",
      "temporal_bias\n",
      "\n",
      "Results from claude-0shot-temporal_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:00, 24688.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:00, 23748.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - TEMPORAL_BIAS Modification:\n",
      "Original Mean F1: 0.533\n",
      "Modified Mean F1: 0.490\n",
      "Mean F1 Percentage Change: -8.0%\n",
      "P-value: 0.0180\n",
      "Significance: *\n",
      "claude-0shot-negation_100_new.csv\n",
      "negation\n",
      "\n",
      "Results from claude-0shot-negation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 22605.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 21544.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - NEGATION Modification:\n",
      "Original Mean F1: 0.465\n",
      "Modified Mean F1: 0.464\n",
      "Mean F1 Percentage Change: -0.2%\n",
      "P-value: 0.6768\n",
      "Significance: ns\n",
      "gpt4o-0shot-dialectal_100_new.csv\n",
      "dialectal\n",
      "\n",
      "Results from gpt4o-0shot-dialectal_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:00, 24182.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:00, 23295.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - DIALECTAL Modification:\n",
      "Original Mean F1: 0.516\n",
      "Modified Mean F1: 0.549\n",
      "Mean F1 Percentage Change: 6.4%\n",
      "P-value: 0.2043\n",
      "Significance: ns\n",
      "llama-0shot-coordinating_conjunction_100_new.csv\n",
      "coordinating_conjunction\n",
      "\n",
      "Results from llama-0shot-coordinating_conjunction_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 16911.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 16958.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - COORDINATING_CONJUNCTION Modification:\n",
      "Original Mean F1: 0.734\n",
      "Modified Mean F1: 0.736\n",
      "Mean F1 Percentage Change: 0.3%\n",
      "P-value: 0.6528\n",
      "Significance: ns\n",
      "llama-0shot-casual_100_compare.csv\n",
      "casual_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-casual_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-active_to_passive_100.csv\n",
      "active_to_passive_100.csv\n",
      "\n",
      "Results from claude-0shot-active_to_passive_100.csv:\n",
      "==================================================\n",
      "llama-0shot-derivation_100_new.csv\n",
      "derivation\n",
      "\n",
      "Results from llama-0shot-derivation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:00, 21394.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:00, 19062.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - DERIVATION Modification:\n",
      "Original Mean F1: 0.601\n",
      "Modified Mean F1: 0.589\n",
      "Mean F1 Percentage Change: -2.1%\n",
      "P-value: 0.1441\n",
      "Significance: ns\n",
      "gpt4o-0shot-discourse_100.csv\n",
      "discourse_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-discourse_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-typo_bias_100_new.csv\n",
      "typo_bias\n",
      "\n",
      "Results from gpt4o-0shot-typo_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 21403.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 21464.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - TYPO_BIAS Modification:\n",
      "Original Mean F1: 0.497\n",
      "Modified Mean F1: 0.503\n",
      "Mean F1 Percentage Change: 1.1%\n",
      "P-value: 0.7088\n",
      "Significance: ns\n",
      "claude-0shot-temporal_bias_100.csv\n",
      "temporal_bias_100.csv\n",
      "\n",
      "Results from claude-0shot-temporal_bias_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-casual_100.csv\n",
      "casual_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-casual_100.csv:\n",
      "==================================================\n",
      "claude-0shot-typo_bias_100.csv\n",
      "typo_bias_100.csv\n",
      "\n",
      "Results from claude-0shot-typo_bias_100.csv:\n",
      "==================================================\n",
      "claude-0shot-coordinating_conjunction_100.csv\n",
      "coordinating_conjunction_100.csv\n",
      "\n",
      "Results from claude-0shot-coordinating_conjunction_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-compound_word_100_compare.csv\n",
      "compound_word_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-compound_word_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-punctuation_100_new.csv\n",
      "punctuation\n",
      "\n",
      "Results from llama-0shot-punctuation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 22814.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 23261.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - PUNCTUATION Modification:\n",
      "Original Mean F1: 0.540\n",
      "Modified Mean F1: 0.474\n",
      "Mean F1 Percentage Change: -12.2%\n",
      "P-value: 0.0030\n",
      "Significance: **\n",
      "claude-0shot-active_to_passive_100_compare.csv\n",
      "active_to_passive_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-active_to_passive_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-dialectal_100.csv\n",
      "dialectal_100.csv\n",
      "\n",
      "Results from claude-0shot-dialectal_100.csv:\n",
      "==================================================\n",
      "claude-0shot-capitalization_100_compare.csv\n",
      "capitalization_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-capitalization_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-concept_replacement_100_new.csv\n",
      "concept_replacement\n",
      "\n",
      "Results from claude-0shot-concept_replacement_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 9536.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 22510.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - CONCEPT_REPLACEMENT Modification:\n",
      "Original Mean F1: 0.492\n",
      "Modified Mean F1: 0.491\n",
      "Mean F1 Percentage Change: -0.2%\n",
      "P-value: 0.9161\n",
      "Significance: ns\n",
      "gpt4o-0shot-derivation_100_compare.csv\n",
      "derivation_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-derivation_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-length_bias_100_compare.csv\n",
      "length_bias_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-length_bias_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-active_to_passive_100.csv\n",
      "active_to_passive_100.csv\n",
      "\n",
      "Results from llama-0shot-active_to_passive_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-temporal_bias_100_new.csv\n",
      "temporal_bias\n",
      "\n",
      "Results from gpt4o-0shot-temporal_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:00, 22134.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:00, 22442.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - TEMPORAL_BIAS Modification:\n",
      "Original Mean F1: 0.570\n",
      "Modified Mean F1: 0.532\n",
      "Mean F1 Percentage Change: -6.7%\n",
      "P-value: 0.0275\n",
      "Significance: *\n",
      "gpt4o-0shot-active_to_passive_100_compare.csv\n",
      "active_to_passive_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-active_to_passive_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-concept_replacement_100.csv\n",
      "concept_replacement_100.csv\n",
      "\n",
      "Results from claude-0shot-concept_replacement_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-capitalization_100_compare.csv\n",
      "capitalization_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-capitalization_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-compound_word_100.csv\n",
      "compound_word_100.csv\n",
      "\n",
      "Results from llama-0shot-compound_word_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-active_to_passive_100.csv\n",
      "active_to_passive_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-active_to_passive_100.csv:\n",
      "==================================================\n",
      "llama-0shot-temporal_bias_100_new.csv\n",
      "temporal_bias\n",
      "\n",
      "Results from llama-0shot-temporal_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:00, 20053.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:00, 21852.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - TEMPORAL_BIAS Modification:\n",
      "Original Mean F1: 0.571\n",
      "Modified Mean F1: 0.558\n",
      "Mean F1 Percentage Change: -2.3%\n",
      "P-value: 0.1730\n",
      "Significance: ns\n",
      "llama-0shot-geographical_bias_100_compare.csv\n",
      "geographical_bias_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-geographical_bias_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-punctuation_100_compare.csv\n",
      "punctuation_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-punctuation_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-grammatical_role_100_new.csv\n",
      "grammatical_role\n",
      "\n",
      "Results from claude-0shot-grammatical_role_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 17709.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 20828.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - GRAMMATICAL_ROLE Modification:\n",
      "Original Mean F1: 0.603\n",
      "Modified Mean F1: 0.580\n",
      "Mean F1 Percentage Change: -3.9%\n",
      "P-value: 0.0853\n",
      "Significance: .\n",
      "llama-0shot-ner.csv\n",
      "llama-0shot-capitalization_100_compare.csv\n",
      "capitalization_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-capitalization_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-derivation_100.csv\n",
      "derivation_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-derivation_100.csv:\n",
      "==================================================\n",
      "llama-0shot-dialectal_100.csv\n",
      "dialectal_100.csv\n",
      "\n",
      "Results from llama-0shot-dialectal_100.csv:\n",
      "==================================================\n",
      "claude-0shot-length_bias_100_new.csv\n",
      "length_bias\n",
      "\n",
      "Results from claude-0shot-length_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 25718.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 24724.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - LENGTH_BIAS Modification:\n",
      "Original Mean F1: 0.483\n",
      "Modified Mean F1: 0.434\n",
      "Mean F1 Percentage Change: -10.2%\n",
      "P-value: 0.2576\n",
      "Significance: ns\n",
      "llama-0shot-sentiment_100_new.csv\n",
      "sentiment\n",
      "\n",
      "Results from llama-0shot-sentiment_100_new.csv:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [00:00, 22376.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [00:00, 23495.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - SENTIMENT Modification:\n",
      "Original Mean F1: 0.599\n",
      "Modified Mean F1: 0.582\n",
      "Mean F1 Percentage Change: -2.9%\n",
      "P-value: 0.1921\n",
      "Significance: ns\n",
      "llama-0shot-concept_replacement_100_new.csv\n",
      "concept_replacement\n",
      "\n",
      "Results from llama-0shot-concept_replacement_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 19097.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 19850.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - CONCEPT_REPLACEMENT Modification:\n",
      "Original Mean F1: 0.561\n",
      "Modified Mean F1: 0.532\n",
      "Mean F1 Percentage Change: -5.2%\n",
      "P-value: 0.1358\n",
      "Significance: ns\n",
      "llama-0shot-casual_100_new.csv\n",
      "casual\n",
      "\n",
      "Results from llama-0shot-casual_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 21551.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 22770.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - CASUAL Modification:\n",
      "Original Mean F1: 0.542\n",
      "Modified Mean F1: 0.556\n",
      "Mean F1 Percentage Change: 2.5%\n",
      "P-value: 0.5196\n",
      "Significance: ns\n",
      "claude-0shot-typo_bias_100_new.csv\n",
      "typo_bias\n",
      "\n",
      "Results from claude-0shot-typo_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 25913.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 24889.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - TYPO_BIAS Modification:\n",
      "Original Mean F1: 0.467\n",
      "Modified Mean F1: 0.457\n",
      "Mean F1 Percentage Change: -2.2%\n",
      "P-value: 0.2367\n",
      "Significance: ns\n",
      "llama-0shot-concept_replacement_100_compare.csv\n",
      "concept_replacement_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-concept_replacement_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-typo_bias_100.csv\n",
      "typo_bias_100.csv\n",
      "\n",
      "Results from llama-0shot-typo_bias_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-temporal_bias_100.csv\n",
      "temporal_bias_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-temporal_bias_100.csv:\n",
      "==================================================\n",
      "llama-0shot-punctuation_100_compare.csv\n",
      "punctuation_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-punctuation_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-dialectal_100_new.csv\n",
      "dialectal\n",
      "\n",
      "Results from claude-0shot-dialectal_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:00, 27116.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:00, 27219.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - DIALECTAL Modification:\n",
      "Original Mean F1: 0.494\n",
      "Modified Mean F1: 0.479\n",
      "Mean F1 Percentage Change: -3.0%\n",
      "P-value: 0.5723\n",
      "Significance: ns\n",
      "claude-0shot-temporal_bias_100_compare.csv\n",
      "temporal_bias_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-temporal_bias_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-dialectal_100_compare.csv\n",
      "dialectal_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-dialectal_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-discourse_100_new.csv\n",
      "discourse\n",
      "\n",
      "Results from claude-0shot-discourse_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:00, 24383.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:00, 23172.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - DISCOURSE Modification:\n",
      "Original Mean F1: 0.532\n",
      "Modified Mean F1: 0.522\n",
      "Mean F1 Percentage Change: -1.9%\n",
      "P-value: 0.2785\n",
      "Significance: ns\n",
      "claude-0shot-compound_word_100_new.csv\n",
      "compound_word\n",
      "\n",
      "Results from claude-0shot-compound_word_100_new.csv:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 25927.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 24265.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - COMPOUND_WORD Modification:\n",
      "Original Mean F1: 0.515\n",
      "Modified Mean F1: 0.497\n",
      "Mean F1 Percentage Change: -3.4%\n",
      "P-value: 0.1088\n",
      "Significance: ns\n",
      "gpt4o-0shot-length_bias_100_new.csv\n",
      "length_bias\n",
      "\n",
      "Results from gpt4o-0shot-length_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 19849.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 20749.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - LENGTH_BIAS Modification:\n",
      "Original Mean F1: 0.504\n",
      "Modified Mean F1: 0.553\n",
      "Mean F1 Percentage Change: 9.8%\n",
      "P-value: 0.0750\n",
      "Significance: .\n",
      "llama-0shot-dialectal_100_compare.csv\n",
      "dialectal_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-dialectal_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-grammatical_role_100_new.csv\n",
      "grammatical_role\n",
      "\n",
      "Results from gpt4o-0shot-grammatical_role_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 14417.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 17577.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - GRAMMATICAL_ROLE Modification:\n",
      "Original Mean F1: 0.597\n",
      "Modified Mean F1: 0.617\n",
      "Mean F1 Percentage Change: 3.4%\n",
      "P-value: 0.1935\n",
      "Significance: ns\n",
      "claude-0shot-active_to_passive_100_new.csv\n",
      "active_to_passive\n",
      "\n",
      "Results from claude-0shot-active_to_passive_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 19872.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 19167.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - ACTIVE_TO_PASSIVE Modification:\n",
      "Original Mean F1: 0.511\n",
      "Modified Mean F1: 0.529\n",
      "Mean F1 Percentage Change: 3.6%\n",
      "P-value: 0.3326\n",
      "Significance: ns\n",
      "llama-0shot-geographical_bias_100_new.csv\n",
      "geographical_bias\n",
      "\n",
      "Results from llama-0shot-geographical_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 12993.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 16335.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - GEOGRAPHICAL_BIAS Modification:\n",
      "Original Mean F1: 0.626\n",
      "Modified Mean F1: 0.633\n",
      "Mean F1 Percentage Change: 1.2%\n",
      "P-value: 0.8502\n",
      "Significance: ns\n",
      "llama-0shot-grammatical_role_100_new.csv\n",
      "grammatical_role\n",
      "\n",
      "Results from llama-0shot-grammatical_role_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 11497.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 13460.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - GRAMMATICAL_ROLE Modification:\n",
      "Original Mean F1: 0.641\n",
      "Modified Mean F1: 0.637\n",
      "Mean F1 Percentage Change: -0.6%\n",
      "P-value: 0.7554\n",
      "Significance: ns\n",
      "llama-0shot-temporal_bias_100_compare.csv\n",
      "temporal_bias_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-temporal_bias_100_compare.csv:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-0shot-coordinating_conjunction_100_compare.csv\n",
      "coordinating_conjunction_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-coordinating_conjunction_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-grammatical_role_100_compare.csv\n",
      "grammatical_role_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-grammatical_role_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-dialectal_100_compare.csv\n",
      "dialectal_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-dialectal_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-compound_word_100_new.csv\n",
      "compound_word\n",
      "\n",
      "Results from gpt4o-0shot-compound_word_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 20364.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 8509.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - COMPOUND_WORD Modification:\n",
      "Original Mean F1: 0.545\n",
      "Modified Mean F1: 0.529\n",
      "Mean F1 Percentage Change: -2.9%\n",
      "P-value: 0.5281\n",
      "Significance: ns\n",
      "gpt4o-DP.csv\n",
      "claude-0shot-geographical_bias_100_new.csv\n",
      "geographical_bias\n",
      "\n",
      "Results from claude-0shot-geographical_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 16991.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 19026.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - GEOGRAPHICAL_BIAS Modification:\n",
      "Original Mean F1: 0.660\n",
      "Modified Mean F1: 0.615\n",
      "Mean F1 Percentage Change: -6.8%\n",
      "P-value: 0.2890\n",
      "Significance: ns\n",
      "llama-0shot-active_to_passive_100_new.csv\n",
      "active_to_passive\n",
      "\n",
      "Results from llama-0shot-active_to_passive_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 14181.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 20305.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - ACTIVE_TO_PASSIVE Modification:\n",
      "Original Mean F1: 0.548\n",
      "Modified Mean F1: 0.568\n",
      "Mean F1 Percentage Change: 3.8%\n",
      "P-value: 0.2198\n",
      "Significance: ns\n",
      "llama-0shot-compound_word_100_new.csv\n",
      "compound_word\n",
      "\n",
      "Results from llama-0shot-compound_word_100_new.csv:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 21257.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 18711.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - COMPOUND_WORD Modification:\n",
      "Original Mean F1: 0.581\n",
      "Modified Mean F1: 0.571\n",
      "Mean F1 Percentage Change: -1.7%\n",
      "P-value: 0.1797\n",
      "Significance: ns\n",
      "claude-0shot-coordinating_conjunction_100_compare.csv\n",
      "coordinating_conjunction_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-coordinating_conjunction_100_compare.csv:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4o-0shot-capitalization_100_new.csv\n",
      "capitalization\n",
      "\n",
      "Results from gpt4o-0shot-capitalization_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 16477.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 15622.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - CAPITALIZATION Modification:\n",
      "Original Mean F1: 0.556\n",
      "Modified Mean F1: 0.551\n",
      "Mean F1 Percentage Change: -1.1%\n",
      "P-value: 0.7260\n",
      "Significance: ns\n",
      "gpt4o-0shot-sentiment_100.csv\n",
      "sentiment_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-sentiment_100.csv:\n",
      "==================================================\n",
      "claude-0shot-derivation_100.csv\n",
      "derivation_100.csv\n",
      "\n",
      "Results from claude-0shot-derivation_100.csv:\n",
      "==================================================\n",
      "llama-0shot-negation_100_new.csv\n",
      "negation\n",
      "\n",
      "Results from llama-0shot-negation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 17252.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 17776.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - NEGATION Modification:\n",
      "Original Mean F1: 0.500\n",
      "Modified Mean F1: 0.492\n",
      "Mean F1 Percentage Change: -1.6%\n",
      "P-value: 0.6872\n",
      "Significance: ns\n",
      "gpt4o-0shot-punctuation_100.csv\n",
      "punctuation_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-punctuation_100.csv:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4o-0shot-length_bias_100.csv\n",
      "length_bias_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-length_bias_100.csv:\n",
      "==================================================\n",
      "llama-0shot-active_to_passive_100_compare.csv\n",
      "active_to_passive_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-active_to_passive_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-typo_bias_100_compare.csv\n",
      "typo_bias_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-typo_bias_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-dialectal_100.csv\n",
      "dialectal_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-dialectal_100.csv:\n",
      "==================================================\n",
      "llama-0shot-casual_100.csv\n",
      "casual_100.csv\n",
      "\n",
      "Results from llama-0shot-casual_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-geographical_bias_100_new.csv\n",
      "geographical_bias\n",
      "\n",
      "Results from gpt4o-0shot-geographical_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 2872.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 1794.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - GEOGRAPHICAL_BIAS Modification:\n",
      "Original Mean F1: 0.643\n",
      "Modified Mean F1: 0.692\n",
      "Mean F1 Percentage Change: 7.6%\n",
      "P-value: 0.0651\n",
      "Significance: .\n",
      "claude-0shot-negation_100_compare.csv\n",
      "negation_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-negation_100_compare.csv:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4o-0shot-grammatical_role_100_compare.csv\n",
      "grammatical_role_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-grammatical_role_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-typo_bias_100_compare.csv\n",
      "typo_bias_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-typo_bias_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-typo_bias_100.csv\n",
      "typo_bias_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-typo_bias_100.csv:\n",
      "==================================================\n",
      "llama-0shot-discourse_100_new.csv\n",
      "discourse\n",
      "\n",
      "Results from llama-0shot-discourse_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:00, 1137.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:00, 1694.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - DISCOURSE Modification:\n",
      "Original Mean F1: 0.586\n",
      "Modified Mean F1: 0.574\n",
      "Mean F1 Percentage Change: -2.0%\n",
      "P-value: 0.3946\n",
      "Significance: ns\n",
      "gpt4o-0shot-geographical_bias_100_compare.csv\n",
      "geographical_bias_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-geographical_bias_100_compare.csv:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-0shot-negation_100.csv\n",
      "negation_100.csv\n",
      "\n",
      "Results from claude-0shot-negation_100.csv:\n",
      "==================================================\n",
      "llama-0shot-dialectal_100_new.csv\n",
      "dialectal\n",
      "\n",
      "Results from llama-0shot-dialectal_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:00, 2444.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:00, 1820.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - DIALECTAL Modification:\n",
      "Original Mean F1: 0.574\n",
      "Modified Mean F1: 0.564\n",
      "Mean F1 Percentage Change: -1.9%\n",
      "P-value: 0.6460\n",
      "Significance: ns\n",
      "llama-0shot-capitalization_100_new.csv\n",
      "capitalization\n",
      "\n",
      "Results from llama-0shot-capitalization_100_new.csv:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 1862.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 4364.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - CAPITALIZATION Modification:\n",
      "Original Mean F1: 0.623\n",
      "Modified Mean F1: 0.552\n",
      "Mean F1 Percentage Change: -11.5%\n",
      "P-value: 0.0385\n",
      "Significance: *\n",
      "llama-0shot-derivation_100.csv\n",
      "derivation_100.csv\n",
      "\n",
      "Results from llama-0shot-derivation_100.csv:\n",
      "==================================================\n",
      "claude-0shot-concept_replacement_100_compare.csv\n",
      "concept_replacement_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-concept_replacement_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-typo_bias_100_new.csv\n",
      "typo_bias\n",
      "\n",
      "Results from llama-0shot-typo_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 21431.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 20579.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - TYPO_BIAS Modification:\n",
      "Original Mean F1: 0.532\n",
      "Modified Mean F1: 0.547\n",
      "Mean F1 Percentage Change: 2.9%\n",
      "P-value: 0.2289\n",
      "Significance: ns\n",
      "llama-0shot-grammatical_role_100_compare.csv\n",
      "grammatical_role_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-grammatical_role_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-sentiment_100_new.csv\n",
      "sentiment\n",
      "\n",
      "Results from claude-0shot-sentiment_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [00:00, 24008.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [00:00, 25004.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - SENTIMENT Modification:\n",
      "Original Mean F1: 0.545\n",
      "Modified Mean F1: 0.516\n",
      "Mean F1 Percentage Change: -5.4%\n",
      "P-value: 0.0172\n",
      "Significance: *\n",
      "llama-DP.csv\n",
      "claude-0shot-geographical_bias_100_compare.csv\n",
      "geographical_bias_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-geographical_bias_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-length_bias_100_new.csv\n",
      "length_bias\n",
      "\n",
      "Results from llama-0shot-length_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 9663.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 5028.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - LENGTH_BIAS Modification:\n",
      "Original Mean F1: 0.542\n",
      "Modified Mean F1: 0.556\n",
      "Mean F1 Percentage Change: 2.5%\n",
      "P-value: 0.5196\n",
      "Significance: ns\n",
      "claude-0shot-ner.csv\n",
      "gpt4o-0shot-casual_100_new.csv\n",
      "casual\n",
      "\n",
      "Results from gpt4o-0shot-casual_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 3618.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 2095.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - CASUAL Modification:\n",
      "Original Mean F1: 0.504\n",
      "Modified Mean F1: 0.553\n",
      "Mean F1 Percentage Change: 9.8%\n",
      "P-value: 0.0750\n",
      "Significance: .\n",
      "claude-0shot-derivation_100_compare.csv\n",
      "derivation_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-derivation_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-negation_100_compare.csv\n",
      "negation_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-negation_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-discourse_100.csv\n",
      "discourse_100.csv\n",
      "\n",
      "Results from claude-0shot-discourse_100.csv:\n",
      "==================================================\n",
      "claude-0shot-coordinating_conjunction_100_new.csv\n",
      "coordinating_conjunction\n",
      "\n",
      "Results from claude-0shot-coordinating_conjunction_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 5064.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 10197.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - COORDINATING_CONJUNCTION Modification:\n",
      "Original Mean F1: 0.697\n",
      "Modified Mean F1: 0.694\n",
      "Mean F1 Percentage Change: -0.4%\n",
      "P-value: 0.7252\n",
      "Significance: ns\n",
      "gpt4o-0shot-length_bias_100_compare.csv\n",
      "length_bias_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-length_bias_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-coordinating_conjunction_100_new.csv\n",
      "coordinating_conjunction\n",
      "\n",
      "Results from gpt4o-0shot-coordinating_conjunction_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 12222.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 14503.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - COORDINATING_CONJUNCTION Modification:\n",
      "Original Mean F1: 0.643\n",
      "Modified Mean F1: 0.690\n",
      "Mean F1 Percentage Change: 7.3%\n",
      "P-value: 0.0411\n",
      "Significance: *\n",
      "claude-0shot-casual_100_new.csv\n",
      "casual\n",
      "\n",
      "Results from claude-0shot-casual_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 23820.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 23974.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - CASUAL Modification:\n",
      "Original Mean F1: 0.483\n",
      "Modified Mean F1: 0.434\n",
      "Mean F1 Percentage Change: -10.2%\n",
      "P-value: 0.2576\n",
      "Significance: ns\n",
      "gpt4o-0shot-negation_100.csv\n",
      "negation_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-negation_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-negation_100_compare.csv\n",
      "negation_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-negation_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-typo_bias_100_compare.csv\n",
      "typo_bias_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-typo_bias_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-concept_replacement_100.csv\n",
      "concept_replacement_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-concept_replacement_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-compound_word_100.csv\n",
      "compound_word_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-compound_word_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-negation_100_new.csv\n",
      "negation\n",
      "\n",
      "Results from gpt4o-0shot-negation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 18518.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 19645.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - NEGATION Modification:\n",
      "Original Mean F1: 0.508\n",
      "Modified Mean F1: 0.509\n",
      "Mean F1 Percentage Change: 0.2%\n",
      "P-value: 0.4870\n",
      "Significance: ns\n",
      "llama-0shot-sentiment_100.csv\n",
      "sentiment_100.csv\n",
      "\n",
      "Results from llama-0shot-sentiment_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-active_to_passive_100_new.csv\n",
      "active_to_passive\n",
      "\n",
      "Results from gpt4o-0shot-active_to_passive_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 20646.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 20725.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - ACTIVE_TO_PASSIVE Modification:\n",
      "Original Mean F1: 0.530\n",
      "Modified Mean F1: 0.559\n",
      "Mean F1 Percentage Change: 5.5%\n",
      "P-value: 0.3258\n",
      "Significance: ns\n",
      "gpt4o-0shot-concept_replacement_100_new.csv\n",
      "concept_replacement\n",
      "\n",
      "Results from gpt4o-0shot-concept_replacement_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 14819.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 18669.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - CONCEPT_REPLACEMENT Modification:\n",
      "Original Mean F1: 0.530\n",
      "Modified Mean F1: 0.571\n",
      "Mean F1 Percentage Change: 7.9%\n",
      "P-value: 0.0435\n",
      "Significance: *\n",
      "claude-0shot-punctuation_100_compare.csv\n",
      "punctuation_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-punctuation_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-temporal_bias_100_compare.csv\n",
      "temporal_bias_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-temporal_bias_100_compare.csv:\n",
      "==================================================\n",
      "\n",
      "Results saved to ner_modification_results_llm.csv\n",
      "\n",
      "Negation results saved to ner_negation_type_results_llm.csv\n"
     ]
    }
   ],
   "source": [
    "# Read and analyze modification results from ner directory\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "ner_results_dir = '../../eval/results/ner/'\n",
    "ner_results_files = glob.glob(os.path.join(ner_results_dir, '*.csv'))\n",
    "\n",
    "print(\"\\nAnalyzing results from ner directory:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create list to store results\n",
    "results_data = []\n",
    "negation_results_data = []\n",
    "\n",
    "for results_file in ner_results_files:\n",
    "    # Extract model and modification from filename\n",
    "    filename = os.path.basename(results_file)\n",
    "    print(filename)\n",
    "    if 'DP' in filename or 'ner' in filename:\n",
    "        continue\n",
    "    model = filename.split('-')[0]\n",
    "    # if model == 'gpt4o':\n",
    "    #     if 'new' not in filename:\n",
    "    #         continue\n",
    "    #     modification = filename.split('-')[2].replace('_100_new.csv', '')\n",
    "    # else:\n",
    "    modification = filename.split('-')[2].replace('_100_new.csv', '')\n",
    "    print(modification)\n",
    "    \n",
    "    print(f\"\\nResults from {filename}:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(results_file)\n",
    "    compare_file = Path(f'../../data/modified_data/ner/{modification}_100.json')\n",
    "    if not compare_file.exists():\n",
    "        continue\n",
    "    compare_df = json.load(open(compare_file))\n",
    "    if len(compare_df) != len(df):\n",
    "        print('mismatch',modification, model)\n",
    "    # Calculate macro F1 scores\n",
    "    # Get labels and predictions\n",
    "    # Get original and modified labels/predictions\n",
    "    ori_labels = df['original_label'].values\n",
    "    ori_preds = df['original_pred'].values\n",
    "    mod_labels = df['modified_label'].values\n",
    "    mod_preds = df['modified_pred'].values\n",
    "    # Calculate F1 scores using helper functions\n",
    "    ori_f1_scores = []\n",
    "    modif_f1_scores = []\n",
    "    print('original')\n",
    "    for l, p in tqdm(zip(ori_labels, ori_preds)):\n",
    "        f1, _ = get_example_f1_and_counts_list(l, p)\n",
    "        ori_f1_scores.append(f1)\n",
    "    print('modified')\n",
    "    for l, p in tqdm(zip(mod_labels, mod_preds)):\n",
    "        f1, _ = get_example_f1_and_counts_list(l, p)\n",
    "        modif_f1_scores.append(f1)\n",
    "    print('done calculating f1 scores')\n",
    "    # Calculate mean F1 scores\n",
    "    ori_mean_f1 = np.mean(ori_f1_scores)\n",
    "    modif_mean_f1 = np.mean(modif_f1_scores)\n",
    "    # Calculate percentage difference\n",
    "    mean_pct_diff = ((modif_mean_f1 - ori_mean_f1) / ori_mean_f1) * 100\n",
    "    # Perform t-test\n",
    "    _, p_value_mw = stats.mannwhitneyu(ori_f1_scores, modif_f1_scores, alternative='two-sided')\n",
    "    _, p_value_w = stats.wilcoxon(ori_f1_scores, modif_f1_scores)\n",
    "    p_value = min(p_value_mw, p_value_w)\n",
    "    \n",
    "    # Determine significance level\n",
    "    if p_value < 0.01:\n",
    "        significance = \"**\"\n",
    "    elif p_value < 0.05:\n",
    "        significance = \"*\"\n",
    "    elif p_value < 0.1:\n",
    "        significance = \".\"\n",
    "    else:\n",
    "        significance = \"ns\"\n",
    "        \n",
    "    print(f\"\\n{model} - {modification.upper()} Modification:\")\n",
    "    print(f\"Original Mean F1: {ori_mean_f1:.3f}\")\n",
    "    print(f\"Modified Mean F1: {modif_mean_f1:.3f}\")\n",
    "    print(f\"Mean F1 Percentage Change: {mean_pct_diff:.1f}%\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    print(f\"Significance: {significance}\")\n",
    "    \n",
    "    # For negation, get subtype results\n",
    "    if modification == 'negation':\n",
    "        for subtype in ['verbal', 'lexical', 'double', 'approximate', 'absolute']:\n",
    "            subtype_df = df[df['type'] == subtype]\n",
    "            if len(subtype_df) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate F1 scores for subtype\n",
    "            ori_subtype_f1 = []\n",
    "            mod_subtype_f1 = []\n",
    "            \n",
    "            for l, p in zip(subtype_df['original_label'], subtype_df['original_pred']):\n",
    "                f1, _ = get_example_f1_and_counts_list(l, p)\n",
    "                ori_subtype_f1.append(f1)\n",
    "                \n",
    "            for l, p in zip(subtype_df['modified_label'], subtype_df['modified_pred']):\n",
    "                f1, _ = get_example_f1_and_counts_list(l, p)\n",
    "                mod_subtype_f1.append(f1)\n",
    "                \n",
    "            # Calculate stats\n",
    "            ori_mean = np.mean(ori_subtype_f1)\n",
    "            mod_mean = np.mean(mod_subtype_f1)\n",
    "            pct_diff = ((mod_mean - ori_mean) / ori_mean) * 100\n",
    "            \n",
    "            # Statistical tests\n",
    "            _, p_mw = stats.mannwhitneyu(ori_subtype_f1, mod_subtype_f1, alternative='two-sided')\n",
    "            _, p_w = stats.wilcoxon(ori_subtype_f1, mod_subtype_f1)\n",
    "            p_val = min(p_mw, p_w)\n",
    "            \n",
    "            # Determine significance\n",
    "            if p_val < 0.001:\n",
    "                sig = '***'\n",
    "            elif p_val < 0.01:\n",
    "                sig = '**'\n",
    "            elif p_val < 0.05:\n",
    "                sig = '*'\n",
    "            elif p_val < 0.1:\n",
    "                sig = '.'\n",
    "            else:\n",
    "                sig = 'ns'\n",
    "                \n",
    "            # Store subtype results\n",
    "            # results_data.append({\n",
    "            #     'model': model,\n",
    "            #     'modification': f'negation_{subtype}',\n",
    "            #     'original_mean_f1': ori_mean,\n",
    "            #     'modified_mean_f1': mod_mean,\n",
    "            #     'mean_f1_pct_change': pct_diff,\n",
    "            #     'p_value': p_val,\n",
    "            #     'significance': sig\n",
    "            # })\n",
    "            \n",
    "            # Also store in negation results\n",
    "            negation_results_data.append({\n",
    "                'model': model,\n",
    "                'negation_type': subtype,\n",
    "                'original_mean_f1': ori_mean,\n",
    "                'modified_mean_f1': mod_mean,\n",
    "                'mean_f1_pct_change': pct_diff,\n",
    "                'sample_size': len(subtype_df),\n",
    "                'p_value': p_val,\n",
    "                'significance': sig\n",
    "            })\n",
    "    \n",
    "    # Store main results\n",
    "    results_data.append({\n",
    "        'model': model,\n",
    "        'modification': modification,\n",
    "        'original_mean_f1': ori_mean_f1,\n",
    "        'modified_mean_f1': modif_mean_f1,\n",
    "        'mean_f1_pct_change': mean_pct_diff,\n",
    "        'p_value': p_value,\n",
    "        'significance': significance\n",
    "    })\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv('ner_modification_results_llm.csv', index=False)\n",
    "print(\"\\nResults saved to ner_modification_results_llm.csv\")\n",
    "\n",
    "# Save negation results\n",
    "negation_results_df = pd.DataFrame(negation_results_data)\n",
    "negation_results_df.to_csv('ner_negation_type_results_llm.csv', index=False)\n",
    "print(\"\\nNegation results saved to ner_negation_type_results_llm.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined results saved to ner_modification_results_combined.csv\n",
      "\n",
      "Combined negation results saved to ner_negation_type_results_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# Read both CSV files\n",
    "llm_results = pd.read_csv('ner_modification_results_llm.csv')\n",
    "plm_results = pd.read_csv('ner_modification_results_plm.csv')\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_results = pd.concat([llm_results, plm_results], ignore_index=True)\n",
    "\n",
    "# Save combined results\n",
    "combined_results.to_csv('ner_modification_results_combined.csv', index=False)\n",
    "print(\"\\nCombined results saved to ner_modification_results_combined.csv\")\n",
    "\n",
    "negation_results_llm = pd.read_csv('ner_negation_type_results_llm.csv')\n",
    "negation_results_plm = pd.read_csv('ner_negation_type_results_plm.csv')\n",
    "\n",
    "combined_negation_results = pd.concat([negation_results_llm, negation_results_plm], ignore_index=True)\n",
    "\n",
    "combined_negation_results.to_csv('ner_negation_type_results_combined.csv', index=False)\n",
    "print(\"\\nCombined negation results saved to ner_negation_type_results_combined.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification_order =[\"B: Tem\", \"B: Geo\", \"B: Len\", \"O: Spell\",\"O: Cap\",\"O: Punc\",\n",
    "\"M: Deri\",\n",
    "\"M: Com\",\n",
    "\"Sx: Voice\",\n",
    "\"Sx: Gra\",\n",
    "\"Sx: Conj\",\n",
    "\"Sm: Con\",\n",
    "\"P: Neg\",\n",
    "\"P: Disc\",\n",
    "\"P: Senti\",\n",
    "\"G: Cas\",\n",
    "\"G: Dial\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model              BERT       GPT2         T5     claude     gpt4o      llama\n",
      "modification                                                                 \n",
      "B: Tem         2.787536  -4.733035   0.622142  -8.020563 -6.712789  -2.299926\n",
      "B: Geo        -0.589161   1.979713   2.789691  -6.793095  7.624427   1.178289\n",
      "B: Len        -5.926327   0.600757   1.492077 -10.196811  9.750040   2.534286\n",
      "O: Spell      -3.115792  -1.889596  -1.743717  -2.198822  1.101309   2.911412\n",
      "O: Cap        -1.201542 -30.717541 -17.331313  -6.976406 -1.054874 -11.495124\n",
      "O: Punc       -7.045910   2.830253  -4.052530 -17.753104 -6.879200 -12.176371\n",
      "M: Deri        0.795521  -5.484404   2.035902  -3.468768 -0.013186  -2.055611\n",
      "M: Com        -4.306794  -0.656631   4.131068  -3.367877 -2.935154  -1.668917\n",
      "Sx: Voice     -2.338702   0.358320   2.298551   3.637659  5.460552   3.778924\n",
      "Sx: Gra       -3.437102  -5.586152  -1.388786  -3.863605  3.430515  -0.593476\n",
      "Sx: Conj       3.046604   0.977305   1.951889  -0.428269  7.336064   0.333971\n",
      "Sm: Con       -3.119287  -7.446183  -1.385813  -0.165715  7.869858  -5.217159\n",
      "P: Neg        -4.159229  -0.411368  -0.062517  -0.234765  0.214433  -1.605728\n",
      "P: Disc        0.383095  -2.520344  -1.900085  -1.917597  0.172895  -2.029334\n",
      "P: Senti      -1.280424  -2.274573  -4.584480  -5.437567  0.097721  -2.896706\n",
      "G: Cas        -5.926327   0.600757   1.492077 -10.196811  9.750040   2.534286\n",
      "G: Dial      -17.854929  -7.863605  -8.381319  -2.987667  6.367236  -1.877708\n",
      "LaTeX table saved to ner_results_table.tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "df = pd.read_csv('ner_modification_results_combined.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "mod_mapping = {\n",
    "    'temporal_bias': 'B: Tem',\n",
    "    'geographical_bias': 'B: Geo', \n",
    "    'length_bias': 'B: Len',\n",
    "    'typo_bias': 'O: Spell',\n",
    "    'capitalization': 'O: Cap',\n",
    "    'punctuation': 'O: Punc',\n",
    "    'derivation': 'M: Deri',\n",
    "    'compound_word': 'M: Com',\n",
    "    'active_to_passive': 'Sx: Voice',\n",
    "    'grammatical_role': 'Sx: Gra',\n",
    "    'coordinating_conjunction': 'Sx: Conj',\n",
    "    'concept_replacement': 'Sm: Con',\n",
    "    'negation': 'P: Neg',\n",
    "    'discourse': 'P: Disc',\n",
    "    'sentiment': 'P: Senti',\n",
    "    'casual': 'G: Cas',\n",
    "    'dialectal': 'G: Dial'\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "model_order = ['BERT', 'GPT2', 'T5', 'gpt4o', 'claude', 'llama']\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='mean_f1_pct_change')\n",
    "p_values = df.pivot(index='modification', columns='model', values='p_value')\n",
    "significance = df.pivot(index='modification', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows according to modification_order\n",
    "pivot_df = pivot_df.reindex(modification_order)\n",
    "p_values = p_values.reindex(modification_order)\n",
    "significance = significance.reindex(modification_order)\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(row[col], significance.loc[idx, col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open('ner_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to ner_results_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results_df.csv\n",
      "Negation results saved to ner_negation_results_df.csv\n"
     ]
    }
   ],
   "source": [
    "# Load results from CSV\n",
    "results_df = pd.read_csv('ner_modification_results_combined.csv')\n",
    "negation_results_df = pd.read_csv('ner_negation_type_results_combined.csv')\n",
    "# Create lists of unique modifications and models\n",
    "modification_order = list(mod_mapping.keys())\n",
    "model_order = ['BERT', 'GPT2', 'T5', 'gpt4o', 'claude', 'llama']\n",
    "negation_order = ['verbal', 'lexical', 'double', 'approximate', 'absolute']\n",
    "# Create empty DataFrame with multi-level columns\n",
    "columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "results_df_pivot = pd.DataFrame(index=modification_order, columns=columns)\n",
    "\n",
    "negation_columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "negation_results_df_pivot = pd.DataFrame(index=negation_order, columns=negation_columns)\n",
    "# Fill DataFrame\n",
    "for mod in modification_order:\n",
    "    for model in model_order:\n",
    "        row = results_df[(results_df['modification'] == mod) & (results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            results_df_pivot.loc[mod, (model, 'original')] = row['original_mean_f1'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'modified')] = row['modified_mean_f1'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'diff')] = row['mean_f1_pct_change'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "        \n",
    "for mod in negation_order:\n",
    "    for model in model_order:\n",
    "        row = negation_results_df[(negation_results_df['negation_type'] == mod) & (negation_results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            negation_results_df_pivot.loc[mod, (model, 'original')] = row['original_mean_f1'].values[0]\n",
    "            negation_results_df_pivot.loc[mod, (model, 'modified')] = row['modified_mean_f1'].values[0]\n",
    "            negation_results_df_pivot.loc[mod, (model, 'diff')] = row['mean_f1_pct_change'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "results_df_pivot.to_csv('ner_results_df.csv')\n",
    "negation_results_df_pivot.to_csv('ner_negation_results_df.csv')\n",
    "\n",
    "print(\"Results saved to ner_results_df.csv\")\n",
    "print(\"Negation results saved to ner_negation_results_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model              BERT      GPT2         T5     claude      gpt4o      llama\n",
      "negation_type                                                                \n",
      "Verbal        -8.037896  0.238901   3.251348  -2.830057  -3.295228  -0.816556\n",
      "Lexical        2.520923 -8.104994   1.110489  -6.573358   3.410971   0.271820\n",
      "Double        -1.548939 -6.375114 -16.877981   7.246377  17.110930   5.379943\n",
      "Approximate   -3.625931  0.296912  -3.466006  10.869565   6.963979 -15.116684\n",
      "Absolute      -7.806122  8.631210   5.836081  -0.220512 -10.765101  -0.810453\n",
      "LaTeX table saved to ner_results_table.tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "df = pd.read_csv('ner_negation_type_results_combined.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "negation_order= ['Verbal', 'Lexical', 'Double', 'Approximate', 'Absolute']\n",
    "mod_mapping = {\n",
    "    'verbal': 'Verbal',\n",
    "    'lexical': 'Lexical',\n",
    "    'double': 'Double',\n",
    "    'approximate': 'Approximate',\n",
    "    'absolute': 'Absolute',\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "model_order = ['BERT', 'GPT2', 'T5', 'gpt4o', 'claude', 'llama']\n",
    "\n",
    "# Map the modification names\n",
    "df['negation_type'] = df['negation_type'].map(mod_mapping)\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='negation_type', columns='model', values='mean_f1_pct_change')\n",
    "p_values = df.pivot(index='negation_type', columns='model', values='p_value')\n",
    "significance = df.pivot(index='negation_type', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows according to modification_order\n",
    "pivot_df = pivot_df.reindex(negation_order)\n",
    "p_values = p_values.reindex(negation_order)\n",
    "significance = significance.reindex(negation_order)\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(row[col], significance.loc[idx, col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open('ner_negation_type_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to ner_results_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
