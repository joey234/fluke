{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space(text):\n",
    "    # Remove multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # lines = text.split('\\n')\n",
    "    # for i,line in enumerate(lines):\n",
    "        \n",
    "    # lines[i] = lines[i].replace('  ', ' ')\n",
    "    # Fix spacing around punctuation\n",
    "    text = re.sub(r'\\s+([.,!?])', r'\\1', text)\n",
    "    text = re.sub(r'([.,!?])\\s+', r'\\1 ', text)\n",
    "    \n",
    "    # Fix contractions\n",
    "    text = re.sub(r'\\s*\\'\\s*s\\b', \"'s\", text)\n",
    "    text = re.sub(r'\\s*n\\s*\\'\\s*t\\b', \"n't\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*ve\\b', \"'ve\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*re\\b', \"'re\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*ll\\b', \"'ll\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*d\\b', \"'d\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*m\\b', \"'m\", text)\n",
    "    \n",
    "    # Fix spaces around parentheses\n",
    "    text = re.sub(r'\\(\\s+', '(', text)\n",
    "    text = re.sub(r'\\s+\\)', ')', text)\n",
    "    \n",
    "    # Remove spaces before and after text\n",
    "    text = text.strip()\n",
    "    text = text.replace('agent 0: ','')\n",
    "    text = text.replace('agent 1: ','')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../data/modified_data/ner/sentiment.json\n",
      "Processing: ../data/modified_data/ner/geographical_bias.json\n",
      "Error processing file ../data/modified_data/ner/geographical_bias.json: Expecting ',' delimiter: line 20 column 41 (char 730)\n",
      "Processing: ../data/modified_data/ner/negation.json\n",
      "Processing: ../data/modified_data/ner/dialectal.json\n",
      "Processing: ../data/modified_data/ner/active_to_passive.json\n",
      "Processing: ../data/modified_data/ner/discourse.json\n",
      "Processing: ../data/modified_data/ner/derivation.json\n",
      "Processing: ../data/modified_data/ner/compound_word.json\n",
      "Processing: ../data/modified_data/ner/coordinating_conjunction.json\n",
      "Error processing file ../data/modified_data/ner/coordinating_conjunction.json: Expecting ',' delimiter: line 213 column 20 (char 6553)\n",
      "Processing: ../data/modified_data/ner/temporal_bias.json\n",
      "Processing: ../data/modified_data/ner/grammatical_role.json\n",
      "Error processing file ../data/modified_data/ner/grammatical_role.json: Expecting ',' delimiter: line 191 column 20 (char 6182)\n",
      "Processing: ../data/modified_data/ner/concept_replacement.json\n",
      "Processing: ../data/modified_data/ner/length_bias.json\n",
      "Processing: ../data/modified_data/ner/casual.json\n"
     ]
    }
   ],
   "source": [
    "# Get all json and jsonl files in data directory\n",
    "json_files = glob.glob('../data/modified_data/ner/*.json*', recursive=True)\n",
    "\n",
    "# Process each file\n",
    "for file_path in json_files:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read data based on file extension\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            if file_path.endswith('.jsonl'):\n",
    "                # For jsonl files, read line by line and convert to json\n",
    "                data = []\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        # Handle escaped characters by using raw string\n",
    "                        line = line.replace('\\\\', '\\\\\\\\')\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error processing line in {file_path}: {e}\")\n",
    "                        continue\n",
    "            else:\n",
    "                # For json files, load entire file\n",
    "                content = f.read()\n",
    "                content = content.replace('\\\\', '\\\\\\\\')\n",
    "                data = json.loads(content)\n",
    "        \n",
    "        # ... rest of the processing code ...\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        continue    # Process all string values recursively\n",
    "    def process_strings(obj):\n",
    "        if isinstance(obj, str):\n",
    "            return remove_space(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return [process_strings(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: process_strings(v) for k, v in obj.items()}\n",
    "        return obj\n",
    "    \n",
    "    processed_data = process_strings(data)\n",
    "    # Save processed data back to file as json\n",
    "    output_path = file_path.replace('.jsonl', '.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(processed_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../data/modified_data/dialog/casual_100.json\n",
      "Processing: ../data/modified_data/dialog/discourse_100.json\n",
      "Processing: ../data/modified_data/dialog/compound_word_100.json\n",
      "Processing: ../data/modified_data/dialog/sentiment.json\n",
      "Processing: ../data/modified_data/dialog/temporal_bias_100.json\n",
      "Processing: ../data/modified_data/dialog/geographical_bias.json\n",
      "Processing: ../data/modified_data/dialog/coordinating_conjunction_100.json\n",
      "Processing: ../data/modified_data/dialog/capitalization_100.json\n",
      "Processing: ../data/modified_data/dialog/dialectal_100.json\n",
      "Processing: ../data/modified_data/dialog/sentiment_100.json\n",
      "Processing: ../data/modified_data/dialog/grammatical_role_100.json\n",
      "Processing: ../data/modified_data/dialog/negation.json\n",
      "Processing: ../data/modified_data/dialog/length_bias_100.json\n",
      "Processing: ../data/modified_data/dialog/concept_replacement_100.json\n",
      "Processing: ../data/modified_data/dialog/typo_bias_100.json\n",
      "Processing: ../data/modified_data/dialog/geographical_bias_100.json\n",
      "Processing: ../data/modified_data/dialog/dialectal.json\n",
      "Processing: ../data/modified_data/dialog/active_to_passive.json\n",
      "Processing: ../data/modified_data/dialog/punctuation_100.json\n",
      "Processing: ../data/modified_data/dialog/discourse.json\n",
      "Processing: ../data/modified_data/dialog/derivation.json\n",
      "Processing: ../data/modified_data/dialog/derivation_100.json\n",
      "Processing: ../data/modified_data/dialog/compound_word.json\n",
      "Processing: ../data/modified_data/dialog/coordinating_conjunction.json\n",
      "Processing: ../data/modified_data/dialog/temporal_bias.json\n",
      "Processing: ../data/modified_data/dialog/grammatical_role.json\n",
      "Processing: ../data/modified_data/dialog/concept_replacement.json\n",
      "Processing: ../data/modified_data/dialog/active_to_passive_100.json\n",
      "Processing: ../data/modified_data/dialog/length_bias.json\n",
      "Processing: ../data/modified_data/dialog/casual.json\n",
      "Processing: ../data/modified_data/dialog/negation_100.json\n"
     ]
    }
   ],
   "source": [
    "# Get all json and jsonl files in data directory\n",
    "json_files = glob.glob('../data/modified_data/dialog/*.json', recursive=True)\n",
    "\n",
    "# Process each file\n",
    "for file_path in json_files:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    \n",
    "    # try:\n",
    "    #     # Read data based on file extension\n",
    "    #     with open(file_path, 'r') as f:\n",
    "    #         if file_path.endswith('.json'):\n",
    "    #             # For jsonl files, read line by line and convert to json\n",
    "    #             data = []\n",
    "    #             for line in f:\n",
    "    #                 try:\n",
    "    #                     # Handle escaped characters by using raw string\n",
    "    #                     line = line.replace('\\\\', '\\\\\\\\')\n",
    "    #                     data.append(json.loads(line))\n",
    "    #                 except json.JSONDecodeError as e:\n",
    "    #                     print(f\"Error processing line in {file_path}: {e}\")\n",
    "    #                     continue\n",
    "    #         else:\n",
    "    #             # For json files, load entire file\n",
    "    #             content = f.read()\n",
    "    #             content = content.replace('\\\\', '\\\\\\\\')\n",
    "    #             data = json.loads(content)\n",
    "            \n",
    "        # ... rest of the processing code ...\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error processing file {file_path}: {e}\")\n",
    "    #     continue    # Process all string values recursively\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        continue    # Process all string values recursively\n",
    "\n",
    "    def process_strings(obj):\n",
    "        if isinstance(obj, str):\n",
    "            return remove_space(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return [process_strings(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: process_strings(v) for k, v in obj.items()}\n",
    "        return obj\n",
    "    \n",
    "    processed_data = process_strings(data)\n",
    "    \n",
    "    # Save processed data back to file as json\n",
    "    output_path = file_path.replace('.jsonl', '.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(processed_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../data/modified_data/ner/sentiment.json\n",
      "finished\n",
      "{'original_text': \"Loyalists recruited from Queens County, New York by Lieutenant Colonel Richard Hewlett for the 3rd battalion DeLancey's Brigade had established a fortified position in early August on the central north shore of Long Island at Setauket, just across Long Island Sound from Fairfield.\", 'type': 'sentiment', 'modified_text': \"Loyalists reluctantly recruited from Queens County, New York by Lieutenant Colonel Richard Hewlett for the 3rd battalion DeLancey's Brigade had barely established a fortified position in early August on the central north shore of Long Island at Setauket, just across Long Island Sound from Fairfield.\", 'Rationale': \"The words 'reluctantly' and 'barely' were added to introduce a sense of hesitation and minimal success, which imparts a negative sentiment to the sentence.\", 'label': [{'text': 'Queens County', 'value': 'LOCATION'}, {'text': 'New York', 'value': 'LOCATION'}, {'text': 'Richard Hewlett', 'value': 'PERSON'}, {'text': \"DeLancey's Brigade\", 'value': 'ORGANIZATION'}, {'text': 'Long Island', 'value': 'LOCATION'}, {'text': 'Setauket', 'value': 'LOCATION'}, {'text': 'Long Island Sound', 'value': 'LOCATION'}, {'text': 'Fairfield', 'value': 'LOCATION'}], 'original_text_highlight_spans': [], 'modified_text_highlight_spans': [[9, 21], [143, 150]]}\n",
      "Processing: ../data/modified_data/ner/geographical_bias.json\n",
      "Processing: ../data/modified_data/ner/negation.json\n",
      "finished\n",
      "{'original_text': 'In the early 1930s the band moved to the Grill Room of the Taft Hotel in New York ; the band was renamed ``George Hall and His Hotel Taft Orchestra``.', 'type': 'double', 'modified_text': \"In the early 1930s the band didn't fail to move to the Grill Room of the Taft Hotel in New York; the band was renamed ``George Hall and His Hotel Taft Orchestra``.\", 'Rationale': \"The double negation is introduced by adding 'didn't fail to' before the verb 'move', which negates the negation and maintains the affirmative meaning of the original sentence.\", 'label': [{'text': 'Grill Room', 'value': 'BUILDING'}, {'text': 'Taft Hotel', 'value': 'BUILDING'}, {'text': 'New York', 'value': 'LOCATION'}, {'text': 'George Hall and His Hotel Taft Orchestra', 'value': 'ORGANIZATION'}], 'original_text_highlight_spans': [[32, 33]], 'modified_text_highlight_spans': [[28, 43]]}\n",
      "Processing: ../data/modified_data/ner/dialectal.json\n",
      "finished\n",
      "{'original_text': 'It has been used to treat hypertension and cardiac failure, and to treat pre-eclampsia during pregnancy.', 'type': 'singaporean_english', 'modified_text': 'It has been used for treating hypertension and cardiac failure lah, and also can use for pre-eclampsia during pregnancy one.', 'Rationale': \"The modification includes the use of 'lah' which is a common particle in Singlish for emphasis, and 'one' at the end of the sentence to indicate a general statement. 'Also can use' is a typical Singlish construction indicating possibility or capability.\", 'label': [{'text': 'hypertension', 'value': 'OTHER'}, {'text': 'cardiac failure', 'value': 'OTHER'}, {'text': 'pre-eclampsia', 'value': 'OTHER'}], 'original_text_highlight_spans': [[17, 18], [64, 65], [67, 68], [69, 72]], 'modified_text_highlight_spans': [[17, 18], [19, 20], [26, 29], [62, 66], [72, 75], [77, 87], [119, 123]]}\n",
      "Processing: ../data/modified_data/ner/active_to_passive.json\n",
      "finished\n",
      "{'original_text': 'Back to Basics focuses on genre classics, showing older movies and underground cult films, with occasional stabs at mainstream and newer genre pieces.', 'type': 'active_to_passive', 'modified_text': 'Genre classics are focused on by Back to Basics, with older movies and underground cult films being shown, and with occasional stabs at mainstream and newer genre pieces.', 'Rationale': \"The original sentence was in active voice, so it was transformed into passive voice by making the object ('genre classics') the subject of the sentence, and using the passive construction 'are focused on by'. The rest of the sentence was adjusted to maintain grammatical correctness and natural flow while keeping the meaning and entity names unchanged.\", 'label': [{'text': 'Back to Basics', 'value': 'ORGANIZATION'}], 'original_text_highlight_spans': [[14, 40], [42, 45], [47, 49]], 'modified_text_highlight_spans': [[0, 33], [51, 53], [93, 105], [106, 110]]}\n",
      "Processing: ../data/modified_data/ner/discourse.json\n",
      "finished\n",
      "{'original_text': 'Moreover, Santa is actually innocent of the crime, which was instead masterminded by scheming relative Cousin Mel, who is mentioned briefly in the song but made into a gold-digging villainness in the special.', 'type': 'delete', 'modified_text': 'Santa is actually innocent of the crime, which was instead masterminded by scheming relative Cousin Mel, who is mentioned briefly in the song, made into a gold-digging villainness in the special.', 'Rationale': \"Deleted the discourse marker 'Moreover' to remove the additive aspect of the sentence and also removed 'but' to directly connect the two clauses without indicating contrast.\", 'label': [{'text': 'Santa', 'value': 'PERSON'}, {'text': 'Cousin Mel', 'value': 'PERSON'}], 'original_text_highlight_spans': [[0, 10], [151, 155]], 'modified_text_highlight_spans': [[141, 142]]}\n",
      "Processing: ../data/modified_data/ner/derivation.json\n",
      "finished\n",
      "{'original_text': 'The government announced a state funeral and a day of national mourning.', 'type': 'derivation', 'modified_text': 'The government announced a state funeral and a day of national grief.', 'Rationale': \"'mourning' is derived from mourn using a -ing suffix. The non-derived word 'grief' is used to replace 'mourning'.\", 'label': [], 'original_text_highlight_spans': [[63, 66], [67, 68], [69, 71]], 'modified_text_highlight_spans': [[63, 64], [66, 68]]}\n",
      "Processing: ../data/modified_data/ner/compound_word.json\n",
      "finished\n",
      "{'original_text': 'Most of the town is actually a gated community called Orchid Island Golf and Beach Club.', 'type': 'compound_word', 'modified_text': 'Most of the town is actually a high-security gated community called Orchid Island Golf and Beach Club.', 'Rationale': \"The word 'gated' was replaced with the compound word 'high-security gated' to add a compound word to the sentence. This modification was made near the named entity 'Orchid Island Golf and Beach Club' and does not change the meaning of the sentence significantly.\", 'label': [{'text': 'Orchid Island Golf and Beach Club', 'value': 'LOCATION'}], 'original_text_highlight_spans': [], 'modified_text_highlight_spans': [[30, 44]]}\n",
      "Processing: ../data/modified_data/ner/coordinating_conjunction.json\n",
      "Processing: ../data/modified_data/ner/temporal_bias.json\n",
      "finished\n",
      "{'original_text': 'Moreover, Santa is actually innocent of the crime, which was instead masterminded by scheming relative Cousin Mel, who is mentioned briefly in the song but made into a gold-digging villainness in the special.', 'type': 'temporal_bias', 'modified_text': 'Moreover, Santa is actually innocent of the crime, which was instead masterminded by scheming relative Cousin Mel, who is mentioned briefly in the song but made into a gold-digging villainess in the special.', 'Rationale': \"The word 'villainness' was replaced with the old-fashioned term 'villainess' to maintain the connection with the entity 'Cousin Mel' and to introduce a subtle change that could expose biases in machine learning models. The modification was made to align with the task of using an old-fashioned word while keeping the sentence grammatically correct and sounding natural.\", 'label': [{'text': 'Santa', 'value': 'PERSON'}, {'text': 'Cousin Mel', 'value': 'PERSON'}], 'original_text_highlight_spans': [[188, 189]], 'modified_text_highlight_spans': []}\n",
      "Processing: ../data/modified_data/ner/grammatical_role.json\n",
      "Processing: ../data/modified_data/ner/concept_replacement.json\n",
      "finished\n",
      "{'original_text': 'It is developed by Golaem, a France -based software company (created in Rennes in 2009).', 'type': 'idiom', 'modified_text': 'It is the brainchild of Golaem, a France-based software company (born in Rennes in 2009).', 'Rationale': \"The phrase 'the brainchild of' is an idiomatic expression that replaces 'developed by' to indicate that Golaem is the originator or creator of the product. The word 'born' replaces 'created' to maintain the metaphor of birth for the inception of the company. These changes preserve the original entities and the overall meaning of the sentence while incorporating idiomatic language.\", 'label': [{'text': 'Golaem', 'value': 'ORGANIZATION'}, {'text': 'France', 'value': 'LOCATION'}, {'text': 'Rennes', 'value': 'LOCATION'}], 'original_text_highlight_spans': [[6, 7], [8, 10], [11, 14], [16, 18], [61, 62], [63, 68]], 'modified_text_highlight_spans': [[6, 8], [9, 18], [21, 23], [65, 67], [68, 69]]}\n",
      "Processing: ../data/modified_data/ner/length_bias.json\n",
      "finished\n",
      "{'original_text': \"He went on to produce Kim Fowley and the BMX Bandits (band) Receiver Records' album ``Hidden Agenda At the Thirteenth Not``.\", 'type': 'length_bias', 'modified_text': \"He produced Kim Fowley and BMX Bandits' ``Hidden Agenda At the Thirteenth Not`` for Receiver Records.\", 'Rationale': \"To shorten the sentence, unnecessary words such as 'went on to' and '(band)' were removed. The phrase 'Receiver Records' album' was restructured to 'for Receiver Records' to maintain grammatical correctness while reducing length. The meaning of the sentence remains the same, focusing on the production of the album by the entities involved.\", 'label': [{'text': 'Kim Fowley', 'value': 'PERSON'}, {'text': 'BMX Bandits', 'value': 'ORGANIZATION'}, {'text': 'Receiver Records', 'value': 'ORGANIZATION'}, {'text': 'Hidden Agenda At the Thirteenth Not', 'value': 'ART'}], 'original_text_highlight_spans': [[2, 13], [37, 41], [52, 76], [77, 83]], 'modified_text_highlight_spans': [[10, 11], [79, 100]]}\n",
      "Processing: ../data/modified_data/ner/casual.json\n"
     ]
    }
   ],
   "source": [
    "json_files = glob.glob('../data/modified_data/ner/*.json*', recursive=True)\n",
    "\n",
    "# Process each file\n",
    "for file_path in json_files:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    new_data = []\n",
    "    flag = False\n",
    "    for sample in data:\n",
    "        if sample.get('Label') != None:\n",
    "            flag = True\n",
    "            label = sample['Label']\n",
    "            new_label = []\n",
    "            for pair in label:\n",
    "                for key, value in pair.items():\n",
    "                    item = {'text': key, 'value': value}\n",
    "                    new_label.append(item)\n",
    "            sample['label'] = new_label\n",
    "            # print(sample['label'])\n",
    "            del sample['Label']\n",
    "            new_data.append(sample)\n",
    "    if flag == True:\n",
    "        print('finished')\n",
    "        print(new_data[0])\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(new_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/modified_data/dialog/casual_100.json\n",
      "../data/modified_data/dialog/discourse_100.json\n",
      "../data/modified_data/dialog/compound_word_100.json\n",
      "../data/modified_data/dialog/temporal_bias_100.json\n",
      "../data/modified_data/dialog/coordinating_conjunction_100.json\n",
      "../data/modified_data/dialog/capitalization_100.json\n",
      "../data/modified_data/dialog/dialectal_100.json\n",
      "../data/modified_data/dialog/sentiment_100.json\n",
      "../data/modified_data/dialog/grammatical_role_100.json\n",
      "Loading: ../data/modified_data/dialog/grammatical_role_100.json\n",
      "Preserve: 75\n",
      "../data/modified_data/dialog/length_bias_100.json\n",
      "../data/modified_data/dialog/concept_replacement_100.json\n",
      "../data/modified_data/dialog/typo_bias_100.json\n",
      "../data/modified_data/dialog/geographical_bias_100.json\n",
      "../data/modified_data/dialog/punctuation_100.json\n",
      "../data/modified_data/dialog/derivation_100.json\n",
      "../data/modified_data/dialog/active_to_passive_100.json\n",
      "../data/modified_data/dialog/negation_100.json\n",
      "Loading: ../data/modified_data/dialog/negation_100.json\n",
      "Preserve: 100\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "import glob\n",
    "# import os\n",
    "import random\n",
    "\n",
    "# Load all JSON files from the specified directory\n",
    "json_files = glob.glob('../data/modified_data/dialog/*_100.json', recursive=True)\n",
    "\n",
    "# Initialize a set to track unique modified_text entries\n",
    "\n",
    "# Process each file\n",
    "for file_path in json_files:\n",
    "    print(file_path)\n",
    "    if 'negation' not in file_path and 'grammatical_role' not in file_path:\n",
    "        continue\n",
    "    print(f\"Loading: {file_path}\")\n",
    "    unique_modified_texts = set()\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    new_data = []  # Reset new_data for each file\n",
    "    for sample in data:\n",
    "        if isinstance(sample, list):\n",
    "            modified_text = sample[1].get('modified_text')\n",
    "            modified_label = sample[1].get('modified_label')\n",
    "        else:\n",
    "            modified_text = sample.get('modified_text')\n",
    "            modified_label = sample.get('modified_label')\n",
    "        \n",
    "        if modified_label and modified_text and modified_text not in unique_modified_texts:\n",
    "            unique_modified_texts.add(modified_text)\n",
    "            new_data.append(sample)\n",
    "\n",
    "    # Sample more data if we have less than 100 entries\n",
    "    if len(new_data) < 100:\n",
    "        # Load the original data again without the _100 suffix\n",
    "        original_file_path = file_path.replace('_100', '')\n",
    "        if 'capitalization' in original_file_path or 'punctuation' in original_file_path or 'typo' in original_file_path:\n",
    "            continue\n",
    "        with open(original_file_path, 'r') as f:\n",
    "            original_data = json.load(f)\n",
    "        \n",
    "        # Filter out already included modified_texts\n",
    "        if isinstance(sample, list):\n",
    "            additional_samples = [s for s in original_data if s[1].get('modified_text') not in unique_modified_texts and s[1].get('modified_label') != 2]\n",
    "        else:\n",
    "            additional_samples = [s for s in original_data if s.get('modified_text') not in unique_modified_texts and s.get('modified_label') != 2]\n",
    "\n",
    "        random.shuffle(additional_samples)  # Shuffle to get random samples\n",
    "\n",
    "        # Add samples until we reach 100\n",
    "        for sample in additional_samples:\n",
    "            if len(new_data) >= 100:\n",
    "                break\n",
    "            new_data.append(sample)\n",
    "            if isinstance(sample, list):\n",
    "                unique_modified_texts.add(sample[1].get('modified_text'))\n",
    "            else:\n",
    "                unique_modified_texts.add(sample.get('modified_text'))\n",
    "\n",
    "    # Save the deduplicated data back to a new JSON file for each original file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(new_data, f, indent=2)\n",
    "\n",
    "    print(f\"Preserve: {len(new_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified active to passive data saved to: ../data/modified_data/coref/active_to_passive.json\n"
     ]
    }
   ],
   "source": [
    "# Load the active to passive data\n",
    "active_to_passive_file_path = '../data/modified_data/coref/active_to_passive.json'\n",
    "with open(active_to_passive_file_path, 'r') as f:\n",
    "    active_to_passive_data = json.load(f)\n",
    "\n",
    "# Replace modified_candidates with original_candidates\n",
    "for sample in active_to_passive_data:\n",
    "    sample['modified_candidates'] = sample['original_candidates']\n",
    "\n",
    "# Save the modified data back to the same file or a new file\n",
    "output_active_to_passive_path = '../data/modified_data/coref/active_to_passive.json'\n",
    "with open(output_active_to_passive_path, 'w') as f:\n",
    "    json.dump(active_to_passive_data, f, indent=2)\n",
    "\n",
    "print(f\"Modified active to passive data saved to: {output_active_to_passive_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified pronouns in: ../data/modified_data/coref/casual_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/discourse_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/compound_word_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/sentiment.json\n",
      "Modified pronouns in: ../data/modified_data/coref/temporal_bias_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/geographical_bias.json\n",
      "Modified pronouns in: ../data/modified_data/coref/coordinating_conjunction_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/capitalization_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/dialectal_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/sentiment_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/grammatical_role_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/length_bias_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/concept_replacement_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/typo_bias_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/geographical_bias_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/dialectal.json\n",
      "Modified pronouns in: ../data/modified_data/coref/active_to_passive.json\n",
      "Modified pronouns in: ../data/modified_data/coref/punctuation_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/discourse.json\n",
      "Modified pronouns in: ../data/modified_data/coref/derivation.json\n",
      "Modified pronouns in: ../data/modified_data/coref/derivation_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/compound_word.json\n",
      "Modified pronouns in: ../data/modified_data/coref/coordinating_conjunction.json\n",
      "Modified pronouns in: ../data/modified_data/coref/temporal_bias.json\n",
      "Modified pronouns in: ../data/modified_data/coref/concept_replacement.json\n",
      "Modified pronouns in: ../data/modified_data/coref/active_to_passive_100.json\n",
      "Modified pronouns in: ../data/modified_data/coref/length_bias.json\n",
      "Modified pronouns in: ../data/modified_data/coref/casual.json\n",
      "Modified pronouns in: ../data/modified_data/coref/negation_100.json\n"
     ]
    }
   ],
   "source": [
    "# Load all JSON files in the specified directory\n",
    "json_files = glob.glob('../data/modified_data/coref/*.json')\n",
    "\n",
    "# Define the files to exclude\n",
    "exclude_files = [\n",
    "    'capitalization.json',\n",
    "    'punctuation.json',\n",
    "    'typo_bias.json',\n",
    "    'grammatical_role.json',\n",
    "    'negation.json'\n",
    "]\n",
    "\n",
    "# Process each JSON file\n",
    "for file_path in json_files:\n",
    "    if os.path.basename(file_path) in exclude_files:\n",
    "        continue\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Replace modified_pronoun with original_pronoun\n",
    "    for sample in data:\n",
    "        sample['modified_pronoun'] = sample['original_pronoun']\n",
    "\n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Modified pronouns in: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences in negation: 27\n",
      "Differences in grammatical_role: 11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize counters\n",
    "negation_differences = 0\n",
    "grammatical_role_differences = 0\n",
    "\n",
    "# Load all JSON files in the specified directory\n",
    "json_files = os.listdir('../data/modified_data/coref/')\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files:\n",
    "    if file_name in ['negation.json', 'grammatical_role.json']:\n",
    "        with open(f'../data/modified_data/coref/{file_name}', 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Count differences in modified_label and original_label, ignoring label 2\n",
    "        for sample in data:\n",
    "            if sample['modified_label'] != sample['original_label'] and sample['modified_label'] != 2:\n",
    "                if file_name == 'negation.json':\n",
    "                    negation_differences += 1\n",
    "                elif file_name == 'grammatical_role.json':\n",
    "                    grammatical_role_differences += 1\n",
    "\n",
    "print(f\"Differences in negation: {negation_differences}\")\n",
    "print(f\"Differences in grammatical_role: {grammatical_role_differences}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/modified_data/coref/casual_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 694421.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/casual_100.json\n",
      "../data/modified_data/coref/discourse_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 713317.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/discourse_100.json\n",
      "../data/modified_data/coref/compound_word_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 871543.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/compound_word_100.json\n",
      "../data/modified_data/coref/sentiment.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 1241734.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/sentiment.json\n",
      "../data/modified_data/coref/temporal_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 693273.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/temporal_bias_100.json\n",
      "../data/modified_data/coref/geographical_bias.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 1080593.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/geographical_bias.json\n",
      "../data/modified_data/coref/coordinating_conjunction_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 875637.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/coordinating_conjunction_100.json\n",
      "../data/modified_data/coref/capitalization_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 991561.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/capitalization_100.json\n",
      "../data/modified_data/coref/dialectal_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1043359.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/dialectal_100.json\n",
      "../data/modified_data/coref/sentiment_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 806596.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/sentiment_100.json\n",
      "../data/modified_data/coref/grammatical_role_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 828504.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/grammatical_role_100.json\n",
      "../data/modified_data/coref/negation.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 1202495.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/negation.json\n",
      "../data/modified_data/coref/length_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1043359.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/length_bias_100.json\n",
      "../data/modified_data/coref/concept_replacement_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1064544.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/concept_replacement_100.json\n",
      "../data/modified_data/coref/typo_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 694421.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/typo_bias_100.json\n",
      "../data/modified_data/coref/geographical_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1121471.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/geographical_bias_100.json\n",
      "../data/modified_data/coref/dialectal.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:00<00:00, 829448.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/dialectal.json\n",
      "../data/modified_data/coref/active_to_passive.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 820949.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/active_to_passive.json\n",
      "../data/modified_data/coref/punctuation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 870187.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/punctuation_100.json\n",
      "../data/modified_data/coref/discourse.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:00<00:00, 1037103.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/discourse.json\n",
      "../data/modified_data/coref/derivation.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 1201383.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/derivation.json\n",
      "../data/modified_data/coref/derivation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 781062.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/derivation_100.json\n",
      "../data/modified_data/coref/compound_word.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:00<00:00, 972189.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/compound_word.json\n",
      "../data/modified_data/coref/coordinating_conjunction.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:00<00:00, 1189064.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/coordinating_conjunction.json\n",
      "../data/modified_data/coref/temporal_bias.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 147/147 [00:00<00:00, 955911.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/temporal_bias.json\n",
      "../data/modified_data/coref/grammatical_role.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 528869.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/grammatical_role.json\n",
      "../data/modified_data/coref/concept_replacement.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 876857.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/concept_replacement.json\n",
      "../data/modified_data/coref/active_to_passive_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 897429.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/active_to_passive_100.json\n",
      "../data/modified_data/coref/length_bias.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:00<00:00, 889700.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/length_bias.json\n",
      "../data/modified_data/coref/casual.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [00:00<00:00, 931363.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/casual.json\n",
      "../data/modified_data/coref/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 877469.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/coref/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the reference CSV file with indices\n",
    "reference_df = pd.read_json('../data/train_dev_test_data/coref/test.json')\n",
    "\n",
    "# Create a mapping of text to index for O(1) lookup\n",
    "text_to_idx = {row['text']: idx for idx, row in reference_df.iterrows()}\n",
    "\n",
    "# Load all JSON files in the specified directory\n",
    "json_files = os.listdir('../data/modified_data/coref/')\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files:\n",
    "    file_path = f'../data/modified_data/coref/{file_name}'\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add index to each sample based on exact text matching\n",
    "    for sample in tqdm(data):\n",
    "        if sample['original_text'] in text_to_idx:\n",
    "            sample['index'] = int(text_to_idx[sample['original_text']])\n",
    "        else:\n",
    "            print(f\"Warning: No exact match found for sample in {file_name}\")\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Added indices to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/modified_data/sent/casual_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/casual_100.json\n",
      "../data/modified_data/sent/discourse_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:25<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/discourse_100.json\n",
      "../data/modified_data/sent/compound_word_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:21<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/compound_word_100.json\n",
      "../data/modified_data/sent/sentiment.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:30<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/sentiment.json\n",
      "../data/modified_data/sent/temporal_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/temporal_bias_100.json\n",
      "../data/modified_data/sent/geographical_bias.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:26<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/geographical_bias.json\n",
      "../data/modified_data/sent/coordinating_conjunction_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:21<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/coordinating_conjunction_100.json\n",
      "../data/modified_data/sent/capitalization_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:22<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/capitalization_100.json\n",
      "../data/modified_data/sent/dialectal_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/dialectal_100.json\n",
      "../data/modified_data/sent/sentiment_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/sentiment_100.json\n",
      "../data/modified_data/sent/grammatical_role_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:16<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/grammatical_role_100.json\n",
      "../data/modified_data/sent/negation.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [01:00<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/negation.json\n",
      "../data/modified_data/sent/length_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/length_bias_100.json\n",
      "../data/modified_data/sent/concept_replacement_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/concept_replacement_100.json\n",
      "../data/modified_data/sent/typo_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:19<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/typo_bias_100.json\n",
      "../data/modified_data/sent/geographical_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/geographical_bias_100.json\n",
      "../data/modified_data/sent/dialectal.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:31<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/dialectal.json\n",
      "../data/modified_data/sent/active_to_passive.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:28<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/active_to_passive.json\n",
      "../data/modified_data/sent/punctuation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/punctuation_100.json\n",
      "../data/modified_data/sent/discourse.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:27<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/discourse.json\n",
      "../data/modified_data/sent/derivation.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:22<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/derivation.json\n",
      "../data/modified_data/sent/derivation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:22<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/derivation_100.json\n",
      "../data/modified_data/sent/compound_word.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:23<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/compound_word.json\n",
      "../data/modified_data/sent/coordinating_conjunction.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:34<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/coordinating_conjunction.json\n",
      "../data/modified_data/sent/temporal_bias.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:29<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/temporal_bias.json\n",
      "../data/modified_data/sent/grammatical_role.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:23<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/grammatical_role.json\n",
      "../data/modified_data/sent/concept_replacement.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:33<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/concept_replacement.json\n",
      "../data/modified_data/sent/active_to_passive_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:21<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/active_to_passive_100.json\n",
      "../data/modified_data/sent/length_bias.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:25<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/length_bias.json\n",
      "../data/modified_data/sent/casual.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:31<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/casual.json\n",
      "../data/modified_data/sent/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:21<00:00,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/sent/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the reference CSV file with indices for sentiment analysis\n",
    "reference_df_sentiment = pd.read_csv('./results/sent/gpt4o-0shot-sst2.csv')\n",
    "\n",
    "# Load all JSON files in the sentiment analysis directory\n",
    "json_files_sentiment = os.listdir('../data/modified_data/sent/')\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files_sentiment:\n",
    "    file_path = f'../data/modified_data/sent/{file_name}'\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add index to each sample based on fuzzy matching original_text\n",
    "    for sample in tqdm(data):\n",
    "        # Find best matching row in reference CSV using fuzzy matching\n",
    "        best_match_ratio = 0\n",
    "        best_match_idx = None\n",
    "        \n",
    "        for idx, ref_text in enumerate(reference_df_sentiment['text']):\n",
    "            similarity = string_similarity(sample['original_text'], ref_text)\n",
    "            if similarity > best_match_ratio:\n",
    "                best_match_ratio = similarity\n",
    "                best_match_idx = idx\n",
    "        \n",
    "        if best_match_ratio >= 0.9:\n",
    "            sample['index'] = int(best_match_idx)\n",
    "        else:\n",
    "            print(f\"Warning: No matching text found (threshold 0.9) for sample in {file_name}\")\n",
    "            # break\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Added indices to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/modified_data/dialog/grammatical_role.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:00<00:00, 536203.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/dialog/grammatical_role.json\n",
      "../data/modified_data/dialog/negation.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:00<00:00, 701813.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/dialog/negation.json\n",
      "../data/modified_data/dialog/capitalization_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 600079.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/dialog/capitalization_100.json\n",
      "../data/modified_data/dialog/grammatical_role_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:00<00:00, 388361.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/dialog/grammatical_role_100.json\n",
      "../data/modified_data/dialog/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 515270.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/dialog/negation_100.json\n",
      "../data/modified_data/dialog/punctuation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 855980.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/dialog/punctuation_100.json\n",
      "../data/modified_data/dialog/typo_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 689852.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/dialog/typo_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the reference CSV file with indices\n",
    "reference_df = pd.read_json('../data/train_dev_test_data/dialog/test.json')\n",
    "\n",
    "# Create a mapping of text to index for O(1) lookup\n",
    "text_to_idx = {row['dialogue'][-1]: idx for idx, row in reference_df.iterrows()}\n",
    "\n",
    "# Load only the specified JSON files\n",
    "json_files = ['grammatical_role.json', 'negation.json', \n",
    "              'capitalization_100.json', 'grammatical_role_100.json', 'negation_100.json', \n",
    "              'punctuation_100.json', 'typo_bias_100.json']\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files:\n",
    "    file_path = f'../data/modified_data/dialog/{file_name}'\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add index to each sample based on exact text matching\n",
    "    new_data = []\n",
    "    for sample_tuple in tqdm(data):\n",
    "        sample = sample_tuple  # Get the sample dict from tuple\n",
    "        if sample['original_text'] in text_to_idx:\n",
    "            idx = int(text_to_idx[sample['original_text']])\n",
    "            new_data.append([idx, sample])  # Create new tuple with index first\n",
    "        else:\n",
    "            print(f\"Warning: No exact match found for sample in {file_name}\")\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(new_data, f, indent=2)\n",
    "\n",
    "    print(f\"Added indices to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"conversation_id\": \"bce9acc6-8e35-46eb-bd2a-9ff45d23117c##hit:10876_conv:21753#truncated#6\",\n",
      "  \"agents\": {\n",
      "    \"1\": {\n",
      "      \"is_human\": true,\n",
      "      \"persona_lines\": \"\"\n",
      "    },\n",
      "    \"0\": {\n",
      "      \"is_human\": true,\n",
      "      \"persona_lines\": \"\"\n",
      "    }\n",
      "  },\n",
      "  \"conversation_contexts\": \"content ## My wife made me pancakes for breakfast. I have a full belly and feel rather happy now.\",\n",
      "  \"auxiliary\": {\n",
      "    \"source\": \"EMPATHY_test\"\n",
      "  },\n",
      "  \"turns\": [\n",
      "    {\n",
      "      \"turn_id\": 0,\n",
      "      \"agent_id\": 0,\n",
      "      \"text\": \"My wife made me pancakes for breakfast. I have a full belly and feel rather happy now.\",\n",
      "      \"turn_context\": \"\",\n",
      "      \"auxiliary\": {\n",
      "        \"contradiction\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"turn_id\": 1,\n",
      "      \"agent_id\": 1,\n",
      "      \"text\": \"Yum!!! I love pancakes. Is it your favorite food too?\",\n",
      "      \"turn_context\": \"\",\n",
      "      \"auxiliary\": {\n",
      "        \"contradiction\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"turn_id\": 2,\n",
      "      \"agent_id\": 0,\n",
      "      \"text\": \"One of my favorite breakfast foods. She also made some thick sliced bacon. She treats me so well.\",\n",
      "      \"turn_context\": \"\",\n",
      "      \"auxiliary\": {\n",
      "        \"contradiction\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"turn_id\": 3,\n",
      "      \"agent_id\": 1,\n",
      "      \"text\": \"That is great you have a sweet wife!\",\n",
      "      \"turn_context\": \"\",\n",
      "      \"auxiliary\": {\n",
      "        \"contradiction\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"turn_id\": 4,\n",
      "      \"agent_id\": 0,\n",
      "      \"text\": \"For sure, no other woman I would rather have.\",\n",
      "      \"turn_context\": \"\",\n",
      "      \"auxiliary\": {\n",
      "        \"contradiction\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"turn_id\": 5,\n",
      "      \"agent_id\": 1,\n",
      "      \"text\": \"It is so great to hear about a happy couple :)\",\n",
      "      \"turn_context\": \"\",\n",
      "      \"auxiliary\": {\n",
      "        \"contradiction\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"is_truncated\": true,\n",
      "  \"is_contradiction\": false,\n",
      "  \"aggregated_contradiction_indices\": [],\n",
      "  \"record_id\": \"83d09076-4fd6-49c8-9ff1-57fe39523bf1\",\n",
      "  \"dialogue\": [\n",
      "    \"My wife made me pancakes for breakfast. I have a full belly and feel rather happy now.\",\n",
      "    \"Yum!!! I love pancakes. Is it your favorite food too?\",\n",
      "    \"One of my favorite breakfast foods. She also made some thick sliced bacon. She treats me so well.\",\n",
      "    \"That is great you have a sweet wife!\",\n",
      "    \"For sure, no other woman I would rather have.\",\n",
      "    \"It is so great to hear about a happy couple :)\"\n",
      "  ],\n",
      "  \"label\": \"no_contradiction\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load test.json and get sample with index 3301\n",
    "with open('../data/train_dev_test_data/dialog/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "sample_3301 = test_data[3301]\n",
    "print(json.dumps(sample_3301, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4214it [42:20,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to ./results/dialog/claude-3-5-sonnet-0shot-dialogue.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "csv_path = './results/dialog/claude-3-5-sonnet-0shot-dialogue.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load test.json for reference\n",
    "with open('../data/train_dev_test_data/dialog/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Create mapping of test dialogues to indices\n",
    "test_dialogues = {sample['dialogue'][-1]: idx for idx, sample in enumerate(test_data)}\n",
    "\n",
    "# Function to find best match using difflib\n",
    "def find_best_match(text, candidates):\n",
    "    best_ratio = 0\n",
    "    best_match = None\n",
    "    for candidate, idx in candidates.items():\n",
    "        ratio = difflib.SequenceMatcher(None, text, candidate).ratio()\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_match = (idx, ratio)\n",
    "    return best_match\n",
    "\n",
    "# Add index column based on matching\n",
    "indices = []\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    dialog = row['dialog'].split('\\n')[-1].replace('agent 0: ', '').replace('agent 1: ', '')  # Get last message\n",
    "    match = find_best_match(dialog, test_dialogues)\n",
    "    if match and match[1] > 0.9:  # Only keep matches with ratio > 0.9\n",
    "        indices.append(match[0])\n",
    "    else:\n",
    "        print(f\"No match found for {dialog}\")\n",
    "        indices.append(-1)  # Use -1 for no match found\n",
    "\n",
    "# Add indices column to dataframe\n",
    "df['index'] = indices\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Added indices to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:00<00:00, 258055.10it/s]\n",
      "100%|██████████| 91/91 [00:00<00:00, 380919.82it/s]\n",
      "100%|██████████| 134/134 [00:00<00:00, 276185.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing target JSON files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# Get specific json files\n",
    "json_dir = '../data/modified_data/ner'\n",
    "target_files = ['coordinating_conjunction_100.json', 'grammatical_role_100.json', 'geographical_bias_100.json']\n",
    "\n",
    "# Process each target file\n",
    "for filename in target_files:\n",
    "    filepath = os.path.join(json_dir, filename)\n",
    "    \n",
    "    # Load the JSON data\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Process each item in the data\n",
    "    for item in tqdm(data):\n",
    "        # Convert label fields if they exist\n",
    "        for field in ['label', 'modified_label', 'original_label']:\n",
    "            if field in item and isinstance(item[field], list):\n",
    "                # Handle original_label and modified_label format conversion\n",
    "                if field in ['original_label', 'modified_label']:\n",
    "                    # Convert list of list format to single list\n",
    "                    if isinstance(item[field], list) and len(item[field]) > 0 and isinstance(item[field][0], list):\n",
    "                        item[field] = item[field][0]\n",
    "                        continue\n",
    "\n",
    "                # Handle list of strings containing JSON\n",
    "                for i, label in enumerate(item[field]):\n",
    "                    if isinstance(label, str):\n",
    "                        try:\n",
    "                            # Handle both single dict strings and list of dict strings\n",
    "                            if label.startswith('['):\n",
    "                                # Parse list of dicts\n",
    "                                # First try to parse as a Python literal\n",
    "                                try:\n",
    "                                    label_list = ast.literal_eval(label)\n",
    "                                except:\n",
    "                                    # If that fails, try cleaning and parsing each item individually\n",
    "                                    label = label.strip('[]')\n",
    "                                    label_list = [l.strip() for l in label.split(',')]\n",
    "                                \n",
    "                                # Handle each dict in the list separately\n",
    "                                parsed_labels = []\n",
    "                                for l in label_list:\n",
    "                                    try:\n",
    "                                        # Try parsing as Python literal first\n",
    "                                        parsed = ast.literal_eval(str(l))\n",
    "                                        parsed_labels.append(parsed)\n",
    "                                    except:\n",
    "                                        try:\n",
    "                                            # Try JSON parsing with quote replacement\n",
    "                                            cleaned = str(l).replace(\"'\", '\"').replace('\\\\\"', '\"')\n",
    "                                            parsed = json.loads(cleaned)\n",
    "                                            parsed_labels.append(parsed)\n",
    "                                        except:\n",
    "                                            # Last resort - manual string cleaning\n",
    "                                            cleaned = str(l).replace(\"\\\\\", \"\").replace(\"'\", '\"')\n",
    "                                            cleaned = cleaned.replace('\"{', '{').replace('}\"', '}')\n",
    "                                            parsed = json.loads(cleaned)\n",
    "                                            parsed_labels.append(parsed)\n",
    "                                            \n",
    "                                item[field][i] = parsed_labels\n",
    "                            else:\n",
    "                                # Parse single dict\n",
    "                                try:\n",
    "                                    item[field][i] = ast.literal_eval(label)\n",
    "                                except:\n",
    "                                    item[field][i] = json.loads(label.replace(\"'\", '\"'))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed to convert {field}[{i}] in file {filename}\")\n",
    "                            print(f\"Value: {label}\")\n",
    "                            print(f\"Error: {str(e)}\")\n",
    "                            continue\n",
    "                \n",
    "    # Save the updated data back to file\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "        \n",
    "print(\"Completed processing target JSON files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/modified_data/ner/coordinating_conjunction_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:02<00:00, 16.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/ner/coordinating_conjunction_100.json\n",
      "../data/modified_data/ner/grammatical_role_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/ner/grammatical_role_100.json\n",
      "../data/modified_data/ner/geographical_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:08<00:00, 12.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added indices to: ../data/modified_data/ner/geographical_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the reference CSV file with indices\n",
    "reference_df = pd.read_json('../data/train_dev_test_data/ner/fewnerd_sample_test.json')\n",
    "\n",
    "# Create a mapping of text to index for O(1) lookup\n",
    "text_to_idx = {row['text']: idx for idx, row in reference_df.iterrows()}\n",
    "\n",
    "# Load all JSON files in the specified directory\n",
    "json_files = [f for f in os.listdir('../data/modified_data/ner/') if f.endswith('.json')]\n",
    "# Filter to only process specific files\n",
    "target_files = ['grammatical_role_100.json', 'coordinating_conjunction_100.json', 'geographical_bias_100.json']\n",
    "json_files = [f for f in json_files if f in target_files]\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files:\n",
    "    file_path = f'../data/modified_data/ner/{file_name}'\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add index to each sample based on exact text matching\n",
    "    for sample in tqdm(data):\n",
    "        # Find best match using difflib ratio with threshold 0.9\n",
    "        best_match = None\n",
    "        best_ratio = 0\n",
    "        for ref_text in text_to_idx:\n",
    "            ratio = difflib.SequenceMatcher(None, sample['original_text'], ref_text).ratio()\n",
    "            if ratio > 0.9 and ratio > best_ratio:\n",
    "                best_ratio = ratio\n",
    "                best_match = ref_text\n",
    "                \n",
    "        if best_match:\n",
    "            sample['index'] = int(text_to_idx[best_match])\n",
    "        else:\n",
    "            print(sample['original_text'])\n",
    "            print(f\"Warning: No exact match found for sample in {file_name}\")\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Added indices to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/modified_data/ner/casual_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:07<00:00, 12.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/casual_100.json\n",
      "../data/modified_data/ner/discourse_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:05<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/discourse_100.json\n",
      "../data/modified_data/ner/compound_word_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [00:06<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/compound_word_100.json\n",
      "../data/modified_data/ner/temporal_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:08<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/temporal_bias_100.json\n",
      "../data/modified_data/ner/coordinating_conjunction_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:05<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/coordinating_conjunction_100.json\n",
      "../data/modified_data/ner/capitalization_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/capitalization_100.json\n",
      "../data/modified_data/ner/dialectal_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:08<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/dialectal_100.json\n",
      "../data/modified_data/ner/sentiment_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:10<00:00, 11.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/sentiment_100.json\n",
      "../data/modified_data/ner/grammatical_role_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:07<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/grammatical_role_100.json\n",
      "../data/modified_data/ner/length_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:07<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/length_bias_100.json\n",
      "../data/modified_data/ner/concept_replacement_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:06<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/concept_replacement_100.json\n",
      "../data/modified_data/ner/typo_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 13.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/typo_bias_100.json\n",
      "../data/modified_data/ner/geographical_bias_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:11<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/geographical_bias_100.json\n",
      "../data/modified_data/ner/punctuation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 11.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/punctuation_100.json\n",
      "../data/modified_data/ner/derivation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:05<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/derivation_100.json\n",
      "../data/modified_data/ner/active_to_passive_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:06<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/active_to_passive_100.json\n",
      "../data/modified_data/ner/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:08<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added original labels to: ../data/modified_data/ner/negation_100.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import difflib\n",
    "\n",
    "# Load the reference data with original labels\n",
    "reference_df = pd.read_json('../data/train_dev_test_data/ner/fewnerd_sample_test.json')\n",
    "\n",
    "# Create mapping of text to original labels\n",
    "text_to_labels = {row['text']: row['label'] for _, row in reference_df.iterrows()}\n",
    "\n",
    "# Process each JSON file\n",
    "json_files = [f for f in os.listdir('../data/modified_data/ner/') if f.endswith('.json')]\n",
    "\n",
    "for file_name in json_files:\n",
    "    file_path = f'../data/modified_data/ner/{file_name}'\n",
    "    print(file_path)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add original labels to each sample based on text matching\n",
    "    for sample in tqdm(data):\n",
    "        # Find best match using difflib ratio with threshold 0.9\n",
    "        best_match = None\n",
    "        best_ratio = 0\n",
    "        for ref_text in text_to_labels:\n",
    "            ratio = difflib.SequenceMatcher(None, sample['original_text'], ref_text).ratio()\n",
    "            if ratio > 0.9 and ratio > best_ratio:\n",
    "                best_ratio = ratio\n",
    "                best_match = ref_text\n",
    "                \n",
    "        if best_match:\n",
    "            sample['original_label'] = text_to_labels[best_match]\n",
    "        else:\n",
    "            print(sample['original_text'])\n",
    "            print(f\"Warning: No match found for sample in {file_name}\")\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Added original labels to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "Filtered and saved 85 entries to ../data/modified_data/coref/grammatical_role.json\n"
     ]
    }
   ],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('/Users/hungcoreft/workspace/ood-modification/data/scripts/grammatical_role_coref_comparison.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin([1, np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "json_path = '../data/modified_data/coref/grammatical_role.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        # data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        data_dict[idx]['modified_label'] = row['modified_label']\n",
    "        data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        data_dict[idx]['modified_candidates'] = row['modified_candidates']\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [data_dict[idx] for idx in filtered_df['index']]\n",
    "\n",
    "# Save filtered data back to JSON\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create mapping of indices to data entries\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m {\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m: item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data}\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Update data based on filtered rows\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m filtered_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(row)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'index'"
     ]
    }
   ],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('/Users/hungcoreft/workspace/ood-modification/data/scripts/grammatical_role_ner_comparison.csv')\n",
    "filtered_df = comparison_df\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "# filtered_df = comparison_df[comparison_df['Keep'].isin([1, np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "json_path = '../data/modified_data/ner/grammatical_role_100.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        # data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        modified_label = ast.literal_eval(row['modified_label'])\n",
    "        data_dict[idx]['modified_label'] = modified_label\n",
    "        # data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        # data_dict[idx]['modified_candidates'] = row['modified_candidates']\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [data_dict[idx] for idx in filtered_df['index']]\n",
    "\n",
    "# Save filtered data back to JSON\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "70\n",
      "Filtered and saved 70 entries to ../data/modified_data/dialog/grammatical_role.json\n"
     ]
    }
   ],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('/Users/hungcoreft/workspace/ood-modification/data/scripts/grammatical_role_dialogue_comparison.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "print(len(comparison_df))\n",
    "\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin([1, np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "json_path = '../data/modified_data/dialog/grammatical_role.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {index: item for index, item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        # data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        data_dict[idx]['modified_label'] = row['modified_label']\n",
    "        # data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        # data_dict[idx]['modified_candidates'] = row['modified_candidates']\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [[idx,data_dict[idx]] for idx in filtered_df['index']]\n",
    "# print(filtered_data[0])\n",
    "# Save filtered data back to JSON\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts in file 1: 53\n",
      "Total texts in file 2: 91\n",
      "Texts in both files: 53\n",
      "\n",
      "Texts ONLY in file 1 (0):\n",
      "\n",
      "Texts ONLY in file 2 (38):\n",
      "\n",
      "Text: The daughter of Khalid Bakdash was married to Jamil, a Syrian politician who was the Secretary-General of the Syrian Communist Party and the first communist to be an elected member of an Arab parliament in 1954.\n",
      "\n",
      "Text: In the early 1930s the band moved to the Taft Hotel of the Grill Room in New York; the band was renamed 'George Hall and His Hotel Taft Orchestra'.\n",
      "\n",
      "Text: The Newfoundland and Labrador was a leader-centred political party in Newfoundland Reform Liberal Party, Canada from 1975 to 1979.\n",
      "\n",
      "Text: Remixes of ``Like It or Not`` and ``Tearing Me Up`` featured the album from artists like A-Trak, RAC, Joris Voorn, and Tale of Us.\n",
      "\n",
      "Text: During the Battle of Antietam on September 17, 1862, the Veterans Reserve Corps was injured and forced to serve the remainder of the war as an officer in Wilkinson.\n",
      "\n",
      "Text: A travel guide in 1895 was authored by the founder of Kōeido (the purveyors of Kibi dango), in which he claimed that Kibitsuhiko rolled with his own hand some kibi dango to give to Emperor Jimmu who stopped at Takayama Palace in Okayama.\n",
      "\n",
      "Text: The Real Live Brady Bunch was one of the creations of Mary Weiss at The Annoyance Theater, which led to a series of feature film remakes.\n",
      "\n",
      "Text: The lowest match aggregate in ODI history for Canada is 91 scored at the 1979 Cricket World Cup against England.\n",
      "\n",
      "Text: Amrinder lost the match 1–3 with all three goals coming after Pune came on.\n",
      "\n",
      "Text: ``Gayachintamani`` and ``Kshatrachudamini`` in prose were written by his pupil Vadeebhasimha based on poet Banas ``Kadambari`` and ``Charitarasara`` was written by minister Chavundaraya.\n",
      "\n",
      "Text: In some versions, a softer version that lacks the characteristic smoky flavor of the original dish is not grilled, but is instead boiled, resulting in the eggplant.\n",
      "\n",
      "Text: Del Imaginario Discos's Argentina also released the album in 2007.\n",
      "\n",
      "Text: A playable character in the multiplayer online battle arena, ``SMITE'', is Anhur.\n",
      "\n",
      "Text: A green artery from the centre of Manchester to the West Pennine Moors was proposed to be created by the Croal Irwell Regional Park Project Proposal, announced in June 2003, supported by Salford, Bury and Bolton councils and the Red Rose Forest.\n",
      "\n",
      "Text: According to Steak' n Shake archives, a trademark on the word in the 1930s was applied for by the restaurant's founder, Gus Belt.\n",
      "\n",
      "Text: The Ninth suffered a serious defeat at the Battle of Camulodunum under Boudica in the rebellion of Quintus Petillius Cerialis (61), when most of the foot-soldiers were killed in a disastrous attempt to relieve the besieged city of Camulodunum (Colchester).\n",
      "\n",
      "Text: A spin-off of Murphy Oil was founded in 2013.\n",
      "\n",
      "Text: The Union for Peru party belongs to a Congressman representing Cajamarca for the period 2006–2011.\n",
      "\n",
      "Text: The 8th legislative district in Omaha is represented by Hunt, consisting of the midtown neighborhoods of Dundee, Benson, and Keystone.\n",
      "\n",
      "Text: A batch of A1 Pacifics they had built in 1924 used several features from them.\n",
      "\n",
      "Text: He has also performed and worked with many other Scandinavian artists such as Silje Nergaard, Bjørn Eidsvåg, Oslo Gospel Choir, Espen Lind, Elizabeth Norberg-Schulz and Carola Häggkvist.\n",
      "\n",
      "Text: A serious defeat at the Battle of Camulodunum was suffered by the Ninth under Quintus Petillius Cerialis in the rebellion of Boudica (61), when the besieged city of Camulodunum (Colchester) was in a disastrous attempt to be relieved by most of the foot-soldiers who were killed.\n",
      "\n",
      "Text: Hunt represents the 8th legislative district in Dundee, consisting of the midtown neighborhoods of Omaha, Benson, and Keystone.\n",
      "\n",
      "Text: The hub of operations for Norfolk Southern in the Greater Pittsburgh area is Conway, featuring a hump yard and a crew change point for virtually all Pittsburgh and Fort Wayne Line trains.\n",
      "\n",
      "Text: Genre classics, older movies, and underground cult films, with occasional stabs at mainstream and newer genre pieces, are focused on by Back to Basics.\n",
      "\n",
      "Text: The Larry G.Messinger pilot, Major B-52, later recalled,\n",
      "\n",
      "Text: The Croal Irwell Regional Park Project Proposal to create a green artery from the centre of the West Pennine Moors to Manchester was announced in June 2003 supported by Salford, Bury and Bolton councils and the Red Rose Forest.\n",
      "\n",
      "Text: An American writer mostly of romance novels is Elin Hilderbrand.\n",
      "\n",
      "Text: 91 is the lowest match aggregate in ODI history for England scored at the 1979 Cricket World Cup against Canada.\n",
      "\n",
      "Text: Victor Cristaldo (born 10 May 1967) is a retired Paraguayan-born Australian professional footballer of Argentine descent.\n",
      "\n",
      "Text: Many other Scandinavian artists such as Bjørn Eidsvåg, Silje Nergaard, Oslo Gospel Choir, Espen Lind, Elizabeth Norberg-Schulz and Carola Häggkvist have also performed and worked with him.\n",
      "\n",
      "Text: Khalid Bakdash was married to the daughter of Jamil, a Syrian politician who was the Secretary-General of the Syrian Communist Party and the first communist to be an elected member of an Arab parliament in 1954.\n",
      "\n",
      "Text: Maynooth Seminary was entered by Kenyon in 1829 – the year of Catholic Emancipation.\n",
      "\n",
      "Text: He is a Congressman representing the Union for Peru party for the period 2006–2011, and belongs to Cajamarca.\n",
      "\n",
      "Text: According to Gus Belt archives, the restaurant's founder, Steak' n Shake, applied for a trademark on the word in the 1930s.\n",
      "\n",
      "Text: A Church of England parish church at Bothenhampton, near Bridport in Dorset, England, is Holy Trinity Church.\n",
      "\n",
      "Text: Amino acid uses ammonia produced by nitrate reduction, glutamine synthetase degradation, and photorespiration.\n",
      "\n",
      "Text: Orchid Island Golf and Beach Club is actually most of the town, a gated community.\n",
      "\n",
      "Example texts present in BOTH files (showing first 3):\n",
      "\n",
      "Text: At a festival, Vin saves Siddhi from being shot at by Rudra.\n",
      "\n",
      "Text: Some of the gold could be removed and melted for coin or bullion in times of severe financial hardship, so that construction was modular to be replaced later when finances had recovered.\n",
      "\n",
      "Text: It is developed by Golaem, a Rennes-based software company (created in France in 2009).\n"
     ]
    }
   ],
   "source": [
    "# Load both JSON files\n",
    "file1_path = '../data/modified_data/ner/grammatical_role_100.json'\n",
    "file2_path = '../data/modified_data/ner/grammatical_role_100_compare.json'\n",
    "\n",
    "with open(file1_path, 'r') as f1, open(file2_path, 'r') as f2:\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# Create sets of modified texts from both files\n",
    "texts1 = {item['modified_text'] for item in data1}\n",
    "texts2 = {item['modified_text'] for item in data2}\n",
    "\n",
    "# Print total counts\n",
    "print(f\"Total texts in file 1: {len(texts1)}\")\n",
    "print(f\"Total texts in file 2: {len(texts2)}\")\n",
    "print(f\"Texts in both files: {len(texts1.intersection(texts2))}\")\n",
    "\n",
    "# Find texts present in only one file\n",
    "only_in_file1 = texts1 - texts2\n",
    "only_in_file2 = texts2 - texts1\n",
    "\n",
    "# Print texts present only in file 1\n",
    "print(f\"\\nTexts ONLY in file 1 ({len(only_in_file1)}):\")\n",
    "for text in only_in_file1:\n",
    "    print(f\"\\nText: {text}\")\n",
    "\n",
    "# Print texts present only in file 2  \n",
    "print(f\"\\nTexts ONLY in file 2 ({len(only_in_file2)}):\")\n",
    "for text in only_in_file2:\n",
    "    print(f\"\\nText: {text}\")\n",
    "\n",
    "# Print some examples of texts in both files\n",
    "common_texts = texts1.intersection(texts2)\n",
    "print(f\"\\nExample texts present in BOTH files (showing first 3):\")\n",
    "for text in list(common_texts)[:3]:\n",
    "    print(f\"\\nText: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'[' was never closed (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[91], line 9\u001b[0m\n    row['modified_label'] = ast.literal_eval(row['modified_label'])\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/lib/python3.12/ast.py:66\u001b[0m in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/miniconda3/lib/python3.12/ast.py:52\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    [{'text': 'Canada', 'value': 'ORGANIZATION'}, {'text': 'Cricket World Cup', 'value': 'EVENT'}, {'text': 'England', 'value': 'ORGANIZATION'}\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '[' was never closed\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('/Users/hungcoreft/workspace/ood-modification/data/scripts/coordinating_conjunction_ner_comparison_detailed.csv', 'r') as f:\n",
    "    csv_reader = csv.DictReader(f)\n",
    "    comparison_data = [row for row in csv_reader]\n",
    "for row in comparison_data:\n",
    "    row['index'] = row['index']\n",
    "    row['original_text'] = row['original_text']\n",
    "    row['modified_text'] = row['modified_text']\n",
    "    row['modified_label'] = ast.literal_eval(row['modified_label'])\n",
    "    row['original_label'] = ast.literal_eval(row['original_label'])\n",
    "json.dump(comparison_data, open('coordinating_conjunction_ner_comparison.json', 'w'), indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts in file 1: 38\n",
      "Total texts in file 2: 71\n",
      "Texts in both files: 38\n",
      "\n",
      "Texts ONLY in file 1 (0):\n",
      "\n",
      "Texts ONLY in file 2 (33):\n",
      "\n",
      "Text: During the last few years, I have lived in Melbourne and Sydney.\n",
      "\n",
      "Text: Victor Cristaldo (born 10 May 1967) is a retired Argentine-born Australian and Paraguayan professional footballer.\n",
      "\n",
      "Text: During the last few years, I live in Melbourne and Sydney.\n",
      "\n",
      "Text: In particular, Reform Jews may be criticized and lampooned for their rejection of traditional Jewish beliefs.\n",
      "\n",
      "Text: Perhaps the best known Ward LaFrance products were the P-80 ``Ambassador'' model of pumper and the Q-90 ``Envoy'' model of ladder truck, one of which was donated as product placement by the company to be used as the fictional Los Angeles County Fire Department Engine 51 on the 1970s television program`` Emergency!\n",
      "\n",
      "Text: A prototype was fitted in the mid-'60s in a one-off DB5 extended 4'' after the doors and driven by Marek and his team personally, and a normally 6-cylinder Aston Martin DB7 was equipped with a V8 unit in 1998.\n",
      "\n",
      "Text: Ronald and his wife will travel to Iceland.\n",
      "\n",
      "Text: He and his team are Congressmen representing Cajamarca for the period 2006–2011, and belong to the Union for Peru party.\n",
      "\n",
      "Text: Conway is the hub of operations for Norfolk Southern in the Greater Pittsburgh and Greater Philadelphia area, featuring a hump yard and a crew change point for virtually all Pittsburgh and Fort Wayne Line trains.\n",
      "\n",
      "Text: The lowest match aggregate in ODI history for England and its team is 91 scored at the 1979 Cricket World Cup against Canada.\n",
      "\n",
      "Text: Jamil was married to the daughter of Khalid Bakdash, a Syrian politician who was the Secretary-General of the Syrian Communist Party and the first communist to be an elected member of an Arab and European parliament in 1954.\n",
      "\n",
      "Text: Victor Cristaldo (born 10 May 1967) has retired and is a former Argentine-born Australian professional footballer of Paraguayan descent.\n",
      "\n",
      "Text: She has since worked with homeless patients and holds the position of Management Consultant for MedAmerica and HealthCorp.\n",
      "\n",
      "Text: During last few years, I live in Melbourne and Sydney.\n",
      "\n",
      "Text: The Croal Irwell Regional Park Project Proposal to create a green artery from the centre of Manchester to the West Pennine Moors was announced in June 2003 and received widespread media coverage, supported by Salford, Bury and Bolton councils and the Red Rose Forest.\n",
      "\n",
      "Text: Elin Hilderbrand and Nicholas Sparks are American writers mostly of romance novels.\n",
      "\n",
      "Text: The Croal Irwell Regional Park Project Proposal to create a green artery from the centre of Manchester to the West Pennine Moors and the Peak District was announced in June 2003 supported by Salford, Bury and Bolton councils and the Red Rose Forest.\n",
      "\n",
      "Text: Jamil was married and settled down with the daughter of Khalid Bakdash, a Syrian politician who was the Secretary-General of the Syrian Communist Party and the first communist to be an elected member of an Arab parliament in 1954.\n",
      "\n",
      "Text: She and her team have since worked with homeless patients and hold the position of Management Consultant for MedAmerica.\n",
      "\n",
      "Text: In particular, Reform Jews and Conservative Jews may be lampooned for their rejection of traditional Jewish beliefs.\n",
      "\n",
      "Text: In the early 1930s the band signed a new record deal and moved to the Grill Room of the Taft Hotel in New York; the band was renamed ``George Hall and His Hotel Taft Orchestra``.\n",
      "\n",
      "Text: They used several features from a batch of A1 Pacifics and A2 Pacifics they had built in 1924.\n",
      "\n",
      "Text: Conway and the surrounding region is the hub of operations for Norfolk Southern in the Greater Pittsburgh area, featuring a hump yard and a crew change point for virtually all Pittsburgh and Fort Wayne Line trains.\n",
      "\n",
      "Text: In the early 1930s, George Hall and the band moved to the Grill Room of the Taft Hotel in New York; the band was renamed ``George Hall and His Hotel Taft Orchestra``.\n",
      "\n",
      "Text: The Croal Irwell Regional Park Project Proposal and the environmental initiative to create a green artery from the centre of Manchester to the West Pennine Moors was announced in June 2003 supported by Salford, Bury and Bolton councils and the Red Rose Forest.\n",
      "\n",
      "Text: He secured funding and went on to produce Kim Fowley and the BMX Bandits (band) Receiver Records' album ``Hidden Agenda At the Thirteenth Not``.\n",
      "\n",
      "Text: Perhaps the best known Ward LaFrance product was the P-80 ``Ambassador'' model of pumper, one of which was showcased and donated as product placement by the company to be used as the fictional Los Angeles County Fire Department Engine 51 on the 1970s television program`` Emergency!\n",
      "\n",
      "Text: He is a Congressman representing Cajamarca and Arequipa for the period 2006–2011, and belongs to the Union for Peru party.\n",
      "\n",
      "Text: USP42 has also been shown to deubiquitinate p53 and MDM2 and may be required for the ability of p53 to respond to stress.\n",
      "\n",
      "Text: Ronald will take a week's leave and travel to Iceland.\n",
      "\n",
      "Text: The lowest match aggregate in ODI history for England is 91, recorded and scored at the 1979 Cricket World Cup against Canada.\n",
      "\n",
      "Text: The Cnidaria and other invertebrates are groups of animals found exclusively in aquatic and mostly marine environments.\n",
      "\n",
      "Text: He was born in Wellingborough, Northamptonshire, where he attended Victoria Junior School, completed his studies, and then went on to Westfield Boys School and Sir Christopher Hatton School.\n",
      "\n",
      "Example texts present in BOTH files (showing first 3):\n",
      "\n",
      "Text: It is developed and marketed by Golaem, a France-based software company (created in Rennes in 2009).\n",
      "\n",
      "Text: Gonu and the subsequent flooding caused strong gusty winds and torrential rainfall along Pakistan's Arabian Sea coast from Karachi to Gwadar.\n",
      "\n",
      "Text: USP42 and its associated factors have also been shown to deubiquitinate p53 and may be required for the ability of p53 to respond to stress.\n"
     ]
    }
   ],
   "source": [
    "file1_path = '../data/modified_data/ner/coordinating_conjunction_100.json'\n",
    "file2_path = '../data/modified_data/ner/coordinating_conjunction_100 copy.json'\n",
    "\n",
    "with open(file1_path, 'r') as f1, open(file2_path, 'r') as f2:\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# Create sets of modified texts from both files\n",
    "texts1 = {item['modified_text'] for item in data1}\n",
    "texts2 = {item['modified_text'] for item in data2}\n",
    "\n",
    "# Print total counts\n",
    "print(f\"Total texts in file 1: {len(texts1)}\")\n",
    "print(f\"Total texts in file 2: {len(texts2)}\")\n",
    "print(f\"Texts in both files: {len(texts1.intersection(texts2))}\")\n",
    "\n",
    "# Find texts present in only one file\n",
    "only_in_file1 = texts1 - texts2\n",
    "only_in_file2 = texts2 - texts1\n",
    "\n",
    "# Print texts present only in file 1\n",
    "print(f\"\\nTexts ONLY in file 1 ({len(only_in_file1)}):\")\n",
    "for text in only_in_file1:\n",
    "    print(f\"\\nText: {text}\")\n",
    "\n",
    "# Print texts present only in file 2  \n",
    "print(f\"\\nTexts ONLY in file 2 ({len(only_in_file2)}):\")\n",
    "for text in only_in_file2:\n",
    "    print(f\"\\nText: {text}\")\n",
    "\n",
    "# Print some examples of texts in both files\n",
    "common_texts = texts1.intersection(texts2)\n",
    "print(f\"\\nExample texts present in BOTH files (showing first 3):\")\n",
    "for text in list(common_texts)[:3]:\n",
    "    print(f\"\\nText: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
